{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70286db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from evaluate import load\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7ffd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('rajpurkar/squad')\n",
    "split = dataset['validation'].train_test_split(test_size=0.5, seed=42)\n",
    "raw_datasets = {\n",
    "    'train': dataset['train'],\n",
    "    'validation': split['train'],\n",
    "    'test': split['test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46af2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "max_length = 384\n",
    "doc_stride = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad38ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['question'], examples['context'],\n",
    "        truncation='only_second',\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    overflow_to_sample_mapping = tokenized.pop('overflow_to_sample_mapping')\n",
    "    offset_mapping = tokenized.pop('offset_mapping')\n",
    "\n",
    "    tokenized['start_positions'] = []\n",
    "    tokenized['end_positions'] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_idx = overflow_to_sample_mapping[i]\n",
    "        answers = examples['answers'][sample_idx]\n",
    "        cls_index = tokenized['input_ids'][i].index(tokenizer.cls_token_id)\n",
    "\n",
    "        if len(answers['answer_start']) == 0:\n",
    "            tokenized['start_positions'].append(cls_index)\n",
    "            tokenized['end_positions'].append(cls_index)\n",
    "        else:\n",
    "            start_char = answers['answer_start'][0]\n",
    "            end_char = start_char + len(answers['text'][0])\n",
    "            sequence_ids = tokenized.sequence_ids(i)\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(tokenized['input_ids'][i]) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if offsets[token_start_index][0] > end_char or offsets[token_end_index][1] < start_char:\n",
    "                tokenized['start_positions'].append(cls_index)\n",
    "                tokenized['end_positions'].append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized['start_positions'].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized['end_positions'].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a7a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = {k: raw_datasets[k].map(\n",
    "    prepare_features,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[k].column_names\n",
    ") for k in ['train', 'validation', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6703eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(row['input_ids']),\n",
    "            'attention_mask': torch.tensor(row['attention_mask']),\n",
    "            'start_positions': torch.tensor(row['start_positions']),\n",
    "            'end_positions': torch.tensor(row['end_positions'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba5d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QADataset(tokenized_datasets['train'])\n",
    "val_dataset   = QADataset(tokenized_datasets['validation'])\n",
    "test_dataset  = QADataset(tokenized_datasets['test'])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0039859",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d139c44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "025ca505",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['query', 'value', 'key', 'dense'],\n",
    "    bias=\"none\",\n",
    "    use_rslora=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef10aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(base_model, lora_config)\n",
    "if not hasattr(model, 'hf_device_map'):\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1599ebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10bao\\AppData\\Local\\Temp\\ipykernel_27496\\3995698535.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n"
     ]
    }
   ],
   "source": [
    "optimizer = bnb.optim.AdamW8bit(\n",
    "    model.parameters(), \n",
    "    lr=2e-4,  # Slightly higher learning rate for QLoRA\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=total_steps//10, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metric = load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd2e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_pos = batch['start_positions'].to(device)\n",
    "        end_pos = batch['end_positions'].to(device)\n",
    "\n",
    "        # Use mixed precision training\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "                loss = (criterion(start_logits, start_pos) + criterion(end_logits, end_pos)) / 2\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "            loss = (criterion(start_logits, start_pos) + criterion(end_logits, end_pos)) / 2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} Step {step}/{len(train_loader)} - Loss: {running_loss/step:.4f}\")\n",
    "\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e52ae9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(data_loader, raw_data):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_pos = batch['start_positions'].to(device)\n",
    "        end_pos = batch['end_positions'].to(device)\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "                loss = (criterion(start_logits, start_pos) + criterion(end_logits, end_pos)) / 2\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "            loss = (criterion(start_logits, start_pos) + criterion(end_logits, end_pos)) / 2\n",
    "\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        # Generate predictions\n",
    "        for b in range(len(input_ids)):\n",
    "            start_idx = torch.argmax(start_logits[b]).item()\n",
    "            end_idx = torch.argmax(end_logits[b]).item()\n",
    "            \n",
    "            if start_idx > end_idx:\n",
    "                answer = \"\"\n",
    "            else:\n",
    "                tokens = input_ids[b][start_idx:end_idx+1]\n",
    "                answer = tokenizer.decode(tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "            sample_index = i * data_loader.batch_size + b\n",
    "            if sample_index < len(raw_data):\n",
    "                predictions.append({\"id\": raw_data[sample_index][\"id\"], \"prediction_text\": answer})\n",
    "                references.append({\"id\": raw_data[sample_index][\"id\"], \"answers\": raw_data[sample_index][\"answers\"]})\n",
    "\n",
    "    metrics = metric.compute(predictions=predictions, references=references)\n",
    "    return eval_loss / len(data_loader), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6029e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/3 ===\n",
      "Epoch 1 Step 100/11066 - Loss: 5.6632\n",
      "Epoch 1 Step 200/11066 - Loss: 5.5571\n",
      "Epoch 1 Step 300/11066 - Loss: 5.5064\n",
      "Epoch 1 Step 400/11066 - Loss: 5.4177\n",
      "Epoch 1 Step 500/11066 - Loss: 5.3560\n",
      "Epoch 1 Step 600/11066 - Loss: 5.2413\n",
      "Epoch 1 Step 700/11066 - Loss: 5.1992\n",
      "Epoch 1 Step 800/11066 - Loss: 5.1560\n",
      "Epoch 1 Step 900/11066 - Loss: 5.0911\n",
      "Epoch 1 Step 1000/11066 - Loss: 5.0320\n",
      "Epoch 1 Step 1100/11066 - Loss: 4.9351\n",
      "Epoch 1 Step 1200/11066 - Loss: 4.8339\n",
      "Epoch 1 Step 1300/11066 - Loss: 4.7842\n",
      "Epoch 1 Step 1400/11066 - Loss: 4.7154\n",
      "Epoch 1 Step 1500/11066 - Loss: 4.6261\n",
      "Epoch 1 Step 1600/11066 - Loss: 4.5342\n",
      "Epoch 1 Step 1700/11066 - Loss: 4.4688\n",
      "Epoch 1 Step 1800/11066 - Loss: 4.3940\n",
      "Epoch 1 Step 1900/11066 - Loss: 4.3026\n",
      "Epoch 1 Step 2000/11066 - Loss: 4.2351\n",
      "Epoch 1 Step 2100/11066 - Loss: 4.1922\n",
      "Epoch 1 Step 2200/11066 - Loss: 4.1141\n",
      "Epoch 1 Step 2300/11066 - Loss: 4.0795\n",
      "Epoch 1 Step 2400/11066 - Loss: 4.0012\n",
      "Epoch 1 Step 2500/11066 - Loss: 3.9369\n",
      "Epoch 1 Step 2600/11066 - Loss: 3.8601\n",
      "Epoch 1 Step 2700/11066 - Loss: 3.8077\n",
      "Epoch 1 Step 2800/11066 - Loss: 3.7669\n",
      "Epoch 1 Step 2900/11066 - Loss: 3.7086\n",
      "Epoch 1 Step 3000/11066 - Loss: 3.6588\n",
      "Epoch 1 Step 3100/11066 - Loss: 3.5817\n",
      "Epoch 1 Step 3200/11066 - Loss: 3.5462\n",
      "Epoch 1 Step 3300/11066 - Loss: 3.5168\n",
      "Epoch 1 Step 3400/11066 - Loss: 3.4431\n",
      "Epoch 1 Step 3500/11066 - Loss: 3.3893\n",
      "Epoch 1 Step 3600/11066 - Loss: 3.3315\n",
      "Epoch 1 Step 3700/11066 - Loss: 3.3216\n",
      "Epoch 1 Step 3800/11066 - Loss: 3.2400\n",
      "Epoch 1 Step 3900/11066 - Loss: 3.1769\n",
      "Epoch 1 Step 4000/11066 - Loss: 3.1694\n",
      "Epoch 1 Step 4100/11066 - Loss: 3.1344\n",
      "Epoch 1 Step 4200/11066 - Loss: 3.0982\n",
      "Epoch 1 Step 4300/11066 - Loss: 3.0802\n",
      "Epoch 1 Step 4400/11066 - Loss: 3.0528\n",
      "Epoch 1 Step 4500/11066 - Loss: 3.0540\n",
      "Epoch 1 Step 4600/11066 - Loss: 2.9813\n",
      "Epoch 1 Step 4700/11066 - Loss: 2.9214\n",
      "Epoch 1 Step 4800/11066 - Loss: 2.9243\n",
      "Epoch 1 Step 4900/11066 - Loss: 2.9195\n",
      "Epoch 1 Step 5000/11066 - Loss: 2.8714\n",
      "Epoch 1 Step 5100/11066 - Loss: 2.8537\n",
      "Epoch 1 Step 5200/11066 - Loss: 2.8350\n",
      "Epoch 1 Step 5300/11066 - Loss: 2.8308\n",
      "Epoch 1 Step 5400/11066 - Loss: 2.8155\n",
      "Epoch 1 Step 5500/11066 - Loss: 2.7914\n",
      "Epoch 1 Step 5600/11066 - Loss: 2.7796\n",
      "Epoch 1 Step 5700/11066 - Loss: 2.7562\n",
      "Epoch 1 Step 5800/11066 - Loss: 2.6999\n",
      "Epoch 1 Step 5900/11066 - Loss: 2.6394\n",
      "Epoch 1 Step 6000/11066 - Loss: 2.6197\n",
      "Epoch 1 Step 6100/11066 - Loss: 2.6168\n",
      "Epoch 1 Step 6200/11066 - Loss: 2.6183\n",
      "Epoch 1 Step 6300/11066 - Loss: 2.6047\n",
      "Epoch 1 Step 6400/11066 - Loss: 2.5880\n",
      "Epoch 1 Step 6500/11066 - Loss: 2.5474\n",
      "Epoch 1 Step 6600/11066 - Loss: 2.5543\n",
      "Epoch 1 Step 6700/11066 - Loss: 2.5015\n",
      "Epoch 1 Step 6800/11066 - Loss: 2.4615\n",
      "Epoch 1 Step 6900/11066 - Loss: 2.4396\n",
      "Epoch 1 Step 7000/11066 - Loss: 2.4398\n",
      "Epoch 1 Step 7100/11066 - Loss: 2.4297\n",
      "Epoch 1 Step 7200/11066 - Loss: 2.3982\n",
      "Epoch 1 Step 7300/11066 - Loss: 2.4032\n",
      "Epoch 1 Step 7400/11066 - Loss: 2.3415\n",
      "Epoch 1 Step 7500/11066 - Loss: 2.3196\n",
      "Epoch 1 Step 7600/11066 - Loss: 2.2633\n",
      "Epoch 1 Step 7700/11066 - Loss: 2.2303\n",
      "Epoch 1 Step 7800/11066 - Loss: 2.2124\n",
      "Epoch 1 Step 7900/11066 - Loss: 2.1993\n",
      "Epoch 1 Step 8000/11066 - Loss: 2.1784\n",
      "Epoch 1 Step 8100/11066 - Loss: 2.1573\n",
      "Epoch 1 Step 8200/11066 - Loss: 2.1383\n",
      "Epoch 1 Step 8300/11066 - Loss: 2.0865\n",
      "Epoch 1 Step 8400/11066 - Loss: 2.0753\n",
      "Epoch 1 Step 8500/11066 - Loss: 2.0154\n",
      "Epoch 1 Step 8600/11066 - Loss: 1.9747\n",
      "Epoch 1 Step 8700/11066 - Loss: 1.9714\n",
      "Epoch 1 Step 8800/11066 - Loss: 1.9723\n",
      "Epoch 1 Step 8900/11066 - Loss: 1.9303\n",
      "Epoch 1 Step 9000/11066 - Loss: 1.8848\n",
      "Epoch 1 Step 9100/11066 - Loss: 1.9027\n",
      "Epoch 1 Step 9200/11066 - Loss: 1.9034\n",
      "Epoch 1 Step 9300/11066 - Loss: 1.8611\n",
      "Epoch 1 Step 9400/11066 - Loss: 1.8571\n",
      "Epoch 1 Step 9500/11066 - Loss: 1.8449\n",
      "Epoch 1 Step 9600/11066 - Loss: 1.8593\n",
      "Epoch 1 Step 9700/11066 - Loss: 1.8393\n",
      "Epoch 1 Step 9800/11066 - Loss: 1.7827\n",
      "Epoch 1 Step 9900/11066 - Loss: 1.7560\n",
      "Epoch 1 Step 10000/11066 - Loss: 1.7237\n",
      "Epoch 1 Step 10100/11066 - Loss: 1.7232\n",
      "Epoch 1 Step 10200/11066 - Loss: 1.7097\n",
      "Epoch 1 Step 10300/11066 - Loss: 1.6619\n",
      "Epoch 1 Step 10400/11066 - Loss: 1.6597\n",
      "Epoch 1 Step 10500/11066 - Loss: 1.6426\n",
      "Epoch 1 Step 10600/11066 - Loss: 1.6321\n",
      "Epoch 1 Step 10700/11066 - Loss: 1.6024\n",
      "Epoch 1 Step 10800/11066 - Loss: 1.5495\n",
      "Epoch 1 Step 10900/11066 - Loss: 1.5474\n",
      "Epoch 1 Step 11000/11066 - Loss: 1.5170\n",
      "\n",
      "Train Loss: 1.5170\n",
      "Val Loss: 1.2192 | EM: 57.57 | F1: 60.40\n",
      "\n",
      "\n",
      "=== Epoch 2/3 ===\n",
      "Epoch 2 Step 100/11066 - Loss: 1.4782\n",
      "Epoch 2 Step 200/11066 - Loss: 1.4657\n",
      "Epoch 2 Step 300/11066 - Loss: 1.4493\n",
      "Epoch 2 Step 400/11066 - Loss: 1.4240\n",
      "Epoch 2 Step 500/11066 - Loss: 1.3903\n",
      "Epoch 2 Step 600/11066 - Loss: 1.4002\n",
      "Epoch 2 Step 700/11066 - Loss: 1.4328\n",
      "Epoch 2 Step 800/11066 - Loss: 1.4025\n",
      "Epoch 2 Step 900/11066 - Loss: 1.3922\n",
      "Epoch 2 Step 1000/11066 - Loss: 1.4139\n",
      "Epoch 2 Step 1100/11066 - Loss: 1.3718\n",
      "Epoch 2 Step 1200/11066 - Loss: 1.3947\n",
      "Epoch 2 Step 1300/11066 - Loss: 1.3617\n",
      "Epoch 2 Step 1400/11066 - Loss: 1.3252\n",
      "Epoch 2 Step 1500/11066 - Loss: 1.2903\n",
      "Epoch 2 Step 1600/11066 - Loss: 1.2501\n",
      "Epoch 2 Step 1700/11066 - Loss: 1.2292\n",
      "Epoch 2 Step 1800/11066 - Loss: 1.2049\n",
      "Epoch 2 Step 1900/11066 - Loss: 1.2370\n",
      "Epoch 2 Step 2000/11066 - Loss: 1.2263\n",
      "Epoch 2 Step 2100/11066 - Loss: 1.1885\n",
      "Epoch 2 Step 2200/11066 - Loss: 1.1693\n",
      "Epoch 2 Step 2300/11066 - Loss: 1.1405\n",
      "Epoch 2 Step 2400/11066 - Loss: 1.1590\n",
      "Epoch 2 Step 2500/11066 - Loss: 1.1878\n",
      "Epoch 2 Step 2600/11066 - Loss: 1.1684\n",
      "Epoch 2 Step 2700/11066 - Loss: 1.1731\n",
      "Epoch 2 Step 2800/11066 - Loss: 1.1848\n",
      "Epoch 2 Step 2900/11066 - Loss: 1.2085\n",
      "Epoch 2 Step 3000/11066 - Loss: 1.1836\n",
      "Epoch 2 Step 3100/11066 - Loss: 1.1930\n",
      "Epoch 2 Step 3200/11066 - Loss: 1.1852\n",
      "Epoch 2 Step 3300/11066 - Loss: 1.2109\n",
      "Epoch 2 Step 3400/11066 - Loss: 1.2203\n",
      "Epoch 2 Step 3500/11066 - Loss: 1.1897\n",
      "Epoch 2 Step 3600/11066 - Loss: 1.1940\n",
      "Epoch 2 Step 3700/11066 - Loss: 1.1696\n",
      "Epoch 2 Step 3800/11066 - Loss: 1.1826\n",
      "Epoch 2 Step 3900/11066 - Loss: 1.1849\n",
      "Epoch 2 Step 4000/11066 - Loss: 1.2019\n",
      "Epoch 2 Step 4100/11066 - Loss: 1.2336\n",
      "Epoch 2 Step 4200/11066 - Loss: 1.2664\n",
      "Epoch 2 Step 4300/11066 - Loss: 1.2446\n",
      "Epoch 2 Step 4400/11066 - Loss: 1.2754\n",
      "Epoch 2 Step 4500/11066 - Loss: 1.2983\n",
      "Epoch 2 Step 4600/11066 - Loss: 1.2563\n",
      "Epoch 2 Step 4700/11066 - Loss: 1.2746\n",
      "Epoch 2 Step 4800/11066 - Loss: 1.2743\n",
      "Epoch 2 Step 4900/11066 - Loss: 1.2787\n",
      "Epoch 2 Step 5000/11066 - Loss: 1.2556\n",
      "Epoch 2 Step 5100/11066 - Loss: 1.2786\n",
      "Epoch 2 Step 5200/11066 - Loss: 1.3094\n",
      "Epoch 2 Step 5300/11066 - Loss: 1.2971\n",
      "Epoch 2 Step 5400/11066 - Loss: 1.3311\n",
      "Epoch 2 Step 5500/11066 - Loss: 1.3362\n",
      "Epoch 2 Step 5600/11066 - Loss: 1.3097\n",
      "Epoch 2 Step 5700/11066 - Loss: 1.2981\n",
      "Epoch 2 Step 5800/11066 - Loss: 1.2701\n",
      "Epoch 2 Step 5900/11066 - Loss: 1.2557\n",
      "Epoch 2 Step 6000/11066 - Loss: 1.2647\n",
      "Epoch 2 Step 6100/11066 - Loss: 1.2375\n",
      "Epoch 2 Step 6200/11066 - Loss: 1.2123\n",
      "Epoch 2 Step 6300/11066 - Loss: 1.1725\n",
      "Epoch 2 Step 6400/11066 - Loss: 1.1920\n",
      "Epoch 2 Step 6500/11066 - Loss: 1.1644\n",
      "Epoch 2 Step 6600/11066 - Loss: 1.1392\n",
      "Epoch 2 Step 6700/11066 - Loss: 1.1190\n",
      "Epoch 2 Step 6800/11066 - Loss: 1.0819\n",
      "Epoch 2 Step 6900/11066 - Loss: 1.0847\n",
      "Epoch 2 Step 7000/11066 - Loss: 1.0701\n",
      "Epoch 2 Step 7100/11066 - Loss: 1.0867\n",
      "Epoch 2 Step 7200/11066 - Loss: 1.0867\n",
      "Epoch 2 Step 7300/11066 - Loss: 1.0646\n",
      "Epoch 2 Step 7400/11066 - Loss: 1.0441\n",
      "Epoch 2 Step 7500/11066 - Loss: 1.0629\n",
      "Epoch 2 Step 7600/11066 - Loss: 1.0928\n",
      "Epoch 2 Step 7700/11066 - Loss: 1.0724\n",
      "Epoch 2 Step 7800/11066 - Loss: 1.0381\n",
      "Epoch 2 Step 7900/11066 - Loss: 1.0112\n",
      "Epoch 2 Step 8000/11066 - Loss: 0.9765\n",
      "Epoch 2 Step 8100/11066 - Loss: 0.9580\n",
      "Epoch 2 Step 8200/11066 - Loss: 0.9855\n",
      "Epoch 2 Step 8300/11066 - Loss: 0.9709\n",
      "Epoch 2 Step 8400/11066 - Loss: 0.9671\n",
      "Epoch 2 Step 8500/11066 - Loss: 0.9815\n",
      "Epoch 2 Step 8600/11066 - Loss: 0.9914\n",
      "Epoch 2 Step 8700/11066 - Loss: 0.9961\n",
      "Epoch 2 Step 8800/11066 - Loss: 0.9910\n",
      "Epoch 2 Step 8900/11066 - Loss: 0.9822\n",
      "Epoch 2 Step 9000/11066 - Loss: 0.9805\n",
      "Epoch 2 Step 9100/11066 - Loss: 0.9428\n",
      "Epoch 2 Step 9200/11066 - Loss: 0.9263\n",
      "Epoch 2 Step 9300/11066 - Loss: 0.9412\n",
      "Epoch 2 Step 9400/11066 - Loss: 0.9060\n",
      "Epoch 2 Step 9500/11066 - Loss: 0.8861\n",
      "Epoch 2 Step 9600/11066 - Loss: 0.8817\n",
      "Epoch 2 Step 9700/11066 - Loss: 0.9112\n",
      "Epoch 2 Step 9800/11066 - Loss: 0.8748\n",
      "Epoch 2 Step 9900/11066 - Loss: 0.9031\n",
      "Epoch 2 Step 10000/11066 - Loss: 0.9044\n",
      "Epoch 2 Step 10100/11066 - Loss: 0.9372\n",
      "Epoch 2 Step 10200/11066 - Loss: 0.9293\n",
      "Epoch 2 Step 10300/11066 - Loss: 0.9580\n",
      "Epoch 2 Step 10400/11066 - Loss: 0.9656\n",
      "Epoch 2 Step 10500/11066 - Loss: 0.9291\n",
      "Epoch 2 Step 10600/11066 - Loss: 0.9605\n",
      "Epoch 2 Step 10700/11066 - Loss: 0.9579\n",
      "Epoch 2 Step 10800/11066 - Loss: 0.9966\n",
      "Epoch 2 Step 10900/11066 - Loss: 1.0300\n",
      "Epoch 2 Step 11000/11066 - Loss: 1.0560\n",
      "\n",
      "Train Loss: 1.0560\n",
      "Val Loss: 0.8088 | EM: 61.53 | F1: 61.85\n",
      "\n",
      "\n",
      "=== Epoch 3/3 ===\n",
      "Epoch 3 Step 100/11066 - Loss: 1.0945\n",
      "Epoch 3 Step 200/11066 - Loss: 1.0628\n",
      "Epoch 3 Step 300/11066 - Loss: 1.0566\n",
      "Epoch 3 Step 400/11066 - Loss: 1.0389\n",
      "Epoch 3 Step 500/11066 - Loss: 1.0239\n",
      "Epoch 3 Step 600/11066 - Loss: 1.0175\n",
      "Epoch 3 Step 700/11066 - Loss: 1.0228\n",
      "Epoch 3 Step 800/11066 - Loss: 1.0238\n",
      "Epoch 3 Step 900/11066 - Loss: 1.0046\n",
      "Epoch 3 Step 1000/11066 - Loss: 1.0445\n",
      "Epoch 3 Step 1100/11066 - Loss: 1.0426\n",
      "Epoch 3 Step 1200/11066 - Loss: 1.0735\n",
      "Epoch 3 Step 1300/11066 - Loss: 1.1137\n",
      "Epoch 3 Step 1400/11066 - Loss: 1.1342\n",
      "Epoch 3 Step 1500/11066 - Loss: 1.1375\n",
      "Epoch 3 Step 1600/11066 - Loss: 1.1749\n",
      "Epoch 3 Step 1700/11066 - Loss: 1.1998\n",
      "Epoch 3 Step 1800/11066 - Loss: 1.1800\n",
      "Epoch 3 Step 1900/11066 - Loss: 1.2105\n",
      "Epoch 3 Step 2000/11066 - Loss: 1.2256\n",
      "Epoch 3 Step 2100/11066 - Loss: 1.2421\n",
      "Epoch 3 Step 2200/11066 - Loss: 1.2308\n",
      "Epoch 3 Step 2300/11066 - Loss: 1.2140\n",
      "Epoch 3 Step 2400/11066 - Loss: 1.2156\n",
      "Epoch 3 Step 2500/11066 - Loss: 1.2298\n",
      "Epoch 3 Step 2600/11066 - Loss: 1.1891\n",
      "Epoch 3 Step 2700/11066 - Loss: 1.1605\n",
      "Epoch 3 Step 2800/11066 - Loss: 1.1583\n",
      "Epoch 3 Step 2900/11066 - Loss: 1.1561\n",
      "Epoch 3 Step 3000/11066 - Loss: 1.1377\n",
      "Epoch 3 Step 3100/11066 - Loss: 1.1250\n",
      "Epoch 3 Step 3200/11066 - Loss: 1.0916\n",
      "Epoch 3 Step 3300/11066 - Loss: 1.0550\n",
      "Epoch 3 Step 3400/11066 - Loss: 1.0694\n",
      "Epoch 3 Step 3500/11066 - Loss: 1.0706\n",
      "Epoch 3 Step 3600/11066 - Loss: 1.0718\n",
      "Epoch 3 Step 3700/11066 - Loss: 1.1121\n",
      "Epoch 3 Step 3800/11066 - Loss: 1.1058\n",
      "Epoch 3 Step 3900/11066 - Loss: 1.1074\n",
      "Epoch 3 Step 4000/11066 - Loss: 1.0683\n",
      "Epoch 3 Step 4100/11066 - Loss: 1.0552\n",
      "Epoch 3 Step 4200/11066 - Loss: 1.0416\n",
      "Epoch 3 Step 4300/11066 - Loss: 1.0229\n",
      "Epoch 3 Step 4400/11066 - Loss: 0.9914\n",
      "Epoch 3 Step 4500/11066 - Loss: 0.9911\n",
      "Epoch 3 Step 4600/11066 - Loss: 1.0185\n",
      "Epoch 3 Step 4700/11066 - Loss: 1.0266\n",
      "Epoch 3 Step 4800/11066 - Loss: 1.0406\n",
      "Epoch 3 Step 4900/11066 - Loss: 1.0602\n",
      "Epoch 3 Step 5000/11066 - Loss: 1.0418\n",
      "Epoch 3 Step 5100/11066 - Loss: 1.0210\n",
      "Epoch 3 Step 5200/11066 - Loss: 1.0489\n",
      "Epoch 3 Step 5300/11066 - Loss: 1.0565\n",
      "Epoch 3 Step 5400/11066 - Loss: 1.0325\n",
      "Epoch 3 Step 5500/11066 - Loss: 1.0288\n",
      "Epoch 3 Step 5600/11066 - Loss: 1.0061\n",
      "Epoch 3 Step 5700/11066 - Loss: 0.9845\n",
      "Epoch 3 Step 5800/11066 - Loss: 0.9756\n",
      "Epoch 3 Step 5900/11066 - Loss: 0.9862\n",
      "Epoch 3 Step 6000/11066 - Loss: 0.9530\n",
      "Epoch 3 Step 6100/11066 - Loss: 0.9880\n",
      "Epoch 3 Step 6200/11066 - Loss: 1.0237\n",
      "Epoch 3 Step 6300/11066 - Loss: 1.0413\n",
      "Epoch 3 Step 6400/11066 - Loss: 1.0403\n",
      "Epoch 3 Step 6500/11066 - Loss: 1.0515\n",
      "Epoch 3 Step 6600/11066 - Loss: 1.0181\n",
      "Epoch 3 Step 6700/11066 - Loss: 0.9840\n",
      "Epoch 3 Step 6800/11066 - Loss: 0.9471\n",
      "Epoch 3 Step 6900/11066 - Loss: 0.9395\n",
      "Epoch 3 Step 7000/11066 - Loss: 0.9249\n",
      "Epoch 3 Step 7100/11066 - Loss: 0.9422\n",
      "Epoch 3 Step 7200/11066 - Loss: 0.9401\n",
      "Epoch 3 Step 7300/11066 - Loss: 0.9303\n",
      "Epoch 3 Step 7400/11066 - Loss: 0.9046\n",
      "Epoch 3 Step 7500/11066 - Loss: 0.9220\n",
      "Epoch 3 Step 7600/11066 - Loss: 0.9039\n",
      "Epoch 3 Step 7700/11066 - Loss: 0.9262\n",
      "Epoch 3 Step 7800/11066 - Loss: 0.8967\n",
      "Epoch 3 Step 7900/11066 - Loss: 0.8597\n",
      "Epoch 3 Step 8000/11066 - Loss: 0.8850\n",
      "Epoch 3 Step 8100/11066 - Loss: 0.9007\n",
      "Epoch 3 Step 8200/11066 - Loss: 0.9152\n",
      "Epoch 3 Step 8300/11066 - Loss: 0.8816\n",
      "Epoch 3 Step 8400/11066 - Loss: 0.8612\n",
      "Epoch 3 Step 8500/11066 - Loss: 0.8374\n",
      "Epoch 3 Step 8600/11066 - Loss: 0.8045\n",
      "Epoch 3 Step 8700/11066 - Loss: 0.7730\n",
      "Epoch 3 Step 8800/11066 - Loss: 0.7667\n",
      "Epoch 3 Step 8900/11066 - Loss: 0.7762\n",
      "Epoch 3 Step 9000/11066 - Loss: 0.7520\n",
      "Epoch 3 Step 9100/11066 - Loss: 0.7741\n",
      "Epoch 3 Step 9200/11066 - Loss: 0.8169\n",
      "Epoch 3 Step 9300/11066 - Loss: 0.8444\n",
      "Epoch 3 Step 9400/11066 - Loss: 0.8202\n",
      "Epoch 3 Step 9500/11066 - Loss: 0.8575\n",
      "Epoch 3 Step 9600/11066 - Loss: 0.8956\n",
      "Epoch 3 Step 9700/11066 - Loss: 0.8624\n",
      "Epoch 3 Step 9800/11066 - Loss: 0.9004\n",
      "Epoch 3 Step 9900/11066 - Loss: 0.9359\n",
      "Epoch 3 Step 10000/11066 - Loss: 0.9546\n",
      "Epoch 3 Step 10100/11066 - Loss: 0.9627\n",
      "Epoch 3 Step 10200/11066 - Loss: 1.0040\n",
      "Epoch 3 Step 10300/11066 - Loss: 1.0226\n",
      "Epoch 3 Step 10400/11066 - Loss: 1.0514\n",
      "Epoch 3 Step 10500/11066 - Loss: 1.0439\n",
      "Epoch 3 Step 10600/11066 - Loss: 1.0314\n",
      "Epoch 3 Step 10700/11066 - Loss: 1.0013\n",
      "Epoch 3 Step 10800/11066 - Loss: 1.0050\n",
      "Epoch 3 Step 10900/11066 - Loss: 0.9737\n",
      "Epoch 3 Step 11000/11066 - Loss: 0.9975\n",
      "\n",
      "Train Loss: 0.9975\n",
      "Val Loss: 0.8375 | EM: 64.68 | F1: 65.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
    "    train_loss = train_epoch(epoch)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        \n",
    "    val_loss, val_metrics = eval_epoch(val_loader, raw_datasets['validation'])\n",
    "    print(f\"Val Loss: {val_loss:.4f} | EM: {val_metrics['exact_match']:.2f} | F1: {val_metrics['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c5c39c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Performance ===\n",
      "Test Loss: 0.7714 | EM: 63.37 | F1: 66.75\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Test Performance ===\")\n",
    "test_loss, test_metrics = eval_epoch(test_loader, raw_datasets['test'])\n",
    "print(f\"Test Loss: {test_loss:.4f} | EM: {test_metrics['exact_match']:.2f} | F1: {test_metrics['f1']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
