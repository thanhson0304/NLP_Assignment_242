{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcc115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizerFast, \n",
    "    EncoderDecoderModel,\n",
    "    BertModel,\n",
    "    BertConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "from bert_score import BERTScorer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adaed992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ca48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\")\n",
    "train_data = dataset['train'].shuffle(seed=42)\n",
    "val_data   = dataset['validation'].shuffle(seed=42)\n",
    "test_data  = dataset['test'].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7eb9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "special_tokens = {\n",
    "    'pad_token': '[PAD]',\n",
    "    'bos_token': '[CLS]',\n",
    "    'eos_token': '[SEP]'\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0087fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_encoder = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config_decoder = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.add_cross_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3c592f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    'bert-base-uncased',\n",
    "    encoder_config=config_encoder,\n",
    "    decoder_config=config_decoder\n",
    ")\n",
    "\n",
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = len(tokenizer)\n",
    "\n",
    "model.config.max_length = 128\n",
    "model.config.min_length = 10\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.early_stopping = True\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac68aeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,537,216 || all params: 253,900,602 || trainable%: 2.5747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EncoderDecoderModel(\n",
       "      (encoder): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): BertPooler(\n",
       "          (dense): lora.Linear(\n",
       "            (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (decoder): BertLMHeadModel(\n",
       "        (bert): BertModel(\n",
       "          (embeddings): BertEmbeddings(\n",
       "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "            (position_embeddings): Embedding(512, 768)\n",
       "            (token_type_embeddings): Embedding(2, 768)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (encoder): BertEncoder(\n",
       "            (layer): ModuleList(\n",
       "              (0-11): 12 x BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSdpaSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (crossattention): BertAttention(\n",
       "                  (self): BertSdpaSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (cls): BertOnlyMLMHead(\n",
       "          (predictions): BertLMPredictionHead(\n",
       "            (transform): BertPredictionHeadTransform(\n",
       "              (dense): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (transform_act_fn): GELUActivation()\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\", \"key\", \"dense\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4d6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.data[idx]['article']\n",
    "        highlights = self.data[idx]['highlights']\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            article,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer(\n",
    "            highlights,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8041aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a280f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataset = SummarizationDataset(train_data, tokenizer)\n",
    "val_dataset   = SummarizationDataset(val_data, tokenizer)\n",
    "test_dataset  = SummarizationDataset(test_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b359457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "epochs = 3\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=total_steps // 10, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7e5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(dataloader, num_samples=100):\n",
    "    model.eval()\n",
    "    generated_summaries = []\n",
    "    reference_summaries = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample_count = 0\n",
    "        for batch in dataloader:\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Generate summaries\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                min_length=10,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Decode generated and reference summaries\n",
    "            for i in range(len(generated_ids)):\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                generated_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "                reference_text = tokenizer.decode(batch['labels'][i][batch['labels'][i] != -100], skip_special_tokens=True)\n",
    "                \n",
    "                generated_summaries.append(generated_text)\n",
    "                reference_summaries.append(reference_text)\n",
    "                sample_count += 1\n",
    "    \n",
    "    return generated_summaries, reference_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6d60366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataloader, dataset_name=\"Validation\"):\n",
    "    print(f\"\\n=== {dataset_name} Evaluation ===\")\n",
    "    generated_summaries, reference_summaries = generate_summaries(dataloader, num_samples=100)\n",
    "    \n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=generated_summaries,\n",
    "        references=reference_summaries,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    P, R, F1 = bert_scorer.score(generated_summaries, reference_summaries)\n",
    "    bert_score = {\n",
    "        'precision': P.mean().item(),\n",
    "        'recall': R.mean().item(),\n",
    "        'f1': F1.mean().item()\n",
    "    }\n",
    "    \n",
    "    print(f\"ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
    "    print(f\"BERTScore F1: {bert_score['f1']:.4f}\")\n",
    "    print(f\"BERTScore Precision: {bert_score['precision']:.4f}\")\n",
    "    print(f\"BERTScore Recall: {bert_score['recall']:.4f}\")\n",
    "    \n",
    "    return rouge_results, bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "181e10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for step, batch in enumerate(dataloader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch} Step {step}/{len(dataloader)} - Loss: {total_loss/step:.4f}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4073789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319d1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 2000/287113 — Loss: 10.4970\n",
      "Epoch 1 Step 4000/287113 — Loss: 10.4915\n",
      "Epoch 1 Step 6000/287113 — Loss: 10.4860\n",
      "Epoch 1 Step 8000/287113 — Loss: 10.4806\n",
      "Epoch 1 Step 10000/287113 — Loss: 10.4751\n",
      "Epoch 1 Step 12000/287113 — Loss: 10.4696\n",
      "Epoch 1 Step 14000/287113 — Loss: 10.4641\n",
      "Epoch 1 Step 16000/287113 — Loss: 10.4586\n",
      "Epoch 1 Step 18000/287113 — Loss: 10.4531\n",
      "Epoch 1 Step 20000/287113 — Loss: 10.4476\n",
      "Epoch 1 Step 22000/287113 — Loss: 10.4422\n",
      "Epoch 1 Step 24000/287113 — Loss: 10.4367\n",
      "Epoch 1 Step 26000/287113 — Loss: 10.4312\n",
      "Epoch 1 Step 28000/287113 — Loss: 10.4257\n",
      "Epoch 1 Step 30000/287113 — Loss: 10.4202\n",
      "Epoch 1 Step 32000/287113 — Loss: 10.4147\n",
      "Epoch 1 Step 34000/287113 — Loss: 10.4092\n",
      "Epoch 1 Step 36000/287113 — Loss: 10.4038\n",
      "Epoch 1 Step 38000/287113 — Loss: 10.3983\n",
      "Epoch 1 Step 40000/287113 — Loss: 10.3928\n",
      "Epoch 1 Step 42000/287113 — Loss: 10.3873\n",
      "Epoch 1 Step 44000/287113 — Loss: 10.3818\n",
      "Epoch 1 Step 46000/287113 — Loss: 10.3763\n",
      "Epoch 1 Step 48000/287113 — Loss: 10.3708\n",
      "Epoch 1 Step 50000/287113 — Loss: 10.3654\n",
      "Epoch 1 Step 52000/287113 — Loss: 10.3599\n",
      "Epoch 1 Step 54000/287113 — Loss: 10.3544\n",
      "Epoch 1 Step 56000/287113 — Loss: 10.3489\n",
      "Epoch 1 Step 58000/287113 — Loss: 10.3434\n",
      "Epoch 1 Step 60000/287113 — Loss: 10.3379\n",
      "Epoch 1 Step 62000/287113 — Loss: 10.3324\n",
      "Epoch 1 Step 64000/287113 — Loss: 10.3270\n",
      "Epoch 1 Step 66000/287113 — Loss: 10.3215\n",
      "Epoch 1 Step 68000/287113 — Loss: 10.3160\n",
      "Epoch 1 Step 70000/287113 — Loss: 10.3105\n",
      "Epoch 1 Step 72000/287113 — Loss: 10.3050\n",
      "Epoch 1 Step 74000/287113 — Loss: 10.2995\n",
      "Epoch 1 Step 76000/287113 — Loss: 10.2940\n",
      "Epoch 1 Step 78000/287113 — Loss: 10.2886\n",
      "Epoch 1 Step 80000/287113 — Loss: 10.2831\n",
      "Epoch 1 Step 82000/287113 — Loss: 10.2776\n",
      "Epoch 1 Step 84000/287113 — Loss: 10.2721\n",
      "Epoch 1 Step 86000/287113 — Loss: 10.2666\n",
      "Epoch 1 Step 88000/287113 — Loss: 10.2611\n",
      "Epoch 1 Step 90000/287113 — Loss: 10.2556\n",
      "Epoch 1 Step 92000/287113 — Loss: 10.2502\n",
      "Epoch 1 Step 94000/287113 — Loss: 10.2447\n",
      "Epoch 1 Step 96000/287113 — Loss: 10.2392\n",
      "Epoch 1 Step 98000/287113 — Loss: 10.2337\n",
      "Epoch 1 Step 100000/287113 — Loss: 10.2282\n",
      "Epoch 1 Step 102000/287113 — Loss: 10.2227\n",
      "Epoch 1 Step 104000/287113 — Loss: 10.2172\n",
      "Epoch 1 Step 106000/287113 — Loss: 10.2118\n",
      "Epoch 1 Step 108000/287113 — Loss: 10.2063\n",
      "Epoch 1 Step 110000/287113 — Loss: 10.2008\n",
      "Epoch 1 Step 112000/287113 — Loss: 10.1953\n",
      "Epoch 1 Step 114000/287113 — Loss: 10.1898\n",
      "Epoch 1 Step 116000/287113 — Loss: 10.1843\n",
      "Epoch 1 Step 118000/287113 — Loss: 10.1788\n",
      "Epoch 1 Step 120000/287113 — Loss: 10.1734\n",
      "Epoch 1 Step 122000/287113 — Loss: 10.1679\n",
      "Epoch 1 Step 124000/287113 — Loss: 10.1624\n",
      "Epoch 1 Step 126000/287113 — Loss: 10.1569\n",
      "Epoch 1 Step 128000/287113 — Loss: 10.1514\n",
      "Epoch 1 Step 130000/287113 — Loss: 10.1459\n",
      "Epoch 1 Step 132000/287113 — Loss: 10.1404\n",
      "Epoch 1 Step 134000/287113 — Loss: 10.1350\n",
      "Epoch 1 Step 136000/287113 — Loss: 10.1295\n",
      "Epoch 1 Step 138000/287113 — Loss: 10.1240\n",
      "Epoch 1 Step 140000/287113 — Loss: 10.1185\n",
      "Epoch 1 Step 142000/287113 — Loss: 10.1130\n",
      "Epoch 1 Step 144000/287113 — Loss: 10.1075\n",
      "Epoch 1 Step 146000/287113 — Loss: 10.1020\n",
      "Epoch 1 Step 148000/287113 — Loss: 10.0966\n",
      "Epoch 1 Step 150000/287113 — Loss: 10.0911\n",
      "Epoch 1 Step 152000/287113 — Loss: 10.0856\n",
      "Epoch 1 Step 154000/287113 — Loss: 10.0801\n",
      "Epoch 1 Step 156000/287113 — Loss: 10.0746\n",
      "Epoch 1 Step 158000/287113 — Loss: 10.0691\n",
      "Epoch 1 Step 160000/287113 — Loss: 10.0636\n",
      "Epoch 1 Step 162000/287113 — Loss: 10.0582\n",
      "Epoch 1 Step 164000/287113 — Loss: 10.0527\n",
      "Epoch 1 Step 166000/287113 — Loss: 10.0472\n",
      "Epoch 1 Step 168000/287113 — Loss: 10.0417\n",
      "Epoch 1 Step 170000/287113 — Loss: 10.0362\n",
      "Epoch 1 Step 172000/287113 — Loss: 10.0307\n",
      "Epoch 1 Step 174000/287113 — Loss: 10.0252\n",
      "Epoch 1 Step 176000/287113 — Loss: 10.0198\n",
      "Epoch 1 Step 178000/287113 — Loss: 10.0143\n",
      "Epoch 1 Step 180000/287113 — Loss: 10.0088\n",
      "Epoch 1 Step 182000/287113 — Loss: 10.0033\n",
      "Epoch 1 Step 184000/287113 — Loss: 9.9978\n",
      "Epoch 1 Step 186000/287113 — Loss: 9.9923\n",
      "Epoch 1 Step 188000/287113 — Loss: 9.9868\n",
      "Epoch 1 Step 190000/287113 — Loss: 9.9814\n",
      "Epoch 1 Step 192000/287113 — Loss: 9.9759\n",
      "Epoch 1 Step 194000/287113 — Loss: 9.9704\n",
      "Epoch 1 Step 196000/287113 — Loss: 9.9649\n",
      "Epoch 1 Step 198000/287113 — Loss: 9.9594\n",
      "Epoch 1 Step 200000/287113 — Loss: 9.9539\n",
      "Epoch 1 Step 202000/287113 — Loss: 9.9484\n",
      "Epoch 1 Step 204000/287113 — Loss: 9.9430\n",
      "Epoch 1 Step 206000/287113 — Loss: 9.9375\n",
      "Epoch 1 Step 208000/287113 — Loss: 9.9320\n",
      "Epoch 1 Step 210000/287113 — Loss: 9.9265\n",
      "Epoch 1 Step 212000/287113 — Loss: 9.9210\n",
      "Epoch 1 Step 214000/287113 — Loss: 9.9155\n",
      "Epoch 1 Step 216000/287113 — Loss: 9.9100\n",
      "Epoch 1 Step 218000/287113 — Loss: 9.9046\n",
      "Epoch 1 Step 220000/287113 — Loss: 9.8991\n",
      "Epoch 1 Step 222000/287113 — Loss: 9.8936\n",
      "Epoch 1 Step 224000/287113 — Loss: 9.8881\n",
      "Epoch 1 Step 226000/287113 — Loss: 9.8826\n",
      "Epoch 1 Step 228000/287113 — Loss: 9.8771\n",
      "Epoch 1 Step 230000/287113 — Loss: 9.8716\n",
      "Epoch 1 Step 232000/287113 — Loss: 9.8662\n",
      "Epoch 1 Step 234000/287113 — Loss: 9.8607\n",
      "Epoch 1 Step 236000/287113 — Loss: 9.8552\n",
      "Epoch 1 Step 238000/287113 — Loss: 9.8497\n",
      "Epoch 1 Step 240000/287113 — Loss: 9.8442\n",
      "Epoch 1 Step 242000/287113 — Loss: 9.8387\n",
      "Epoch 1 Step 244000/287113 — Loss: 9.8333\n",
      "Epoch 1 Step 246000/287113 — Loss: 9.8278\n",
      "Epoch 1 Step 248000/287113 — Loss: 9.8223\n",
      "Epoch 1 Step 250000/287113 — Loss: 9.8168\n",
      "Epoch 1 Step 252000/287113 — Loss: 9.8113\n",
      "Epoch 1 Step 254000/287113 — Loss: 9.8058\n",
      "Epoch 1 Step 256000/287113 — Loss: 9.8003\n",
      "Epoch 1 Step 258000/287113 — Loss: 9.7949\n",
      "Epoch 1 Step 260000/287113 — Loss: 9.7894\n",
      "Epoch 1 Step 262000/287113 — Loss: 9.7839\n",
      "Epoch 1 Step 264000/287113 — Loss: 9.7784\n",
      "Epoch 1 Step 266000/287113 — Loss: 9.7729\n",
      "Epoch 1 Step 268000/287113 — Loss: 9.7674\n",
      "Epoch 1 Step 270000/287113 — Loss: 9.7619\n",
      "Epoch 1 Step 272000/287113 — Loss: 9.7565\n",
      "Epoch 1 Step 274000/287113 — Loss: 9.7510\n",
      "Epoch 1 Step 276000/287113 — Loss: 9.7455\n",
      "Epoch 1 Step 278000/287113 — Loss: 9.7400\n",
      "Epoch 1 Step 280000/287113 — Loss: 9.7345\n",
      "Epoch 1 Step 282000/287113 — Loss: 9.7290\n",
      "Epoch 1 Step 284000/287113 — Loss: 9.7235\n",
      "Epoch 1 Step 286000/287113 — Loss: 9.7181\n",
      "\n",
      "=== Epoch 1/3 ===\n",
      "Train Loss: 9.7150\n",
      " Val Loss:  9.6244\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROUGE-1:            0.4431\n",
      "ROUGE-2:            0.2286\n",
      "ROUGE-L:            0.3939\n",
      "BERTScore Precision:0.7034\n",
      "BERTScore Recall:   0.7286\n",
      "BERTScore F1:       0.7158\n",
      "\n",
      "Epoch 2 Step 2000/287113 — Loss: 8.9167\n",
      "Epoch 2 Step 4000/287113 — Loss: 8.9120\n",
      "Epoch 2 Step 6000/287113 — Loss: 8.9074\n",
      "Epoch 2 Step 8000/287113 — Loss: 8.9027\n",
      "Epoch 2 Step 10000/287113 — Loss: 8.8981\n",
      "Epoch 2 Step 12000/287113 — Loss: 8.8934\n",
      "Epoch 2 Step 14000/287113 — Loss: 8.8887\n",
      "Epoch 2 Step 16000/287113 — Loss: 8.8841\n",
      "Epoch 2 Step 18000/287113 — Loss: 8.8794\n",
      "Epoch 2 Step 20000/287113 — Loss: 8.8748\n",
      "Epoch 2 Step 22000/287113 — Loss: 8.8701\n",
      "Epoch 2 Step 24000/287113 — Loss: 8.8655\n",
      "Epoch 2 Step 26000/287113 — Loss: 8.8608\n",
      "Epoch 2 Step 28000/287113 — Loss: 8.8561\n",
      "Epoch 2 Step 30000/287113 — Loss: 8.8515\n",
      "Epoch 2 Step 32000/287113 — Loss: 8.8468\n",
      "Epoch 2 Step 34000/287113 — Loss: 8.8422\n",
      "Epoch 2 Step 36000/287113 — Loss: 8.8375\n",
      "Epoch 2 Step 38000/287113 — Loss: 8.8328\n",
      "Epoch 2 Step 40000/287113 — Loss: 8.8282\n",
      "Epoch 2 Step 42000/287113 — Loss: 8.8235\n",
      "Epoch 2 Step 44000/287113 — Loss: 8.8189\n",
      "Epoch 2 Step 46000/287113 — Loss: 8.8142\n",
      "Epoch 2 Step 48000/287113 — Loss: 8.8095\n",
      "Epoch 2 Step 50000/287113 — Loss: 8.8049\n",
      "Epoch 2 Step 52000/287113 — Loss: 8.8002\n",
      "Epoch 2 Step 54000/287113 — Loss: 8.7956\n",
      "Epoch 2 Step 56000/287113 — Loss: 8.7909\n",
      "Epoch 2 Step 58000/287113 — Loss: 8.7862\n",
      "Epoch 2 Step 60000/287113 — Loss: 8.7816\n",
      "Epoch 2 Step 62000/287113 — Loss: 8.7769\n",
      "Epoch 2 Step 64000/287113 — Loss: 8.7723\n",
      "Epoch 2 Step 66000/287113 — Loss: 8.7676\n",
      "Epoch 2 Step 68000/287113 — Loss: 8.7629\n",
      "Epoch 2 Step 70000/287113 — Loss: 8.7583\n",
      "Epoch 2 Step 72000/287113 — Loss: 8.7536\n",
      "Epoch 2 Step 74000/287113 — Loss: 8.7490\n",
      "Epoch 2 Step 76000/287113 — Loss: 8.7443\n",
      "Epoch 2 Step 78000/287113 — Loss: 8.7396\n",
      "Epoch 2 Step 80000/287113 — Loss: 8.7350\n",
      "Epoch 2 Step 82000/287113 — Loss: 8.7303\n",
      "Epoch 2 Step 84000/287113 — Loss: 8.7257\n",
      "Epoch 2 Step 86000/287113 — Loss: 8.7210\n",
      "Epoch 2 Step 88000/287113 — Loss: 8.7163\n",
      "Epoch 2 Step 90000/287113 — Loss: 8.7117\n",
      "Epoch 2 Step 92000/287113 — Loss: 8.7070\n",
      "Epoch 2 Step 94000/287113 — Loss: 8.7024\n",
      "Epoch 2 Step 96000/287113 — Loss: 8.6977\n",
      "Epoch 2 Step 98000/287113 — Loss: 8.6930\n",
      "Epoch 2 Step 100000/287113 — Loss: 8.6884\n",
      "Epoch 2 Step 102000/287113 — Loss: 8.6837\n",
      "Epoch 2 Step 104000/287113 — Loss: 8.6791\n",
      "Epoch 2 Step 106000/287113 — Loss: 8.6744\n",
      "Epoch 2 Step 108000/287113 — Loss: 8.6697\n",
      "Epoch 2 Step 110000/287113 — Loss: 8.6651\n",
      "Epoch 2 Step 112000/287113 — Loss: 8.6604\n",
      "Epoch 2 Step 114000/287113 — Loss: 8.6558\n",
      "Epoch 2 Step 116000/287113 — Loss: 8.6511\n",
      "Epoch 2 Step 118000/287113 — Loss: 8.6465\n",
      "Epoch 2 Step 120000/287113 — Loss: 8.6418\n",
      "Epoch 2 Step 122000/287113 — Loss: 8.6371\n",
      "Epoch 2 Step 124000/287113 — Loss: 8.6325\n",
      "Epoch 2 Step 126000/287113 — Loss: 8.6278\n",
      "Epoch 2 Step 128000/287113 — Loss: 8.6232\n",
      "Epoch 2 Step 130000/287113 — Loss: 8.6185\n",
      "Epoch 2 Step 132000/287113 — Loss: 8.6138\n",
      "Epoch 2 Step 134000/287113 — Loss: 8.6092\n",
      "Epoch 2 Step 136000/287113 — Loss: 8.6045\n",
      "Epoch 2 Step 138000/287113 — Loss: 8.5999\n",
      "Epoch 2 Step 140000/287113 — Loss: 8.5952\n",
      "Epoch 2 Step 142000/287113 — Loss: 8.5905\n",
      "Epoch 2 Step 144000/287113 — Loss: 8.5859\n",
      "Epoch 2 Step 146000/287113 — Loss: 8.5812\n",
      "Epoch 2 Step 148000/287113 — Loss: 8.5766\n",
      "Epoch 2 Step 150000/287113 — Loss: 8.5719\n",
      "Epoch 2 Step 152000/287113 — Loss: 8.5672\n",
      "Epoch 2 Step 154000/287113 — Loss: 8.5626\n",
      "Epoch 2 Step 156000/287113 — Loss: 8.5579\n",
      "Epoch 2 Step 158000/287113 — Loss: 8.5533\n",
      "Epoch 2 Step 160000/287113 — Loss: 8.5486\n",
      "Epoch 2 Step 162000/287113 — Loss: 8.5439\n",
      "Epoch 2 Step 164000/287113 — Loss: 8.5393\n",
      "Epoch 2 Step 166000/287113 — Loss: 8.5346\n",
      "Epoch 2 Step 168000/287113 — Loss: 8.5300\n",
      "Epoch 2 Step 170000/287113 — Loss: 8.5253\n",
      "Epoch 2 Step 172000/287113 — Loss: 8.5206\n",
      "Epoch 2 Step 174000/287113 — Loss: 8.5160\n",
      "Epoch 2 Step 176000/287113 — Loss: 8.5113\n",
      "Epoch 2 Step 178000/287113 — Loss: 8.5067\n",
      "Epoch 2 Step 180000/287113 — Loss: 8.5020\n",
      "Epoch 2 Step 182000/287113 — Loss: 8.4973\n",
      "Epoch 2 Step 184000/287113 — Loss: 8.4927\n",
      "Epoch 2 Step 186000/287113 — Loss: 8.4880\n",
      "Epoch 2 Step 188000/287113 — Loss: 8.4834\n",
      "Epoch 2 Step 190000/287113 — Loss: 8.4787\n",
      "Epoch 2 Step 192000/287113 — Loss: 8.4740\n",
      "Epoch 2 Step 194000/287113 — Loss: 8.4694\n",
      "Epoch 2 Step 196000/287113 — Loss: 8.4647\n",
      "Epoch 2 Step 198000/287113 — Loss: 8.4601\n",
      "Epoch 2 Step 200000/287113 — Loss: 8.4554\n",
      "Epoch 2 Step 202000/287113 — Loss: 8.4507\n",
      "Epoch 2 Step 204000/287113 — Loss: 8.4461\n",
      "Epoch 2 Step 206000/287113 — Loss: 8.4414\n",
      "Epoch 2 Step 208000/287113 — Loss: 8.4368\n",
      "Epoch 2 Step 210000/287113 — Loss: 8.4321\n",
      "Epoch 2 Step 212000/287113 — Loss: 8.4274\n",
      "Epoch 2 Step 214000/287113 — Loss: 8.4228\n",
      "Epoch 2 Step 216000/287113 — Loss: 8.4181\n",
      "Epoch 2 Step 218000/287113 — Loss: 8.4135\n",
      "Epoch 2 Step 220000/287113 — Loss: 8.4088\n",
      "Epoch 2 Step 222000/287113 — Loss: 8.4042\n",
      "Epoch 2 Step 224000/287113 — Loss: 8.3995\n",
      "Epoch 2 Step 226000/287113 — Loss: 8.3948\n",
      "Epoch 2 Step 228000/287113 — Loss: 8.3902\n",
      "Epoch 2 Step 230000/287113 — Loss: 8.3855\n",
      "Epoch 2 Step 232000/287113 — Loss: 8.3809\n",
      "Epoch 2 Step 234000/287113 — Loss: 8.3762\n",
      "Epoch 2 Step 236000/287113 — Loss: 8.3715\n",
      "Epoch 2 Step 238000/287113 — Loss: 8.3669\n",
      "Epoch 2 Step 240000/287113 — Loss: 8.3622\n",
      "Epoch 2 Step 242000/287113 — Loss: 8.3576\n",
      "Epoch 2 Step 244000/287113 — Loss: 8.3529\n",
      "Epoch 2 Step 246000/287113 — Loss: 8.3482\n",
      "Epoch 2 Step 248000/287113 — Loss: 8.3436\n",
      "Epoch 2 Step 250000/287113 — Loss: 8.3389\n",
      "Epoch 2 Step 252000/287113 — Loss: 8.3343\n",
      "Epoch 2 Step 254000/287113 — Loss: 8.3296\n",
      "Epoch 2 Step 256000/287113 — Loss: 8.3249\n",
      "Epoch 2 Step 258000/287113 — Loss: 8.3203\n",
      "Epoch 2 Step 260000/287113 — Loss: 8.3156\n",
      "Epoch 2 Step 262000/287113 — Loss: 8.3110\n",
      "Epoch 2 Step 264000/287113 — Loss: 8.3063\n",
      "Epoch 2 Step 266000/287113 — Loss: 8.3016\n",
      "Epoch 2 Step 268000/287113 — Loss: 8.2970\n",
      "Epoch 2 Step 270000/287113 — Loss: 8.2923\n",
      "Epoch 2 Step 272000/287113 — Loss: 8.2877\n",
      "Epoch 2 Step 274000/287113 — Loss: 8.2830\n",
      "Epoch 2 Step 276000/287113 — Loss: 8.2783\n",
      "Epoch 2 Step 278000/287113 — Loss: 8.2737\n",
      "Epoch 2 Step 280000/287113 — Loss: 8.2690\n",
      "Epoch 2 Step 282000/287113 — Loss: 8.2644\n",
      "Epoch 2 Step 284000/287113 — Loss: 8.2597\n",
      "Epoch 2 Step 286000/287113 — Loss: 8.2550\n",
      "\n",
      "=== Epoch 2/3 ===\n",
      "Train Loss: 8.2525\n",
      " Val Loss:  8.1154\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROUGE-1:            0.4685\n",
      "ROUGE-2:            0.3549\n",
      "ROUGE-L:            0.4221\n",
      "BERTScore Precision:0.7724\n",
      "BERTScore Recall:   0.7785\n",
      "BERTScore F1:       0.7754\n",
      "\n",
      "Epoch 3 Step 2000/287113 — Loss: 7.5708\n",
      "Epoch 3 Step 4000/287113 — Loss: 7.5669\n",
      "Epoch 3 Step 6000/287113 — Loss: 7.5629\n",
      "Epoch 3 Step 8000/287113 — Loss: 7.5590\n",
      "Epoch 3 Step 10000/287113 — Loss: 7.5550\n",
      "Epoch 3 Step 12000/287113 — Loss: 7.5511\n",
      "Epoch 3 Step 14000/287113 — Loss: 7.5471\n",
      "Epoch 3 Step 16000/287113 — Loss: 7.5432\n",
      "Epoch 3 Step 18000/287113 — Loss: 7.5392\n",
      "Epoch 3 Step 20000/287113 — Loss: 7.5353\n",
      "Epoch 3 Step 22000/287113 — Loss: 7.5313\n",
      "Epoch 3 Step 24000/287113 — Loss: 7.5273\n",
      "Epoch 3 Step 26000/287113 — Loss: 7.5234\n",
      "Epoch 3 Step 28000/287113 — Loss: 7.5194\n",
      "Epoch 3 Step 30000/287113 — Loss: 7.5155\n",
      "Epoch 3 Step 32000/287113 — Loss: 7.5115\n",
      "Epoch 3 Step 34000/287113 — Loss: 7.5076\n",
      "Epoch 3 Step 36000/287113 — Loss: 7.5036\n",
      "Epoch 3 Step 38000/287113 — Loss: 7.4996\n",
      "Epoch 3 Step 40000/287113 — Loss: 7.4957\n",
      "Epoch 3 Step 42000/287113 — Loss: 7.4917\n",
      "Epoch 3 Step 44000/287113 — Loss: 7.4878\n",
      "Epoch 3 Step 46000/287113 — Loss: 7.4838\n",
      "Epoch 3 Step 48000/287113 — Loss: 7.4799\n",
      "Epoch 3 Step 50000/287113 — Loss: 7.4759\n",
      "Epoch 3 Step 52000/287113 — Loss: 7.4719\n",
      "Epoch 3 Step 54000/287113 — Loss: 7.4680\n",
      "Epoch 3 Step 56000/287113 — Loss: 7.4640\n",
      "Epoch 3 Step 58000/287113 — Loss: 7.4601\n",
      "Epoch 3 Step 60000/287113 — Loss: 7.4561\n",
      "Epoch 3 Step 62000/287113 — Loss: 7.4522\n",
      "Epoch 3 Step 64000/287113 — Loss: 7.4482\n",
      "Epoch 3 Step 66000/287113 — Loss: 7.4443\n",
      "Epoch 3 Step 68000/287113 — Loss: 7.4403\n",
      "Epoch 3 Step 70000/287113 — Loss: 7.4363\n",
      "Epoch 3 Step 72000/287113 — Loss: 7.4324\n",
      "Epoch 3 Step 74000/287113 — Loss: 7.4284\n",
      "Epoch 3 Step 76000/287113 — Loss: 7.4245\n",
      "Epoch 3 Step 78000/287113 — Loss: 7.4205\n",
      "Epoch 3 Step 80000/287113 — Loss: 7.4166\n",
      "Epoch 3 Step 82000/287113 — Loss: 7.4126\n",
      "Epoch 3 Step 84000/287113 — Loss: 7.4087\n",
      "Epoch 3 Step 86000/287113 — Loss: 7.4047\n",
      "Epoch 3 Step 88000/287113 — Loss: 7.4007\n",
      "Epoch 3 Step 90000/287113 — Loss: 7.3968\n",
      "Epoch 3 Step 92000/287113 — Loss: 7.3928\n",
      "Epoch 3 Step 94000/287113 — Loss: 7.3889\n",
      "Epoch 3 Step 96000/287113 — Loss: 7.3849\n",
      "Epoch 3 Step 98000/287113 — Loss: 7.3810\n",
      "Epoch 3 Step 100000/287113 — Loss: 7.3770\n",
      "Epoch 3 Step 102000/287113 — Loss: 7.3730\n",
      "Epoch 3 Step 104000/287113 — Loss: 7.3691\n",
      "Epoch 3 Step 106000/287113 — Loss: 7.3651\n",
      "Epoch 3 Step 108000/287113 — Loss: 7.3612\n",
      "Epoch 3 Step 110000/287113 — Loss: 7.3572\n",
      "Epoch 3 Step 112000/287113 — Loss: 7.3533\n",
      "Epoch 3 Step 114000/287113 — Loss: 7.3493\n",
      "Epoch 3 Step 116000/287113 — Loss: 7.3454\n",
      "Epoch 3 Step 118000/287113 — Loss: 7.3414\n",
      "Epoch 3 Step 120000/287113 — Loss: 7.3374\n",
      "Epoch 3 Step 122000/287113 — Loss: 7.3335\n",
      "Epoch 3 Step 124000/287113 — Loss: 7.3295\n",
      "Epoch 3 Step 126000/287113 — Loss: 7.3256\n",
      "Epoch 3 Step 128000/287113 — Loss: 7.3216\n",
      "Epoch 3 Step 130000/287113 — Loss: 7.3177\n",
      "Epoch 3 Step 132000/287113 — Loss: 7.3137\n",
      "Epoch 3 Step 134000/287113 — Loss: 7.3098\n",
      "Epoch 3 Step 136000/287113 — Loss: 7.3058\n",
      "Epoch 3 Step 138000/287113 — Loss: 7.3018\n",
      "Epoch 3 Step 140000/287113 — Loss: 7.2979\n",
      "Epoch 3 Step 142000/287113 — Loss: 7.2939\n",
      "Epoch 3 Step 144000/287113 — Loss: 7.2900\n",
      "Epoch 3 Step 146000/287113 — Loss: 7.2860\n",
      "Epoch 3 Step 148000/287113 — Loss: 7.2821\n",
      "Epoch 3 Step 150000/287113 — Loss: 7.2781\n",
      "Epoch 3 Step 152000/287113 — Loss: 7.2741\n",
      "Epoch 3 Step 154000/287113 — Loss: 7.2702\n",
      "Epoch 3 Step 156000/287113 — Loss: 7.2662\n",
      "Epoch 3 Step 158000/287113 — Loss: 7.2623\n",
      "Epoch 3 Step 160000/287113 — Loss: 7.2583\n",
      "Epoch 3 Step 162000/287113 — Loss: 7.2544\n",
      "Epoch 3 Step 164000/287113 — Loss: 7.2504\n",
      "Epoch 3 Step 166000/287113 — Loss: 7.2465\n",
      "Epoch 3 Step 168000/287113 — Loss: 7.2425\n",
      "Epoch 3 Step 170000/287113 — Loss: 7.2385\n",
      "Epoch 3 Step 172000/287113 — Loss: 7.2346\n",
      "Epoch 3 Step 174000/287113 — Loss: 7.2306\n",
      "Epoch 3 Step 176000/287113 — Loss: 7.2267\n",
      "Epoch 3 Step 178000/287113 — Loss: 7.2227\n",
      "Epoch 3 Step 180000/287113 — Loss: 7.2188\n",
      "Epoch 3 Step 182000/287113 — Loss: 7.2148\n",
      "Epoch 3 Step 184000/287113 — Loss: 7.2109\n",
      "Epoch 3 Step 186000/287113 — Loss: 7.2069\n",
      "Epoch 3 Step 188000/287113 — Loss: 7.2029\n",
      "Epoch 3 Step 190000/287113 — Loss: 7.1990\n",
      "Epoch 3 Step 192000/287113 — Loss: 7.1950\n",
      "Epoch 3 Step 194000/287113 — Loss: 7.1911\n",
      "Epoch 3 Step 196000/287113 — Loss: 7.1871\n",
      "Epoch 3 Step 198000/287113 — Loss: 7.1832\n",
      "Epoch 3 Step 200000/287113 — Loss: 7.1792\n",
      "Epoch 3 Step 202000/287113 — Loss: 7.1752\n",
      "Epoch 3 Step 204000/287113 — Loss: 7.1713\n",
      "Epoch 3 Step 206000/287113 — Loss: 7.1673\n",
      "Epoch 3 Step 208000/287113 — Loss: 7.1634\n",
      "Epoch 3 Step 210000/287113 — Loss: 7.1594\n",
      "Epoch 3 Step 212000/287113 — Loss: 7.1555\n",
      "Epoch 3 Step 214000/287113 — Loss: 7.1515\n",
      "Epoch 3 Step 216000/287113 — Loss: 7.1476\n",
      "Epoch 3 Step 218000/287113 — Loss: 7.1436\n",
      "Epoch 3 Step 220000/287113 — Loss: 7.1396\n",
      "Epoch 3 Step 222000/287113 — Loss: 7.1357\n",
      "Epoch 3 Step 224000/287113 — Loss: 7.1317\n",
      "Epoch 3 Step 226000/287113 — Loss: 7.1278\n",
      "Epoch 3 Step 228000/287113 — Loss: 7.1238\n",
      "Epoch 3 Step 230000/287113 — Loss: 7.1199\n",
      "Epoch 3 Step 232000/287113 — Loss: 7.1159\n",
      "Epoch 3 Step 234000/287113 — Loss: 7.1119\n",
      "Epoch 3 Step 236000/287113 — Loss: 7.1080\n",
      "Epoch 3 Step 238000/287113 — Loss: 7.1040\n",
      "Epoch 3 Step 240000/287113 — Loss: 7.1001\n",
      "Epoch 3 Step 242000/287113 — Loss: 7.0961\n",
      "Epoch 3 Step 244000/287113 — Loss: 7.0922\n",
      "Epoch 3 Step 246000/287113 — Loss: 7.0882\n",
      "Epoch 3 Step 248000/287113 — Loss: 7.0843\n",
      "Epoch 3 Step 250000/287113 — Loss: 7.0803\n",
      "Epoch 3 Step 252000/287113 — Loss: 7.0763\n",
      "Epoch 3 Step 254000/287113 — Loss: 7.0724\n",
      "Epoch 3 Step 256000/287113 — Loss: 7.0684\n",
      "Epoch 3 Step 258000/287113 — Loss: 7.0645\n",
      "Epoch 3 Step 260000/287113 — Loss: 7.0605\n",
      "Epoch 3 Step 262000/287113 — Loss: 7.0566\n",
      "Epoch 3 Step 264000/287113 — Loss: 7.0526\n",
      "Epoch 3 Step 266000/287113 — Loss: 7.0487\n",
      "Epoch 3 Step 268000/287113 — Loss: 7.0447\n",
      "Epoch 3 Step 270000/287113 — Loss: 7.0407\n",
      "Epoch 3 Step 272000/287113 — Loss: 7.0368\n",
      "Epoch 3 Step 274000/287113 — Loss: 7.0328\n",
      "Epoch 3 Step 276000/287113 — Loss: 7.0289\n",
      "Epoch 3 Step 278000/287113 — Loss: 7.0249\n",
      "Epoch 3 Step 280000/287113 — Loss: 7.0210\n",
      "Epoch 3 Step 282000/287113 — Loss: 7.0170\n",
      "Epoch 3 Step 284000/287113 — Loss: 7.0130\n",
      "Epoch 3 Step 286000/287113 — Loss: 7.0091\n",
      "\n",
      "=== Epoch 3/3 ===\n",
      "Train Loss: 7.0069\n",
      " Val Loss:  6.8735\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROUGE-1:            0.4506\n",
      "ROUGE-2:            0.3772\n",
      "ROUGE-L:            0.4876\n",
      "BERTScore Precision:0.7958\n",
      "BERTScore Recall:   0.6819\n",
      "BERTScore F1:       0.7345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
    "    train_loss = train_epoch(train_loader, epoch)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_loss = eval_epoch(val_loader)\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    evaluate_model(val_loader, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c95f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.2265\n",
      "=== Test Evaluation ===\n",
      "ROUGE-1: 0.2688\n",
      "ROUGE-2: 0.1269\n",
      "ROUGE-L: 0.2452\n",
      "BERTScore F1: 0.5623\n",
      "BERTScore Precision: 0.5514\n",
      "BERTScore Recall: 0.5568\n"
     ]
    }
   ],
   "source": [
    "test_loss = eval_epoch(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "rouge_results, bert_score = evaluate_model(test_loader, \"Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
