{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebdc3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizerFast, \n",
    "    EncoderDecoderModel,\n",
    "    BertModel,\n",
    "    BertConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from bert_score import BERTScorer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1323dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1b7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\")\n",
    "train_data = dataset['train'].shuffle(seed=42).select(range(100))  # Increased sample size\n",
    "val_data   = dataset['validation'].shuffle(seed=42).select(range(50))\n",
    "test_data  = dataset['test'].shuffle(seed=42).select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "029d2c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "special_tokens = {\n",
    "    'pad_token': '[PAD]',\n",
    "    'bos_token': '[CLS]',\n",
    "    'eos_token': '[SEP]'\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e58f1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config_encoder = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config_decoder = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.add_cross_attention = True\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    'bert-base-uncased',\n",
    "    encoder_config=config_encoder,\n",
    "    decoder_config=config_decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef659cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = len(tokenizer)\n",
    "\n",
    "model.config.max_length = 128\n",
    "model.config.min_length = 10\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.early_stopping = True\n",
    "model.config.num_beams = 4\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9857b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.data[idx]['article']\n",
    "        highlights = self.data[idx]['highlights']\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            article,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer(\n",
    "            highlights,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e27cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8cd8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SummarizationDataset(train_data, tokenizer)\n",
    "val_dataset   = SummarizationDataset(val_data, tokenizer)\n",
    "test_dataset  = SummarizationDataset(test_data, tokenizer)\n",
    "\n",
    "batch_size = 4 \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "257fcd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "epochs = 3\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=total_steps // 10, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "589cc439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(dataloader, num_samples=50):\n",
    "    model.eval()\n",
    "    generated_summaries = []\n",
    "    reference_summaries = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample_count = 0\n",
    "        for batch in dataloader:\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                min_length=10,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            for i in range(len(generated_ids)):\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                generated_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "                reference_text = tokenizer.decode(batch['labels'][i][batch['labels'][i] != -100], skip_special_tokens=True)\n",
    "                \n",
    "                generated_summaries.append(generated_text)\n",
    "                reference_summaries.append(reference_text)\n",
    "                sample_count += 1\n",
    "    \n",
    "    return generated_summaries, reference_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2eff8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataloader, dataset_name=\"Validation\"):\n",
    "    print(f\"\\n=== {dataset_name} Evaluation ===\")\n",
    "    generated_summaries, reference_summaries = generate_summaries(dataloader, num_samples=50)\n",
    "    \n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=generated_summaries,\n",
    "        references=reference_summaries,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    P, R, F1 = bert_scorer.score(generated_summaries, reference_summaries)\n",
    "    bert_score = {\n",
    "        'precision': P.mean().item(),\n",
    "        'recall': R.mean().item(),\n",
    "        'f1': F1.mean().item()\n",
    "    }\n",
    "    \n",
    "    print(f\"ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
    "    print(f\"BERTScore F1: {bert_score['f1']:.4f}\")\n",
    "    print(f\"BERTScore Precision: {bert_score['precision']:.4f}\")\n",
    "    print(f\"BERTScore Recall: {bert_score['recall']:.4f}\")\n",
    "    \n",
    "    return rouge_results, bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11186e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for step, batch in enumerate(dataloader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % 100 == 0:  \n",
    "            avg_loss = total_loss / step\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"Epoch {epoch} Step {step}/{len(dataloader)} - Loss: {avg_loss:.4f} - LR: {current_lr:.2e}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fc468a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 2000/287113 — Loss: 10.4971\n",
      "Epoch 1 Step 4000/287113 — Loss: 10.4916\n",
      "Epoch 1 Step 6000/287113 — Loss: 10.4861\n",
      "Epoch 1 Step 8000/287113 — Loss: 10.4806\n",
      "Epoch 1 Step 10000/287113 — Loss: 10.4751\n",
      "Epoch 1 Step 12000/287113 — Loss: 10.4696\n",
      "Epoch 1 Step 14000/287113 — Loss: 10.4641\n",
      "Epoch 1 Step 16000/287113 — Loss: 10.4586\n",
      "Epoch 1 Step 18000/287113 — Loss: 10.4531\n",
      "Epoch 1 Step 20000/287113 — Loss: 10.4476\n",
      "Epoch 1 Step 22000/287113 — Loss: 10.4422\n",
      "Epoch 1 Step 24000/287113 — Loss: 10.4367\n",
      "Epoch 1 Step 26000/287113 — Loss: 10.4312\n",
      "Epoch 1 Step 28000/287113 — Loss: 10.4257\n",
      "Epoch 1 Step 30000/287113 — Loss: 10.4202\n",
      "Epoch 1 Step 32000/287113 — Loss: 10.4147\n",
      "Epoch 1 Step 34000/287113 — Loss: 10.4092\n",
      "Epoch 1 Step 36000/287113 — Loss: 10.4038\n",
      "Epoch 1 Step 38000/287113 — Loss: 10.3983\n",
      "Epoch 1 Step 40000/287113 — Loss: 10.3928\n",
      "Epoch 1 Step 42000/287113 — Loss: 10.3873\n",
      "Epoch 1 Step 44000/287113 — Loss: 10.3818\n",
      "Epoch 1 Step 46000/287113 — Loss: 10.3763\n",
      "Epoch 1 Step 48000/287113 — Loss: 10.3708\n",
      "Epoch 1 Step 50000/287113 — Loss: 10.3654\n",
      "Epoch 1 Step 52000/287113 — Loss: 10.3599\n",
      "Epoch 1 Step 54000/287113 — Loss: 10.3544\n",
      "Epoch 1 Step 56000/287113 — Loss: 10.3489\n",
      "Epoch 1 Step 58000/287113 — Loss: 10.3434\n",
      "Epoch 1 Step 60000/287113 — Loss: 10.3379\n",
      "Epoch 1 Step 62000/287113 — Loss: 10.3324\n",
      "Epoch 1 Step 64000/287113 — Loss: 10.3270\n",
      "Epoch 1 Step 66000/287113 — Loss: 10.3215\n",
      "Epoch 1 Step 68000/287113 — Loss: 10.3160\n",
      "Epoch 1 Step 70000/287113 — Loss: 10.3105\n",
      "Epoch 1 Step 72000/287113 — Loss: 10.3050\n",
      "Epoch 1 Step 74000/287113 — Loss: 10.2995\n",
      "Epoch 1 Step 76000/287113 — Loss: 10.2940\n",
      "Epoch 1 Step 78000/287113 — Loss: 10.2886\n",
      "Epoch 1 Step 80000/287113 — Loss: 10.2831\n",
      "Epoch 1 Step 82000/287113 — Loss: 10.2776\n",
      "Epoch 1 Step 84000/287113 — Loss: 10.2721\n",
      "Epoch 1 Step 86000/287113 — Loss: 10.2666\n",
      "Epoch 1 Step 88000/287113 — Loss: 10.2611\n",
      "Epoch 1 Step 90000/287113 — Loss: 10.2556\n",
      "Epoch 1 Step 92000/287113 — Loss: 10.2502\n",
      "Epoch 1 Step 94000/287113 — Loss: 10.2447\n",
      "Epoch 1 Step 96000/287113 — Loss: 10.2392\n",
      "Epoch 1 Step 98000/287113 — Loss: 10.2337\n",
      "Epoch 1 Step 100000/287113 — Loss: 10.2282\n",
      "Epoch 1 Step 102000/287113 — Loss: 10.2227\n",
      "Epoch 1 Step 104000/287113 — Loss: 10.2172\n",
      "Epoch 1 Step 106000/287113 — Loss: 10.2118\n",
      "Epoch 1 Step 108000/287113 — Loss: 10.2063\n",
      "Epoch 1 Step 110000/287113 — Loss: 10.2008\n",
      "Epoch 1 Step 112000/287113 — Loss: 10.1953\n",
      "Epoch 1 Step 114000/287113 — Loss: 10.1898\n",
      "Epoch 1 Step 116000/287113 — Loss: 10.1843\n",
      "Epoch 1 Step 118000/287113 — Loss: 10.1788\n",
      "Epoch 1 Step 120000/287113 — Loss: 10.1734\n",
      "Epoch 1 Step 122000/287113 — Loss: 10.1679\n",
      "Epoch 1 Step 124000/287113 — Loss: 10.1624\n",
      "Epoch 1 Step 126000/287113 — Loss: 10.1569\n",
      "Epoch 1 Step 128000/287113 — Loss: 10.1514\n",
      "Epoch 1 Step 130000/287113 — Loss: 10.1459\n",
      "Epoch 1 Step 132000/287113 — Loss: 10.1404\n",
      "Epoch 1 Step 134000/287113 — Loss: 10.1350\n",
      "Epoch 1 Step 136000/287113 — Loss: 10.1295\n",
      "Epoch 1 Step 138000/287113 — Loss: 10.1240\n",
      "Epoch 1 Step 140000/287113 — Loss: 10.1185\n",
      "Epoch 1 Step 142000/287113 — Loss: 10.1130\n",
      "Epoch 1 Step 144000/287113 — Loss: 10.1075\n",
      "Epoch 1 Step 146000/287113 — Loss: 10.1020\n",
      "Epoch 1 Step 148000/287113 — Loss: 10.0966\n",
      "Epoch 1 Step 150000/287113 — Loss: 10.0911\n",
      "Epoch 1 Step 152000/287113 — Loss: 10.0856\n",
      "Epoch 1 Step 154000/287113 — Loss: 10.0801\n",
      "Epoch 1 Step 156000/287113 — Loss: 10.0746\n",
      "Epoch 1 Step 158000/287113 — Loss: 10.0691\n",
      "Epoch 1 Step 160000/287113 — Loss: 10.0636\n",
      "Epoch 1 Step 162000/287113 — Loss: 10.0582\n",
      "Epoch 1 Step 164000/287113 — Loss: 10.0527\n",
      "Epoch 1 Step 166000/287113 — Loss: 10.0472\n",
      "Epoch 1 Step 168000/287113 — Loss: 10.0417\n",
      "Epoch 1 Step 170000/287113 — Loss: 10.0362\n",
      "Epoch 1 Step 172000/287113 — Loss: 10.0307\n",
      "Epoch 1 Step 174000/287113 — Loss: 10.0252\n",
      "Epoch 1 Step 176000/287113 — Loss: 10.0198\n",
      "Epoch 1 Step 178000/287113 — Loss: 10.0143\n",
      "Epoch 1 Step 180000/287113 — Loss: 10.0088\n",
      "Epoch 1 Step 182000/287113 — Loss: 10.0033\n",
      "Epoch 1 Step 184000/287113 — Loss: 9.9978\n",
      "Epoch 1 Step 186000/287113 — Loss: 9.9923\n",
      "Epoch 1 Step 188000/287113 — Loss: 9.9868\n",
      "Epoch 1 Step 190000/287113 — Loss: 9.9814\n",
      "Epoch 1 Step 192000/287113 — Loss: 9.9759\n",
      "Epoch 1 Step 194000/287113 — Loss: 9.9704\n",
      "Epoch 1 Step 196000/287113 — Loss: 9.9649\n",
      "Epoch 1 Step 198000/287113 — Loss: 9.9594\n",
      "Epoch 1 Step 200000/287113 — Loss: 9.9539\n",
      "Epoch 1 Step 202000/287113 — Loss: 9.9484\n",
      "Epoch 1 Step 204000/287113 — Loss: 9.9430\n",
      "Epoch 1 Step 206000/287113 — Loss: 9.9375\n",
      "Epoch 1 Step 208000/287113 — Loss: 9.9320\n",
      "Epoch 1 Step 210000/287113 — Loss: 9.9265\n",
      "Epoch 1 Step 212000/287113 — Loss: 9.9210\n",
      "Epoch 1 Step 214000/287113 — Loss: 9.9155\n",
      "Epoch 1 Step 216000/287113 — Loss: 9.9100\n",
      "Epoch 1 Step 218000/287113 — Loss: 9.9046\n",
      "Epoch 1 Step 220000/287113 — Loss: 9.8991\n",
      "Epoch 1 Step 222000/287113 — Loss: 9.8936\n",
      "Epoch 1 Step 224000/287113 — Loss: 9.8881\n",
      "Epoch 1 Step 226000/287113 — Loss: 9.8826\n",
      "Epoch 1 Step 228000/287113 — Loss: 9.8771\n",
      "Epoch 1 Step 230000/287113 — Loss: 9.8717\n",
      "Epoch 1 Step 232000/287113 — Loss: 9.8662\n",
      "Epoch 1 Step 234000/287113 — Loss: 9.8607\n",
      "Epoch 1 Step 236000/287113 — Loss: 9.8552\n",
      "Epoch 1 Step 238000/287113 — Loss: 9.8497\n",
      "Epoch 1 Step 240000/287113 — Loss: 9.8442\n",
      "Epoch 1 Step 242000/287113 — Loss: 9.8387\n",
      "Epoch 1 Step 244000/287113 — Loss: 9.8333\n",
      "Epoch 1 Step 246000/287113 — Loss: 9.8278\n",
      "Epoch 1 Step 248000/287113 — Loss: 9.8223\n",
      "Epoch 1 Step 250000/287113 — Loss: 9.8168\n",
      "Epoch 1 Step 252000/287113 — Loss: 9.8113\n",
      "Epoch 1 Step 254000/287113 — Loss: 9.8058\n",
      "Epoch 1 Step 256000/287113 — Loss: 9.8003\n",
      "Epoch 1 Step 258000/287113 — Loss: 9.7949\n",
      "Epoch 1 Step 260000/287113 — Loss: 9.7894\n",
      "Epoch 1 Step 262000/287113 — Loss: 9.7839\n",
      "Epoch 1 Step 264000/287113 — Loss: 9.7784\n",
      "Epoch 1 Step 266000/287113 — Loss: 9.7729\n",
      "Epoch 1 Step 268000/287113 — Loss: 9.7674\n",
      "Epoch 1 Step 270000/287113 — Loss: 9.7619\n",
      "Epoch 1 Step 272000/287113 — Loss: 9.7565\n",
      "Epoch 1 Step 274000/287113 — Loss: 9.7510\n",
      "Epoch 1 Step 276000/287113 — Loss: 9.7455\n",
      "Epoch 1 Step 278000/287113 — Loss: 9.7400\n",
      "Epoch 1 Step 280000/287113 — Loss: 9.7345\n",
      "Epoch 1 Step 282000/287113 — Loss: 9.7290\n",
      "Epoch 1 Step 284000/287113 — Loss: 9.7235\n",
      "Epoch 1 Step 286000/287113 — Loss: 9.7181\n",
      "\n",
      "=== Epoch 1/3 ===\n",
      "Train Loss: 9.7150\n",
      " Val Loss:  9.6552\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROUGE-1:            0.4299\n",
      "ROUGE-2:            0.2504\n",
      "ROUGE-L:            0.4233\n",
      "BERTScore Precision:0.6557\n",
      "BERTScore Recall:   0.7323\n",
      "BERTScore F1:       0.6919\n",
      "\n",
      "Epoch 2 Step 2000/287113 — Loss: 8.9121\n",
      "Epoch 2 Step 4000/287113 — Loss: 8.9074\n",
      "Epoch 2 Step 6000/287113 — Loss: 8.9027\n",
      "Epoch 2 Step 8000/287113 — Loss: 8.8981\n",
      "Epoch 2 Step 10000/287113 — Loss: 8.8934\n",
      "Epoch 2 Step 12000/287113 — Loss: 8.8888\n",
      "Epoch 2 Step 14000/287113 — Loss: 8.8841\n",
      "Epoch 2 Step 16000/287113 — Loss: 8.8795\n",
      "Epoch 2 Step 18000/287113 — Loss: 8.8748\n",
      "Epoch 2 Step 20000/287113 — Loss: 8.8701\n",
      "Epoch 2 Step 22000/287113 — Loss: 8.8655\n",
      "Epoch 2 Step 24000/287113 — Loss: 8.8608\n",
      "Epoch 2 Step 26000/287113 — Loss: 8.8562\n",
      "Epoch 2 Step 28000/287113 — Loss: 8.8515\n",
      "Epoch 2 Step 30000/287113 — Loss: 8.8469\n",
      "Epoch 2 Step 32000/287113 — Loss: 8.8422\n",
      "Epoch 2 Step 34000/287113 — Loss: 8.8376\n",
      "Epoch 2 Step 36000/287113 — Loss: 8.8329\n",
      "Epoch 2 Step 38000/287113 — Loss: 8.8282\n",
      "Epoch 2 Step 40000/287113 — Loss: 8.8236\n",
      "Epoch 2 Step 42000/287113 — Loss: 8.8189\n",
      "Epoch 2 Step 44000/287113 — Loss: 8.8143\n",
      "Epoch 2 Step 46000/287113 — Loss: 8.8096\n",
      "Epoch 2 Step 48000/287113 — Loss: 8.8049\n",
      "Epoch 2 Step 50000/287113 — Loss: 8.8003\n",
      "Epoch 2 Step 52000/287113 — Loss: 8.7956\n",
      "Epoch 2 Step 54000/287113 — Loss: 8.7910\n",
      "Epoch 2 Step 56000/287113 — Loss: 8.7863\n",
      "Epoch 2 Step 58000/287113 — Loss: 8.7817\n",
      "Epoch 2 Step 60000/287113 — Loss: 8.7770\n",
      "Epoch 2 Step 62000/287113 — Loss: 8.7723\n",
      "Epoch 2 Step 64000/287113 — Loss: 8.7677\n",
      "Epoch 2 Step 66000/287113 — Loss: 8.7630\n",
      "Epoch 2 Step 68000/287113 — Loss: 8.7584\n",
      "Epoch 2 Step 70000/287113 — Loss: 8.7537\n",
      "Epoch 2 Step 72000/287113 — Loss: 8.7491\n",
      "Epoch 2 Step 74000/287113 — Loss: 8.7444\n",
      "Epoch 2 Step 76000/287113 — Loss: 8.7397\n",
      "Epoch 2 Step 78000/287113 — Loss: 8.7351\n",
      "Epoch 2 Step 80000/287113 — Loss: 8.7304\n",
      "Epoch 2 Step 82000/287113 — Loss: 8.7258\n",
      "Epoch 2 Step 84000/287113 — Loss: 8.7211\n",
      "Epoch 2 Step 86000/287113 — Loss: 8.7165\n",
      "Epoch 2 Step 88000/287113 — Loss: 8.7118\n",
      "Epoch 2 Step 90000/287113 — Loss: 8.7071\n",
      "Epoch 2 Step 92000/287113 — Loss: 8.7025\n",
      "Epoch 2 Step 94000/287113 — Loss: 8.6978\n",
      "Epoch 2 Step 96000/287113 — Loss: 8.6932\n",
      "Epoch 2 Step 98000/287113 — Loss: 8.6885\n",
      "Epoch 2 Step 100000/287113 — Loss: 8.6839\n",
      "Epoch 2 Step 102000/287113 — Loss: 8.6792\n",
      "Epoch 2 Step 104000/287113 — Loss: 8.6745\n",
      "Epoch 2 Step 106000/287113 — Loss: 8.6699\n",
      "Epoch 2 Step 108000/287113 — Loss: 8.6652\n",
      "Epoch 2 Step 110000/287113 — Loss: 8.6606\n",
      "Epoch 2 Step 112000/287113 — Loss: 8.6559\n",
      "Epoch 2 Step 114000/287113 — Loss: 8.6513\n",
      "Epoch 2 Step 116000/287113 — Loss: 8.6466\n",
      "Epoch 2 Step 118000/287113 — Loss: 8.6419\n",
      "Epoch 2 Step 120000/287113 — Loss: 8.6373\n",
      "Epoch 2 Step 122000/287113 — Loss: 8.6326\n",
      "Epoch 2 Step 124000/287113 — Loss: 8.6280\n",
      "Epoch 2 Step 126000/287113 — Loss: 8.6233\n",
      "Epoch 2 Step 128000/287113 — Loss: 8.6187\n",
      "Epoch 2 Step 130000/287113 — Loss: 8.6140\n",
      "Epoch 2 Step 132000/287113 — Loss: 8.6093\n",
      "Epoch 2 Step 134000/287113 — Loss: 8.6047\n",
      "Epoch 2 Step 136000/287113 — Loss: 8.6000\n",
      "Epoch 2 Step 138000/287113 — Loss: 8.5954\n",
      "Epoch 2 Step 140000/287113 — Loss: 8.5907\n",
      "Epoch 2 Step 142000/287113 — Loss: 8.5861\n",
      "Epoch 2 Step 144000/287113 — Loss: 8.5814\n",
      "Epoch 2 Step 146000/287113 — Loss: 8.5767\n",
      "Epoch 2 Step 148000/287113 — Loss: 8.5721\n",
      "Epoch 2 Step 150000/287113 — Loss: 8.5674\n",
      "Epoch 2 Step 152000/287113 — Loss: 8.5628\n",
      "Epoch 2 Step 154000/287113 — Loss: 8.5581\n",
      "Epoch 2 Step 156000/287113 — Loss: 8.5535\n",
      "Epoch 2 Step 158000/287113 — Loss: 8.5488\n",
      "Epoch 2 Step 160000/287113 — Loss: 8.5441\n",
      "Epoch 2 Step 162000/287113 — Loss: 8.5395\n",
      "Epoch 2 Step 164000/287113 — Loss: 8.5348\n",
      "Epoch 2 Step 166000/287113 — Loss: 8.5302\n",
      "Epoch 2 Step 168000/287113 — Loss: 8.5255\n",
      "Epoch 2 Step 170000/287113 — Loss: 8.5209\n",
      "Epoch 2 Step 172000/287113 — Loss: 8.5162\n",
      "Epoch 2 Step 174000/287113 — Loss: 8.5115\n",
      "Epoch 2 Step 176000/287113 — Loss: 8.5069\n",
      "Epoch 2 Step 178000/287113 — Loss: 8.5022\n",
      "Epoch 2 Step 180000/287113 — Loss: 8.4976\n",
      "Epoch 2 Step 182000/287113 — Loss: 8.4929\n",
      "Epoch 2 Step 184000/287113 — Loss: 8.4883\n",
      "Epoch 2 Step 186000/287113 — Loss: 8.4836\n",
      "Epoch 2 Step 188000/287113 — Loss: 8.4789\n",
      "Epoch 2 Step 190000/287113 — Loss: 8.4743\n",
      "Epoch 2 Step 192000/287113 — Loss: 8.4696\n",
      "Epoch 2 Step 194000/287113 — Loss: 8.4650\n",
      "Epoch 2 Step 196000/287113 — Loss: 8.4603\n",
      "Epoch 2 Step 198000/287113 — Loss: 8.4557\n",
      "Epoch 2 Step 200000/287113 — Loss: 8.4510\n",
      "Epoch 2 Step 202000/287113 — Loss: 8.4463\n",
      "Epoch 2 Step 204000/287113 — Loss: 8.4417\n",
      "Epoch 2 Step 206000/287113 — Loss: 8.4370\n",
      "Epoch 2 Step 208000/287113 — Loss: 8.4324\n",
      "Epoch 2 Step 210000/287113 — Loss: 8.4277\n",
      "Epoch 2 Step 212000/287113 — Loss: 8.4231\n",
      "Epoch 2 Step 214000/287113 — Loss: 8.4184\n",
      "Epoch 2 Step 216000/287113 — Loss: 8.4137\n",
      "Epoch 2 Step 218000/287113 — Loss: 8.4091\n",
      "Epoch 2 Step 220000/287113 — Loss: 8.4044\n",
      "Epoch 2 Step 222000/287113 — Loss: 8.3998\n",
      "Epoch 2 Step 224000/287113 — Loss: 8.3951\n",
      "Epoch 2 Step 226000/287113 — Loss: 8.3905\n",
      "Epoch 2 Step 228000/287113 — Loss: 8.3858\n",
      "Epoch 2 Step 230000/287113 — Loss: 8.3811\n",
      "Epoch 2 Step 232000/287113 — Loss: 8.3765\n",
      "Epoch 2 Step 234000/287113 — Loss: 8.3718\n",
      "Epoch 2 Step 236000/287113 — Loss: 8.3672\n",
      "Epoch 2 Step 238000/287113 — Loss: 8.3625\n",
      "Epoch 2 Step 240000/287113 — Loss: 8.3579\n",
      "Epoch 2 Step 242000/287113 — Loss: 8.3532\n",
      "Epoch 2 Step 244000/287113 — Loss: 8.3485\n",
      "Epoch 2 Step 246000/287113 — Loss: 8.3439\n",
      "Epoch 2 Step 248000/287113 — Loss: 8.3392\n",
      "Epoch 2 Step 250000/287113 — Loss: 8.3346\n",
      "Epoch 2 Step 252000/287113 — Loss: 8.3299\n",
      "Epoch 2 Step 254000/287113 — Loss: 8.3253\n",
      "Epoch 2 Step 256000/287113 — Loss: 8.3206\n",
      "Epoch 2 Step 258000/287113 — Loss: 8.3159\n",
      "Epoch 2 Step 260000/287113 — Loss: 8.3113\n",
      "Epoch 2 Step 262000/287113 — Loss: 8.3066\n",
      "Epoch 2 Step 264000/287113 — Loss: 8.3020\n",
      "Epoch 2 Step 266000/287113 — Loss: 8.2973\n",
      "Epoch 2 Step 268000/287113 — Loss: 8.2927\n",
      "Epoch 2 Step 270000/287113 — Loss: 8.2880\n",
      "Epoch 2 Step 272000/287113 — Loss: 8.2833\n",
      "Epoch 2 Step 274000/287113 — Loss: 8.2787\n",
      "Epoch 2 Step 276000/287113 — Loss: 8.2740\n",
      "Epoch 2 Step 278000/287113 — Loss: 8.2694\n",
      "Epoch 2 Step 280000/287113 — Loss: 8.2647\n",
      "Epoch 2 Step 282000/287113 — Loss: 8.2601\n",
      "Epoch 2 Step 284000/287113 — Loss: 8.2554\n",
      "Epoch 2 Step 286000/287113 — Loss: 8.2507\n",
      "\n",
      "=== Epoch 2/3 ===\n",
      "Train Loss: 8.2482\n",
      " Val Loss:  8.1642\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROUGE-1:            0.4309\n",
      "ROUGE-2:            0.2591\n",
      "ROUGE-L:            0.3815\n",
      "BERTScore Precision:0.7274\n",
      "BERTScore Recall:   0.6969\n",
      "BERTScore F1:       0.7118\n",
      "\n",
      "Epoch 3 Step 2000/287113 — Loss: 7.5703\n",
      "Epoch 3 Step 4000/287113 — Loss: 7.5663\n",
      "Epoch 3 Step 6000/287113 — Loss: 7.5624\n",
      "Epoch 3 Step 8000/287113 — Loss: 7.5584\n",
      "Epoch 3 Step 10000/287113 — Loss: 7.5544\n",
      "Epoch 3 Step 12000/287113 — Loss: 7.5505\n",
      "Epoch 3 Step 14000/287113 — Loss: 7.5465\n",
      "Epoch 3 Step 16000/287113 — Loss: 7.5426\n",
      "Epoch 3 Step 18000/287113 — Loss: 7.5386\n",
      "Epoch 3 Step 20000/287113 — Loss: 7.5347\n",
      "Epoch 3 Step 22000/287113 — Loss: 7.5307\n",
      "Epoch 3 Step 24000/287113 — Loss: 7.5268\n",
      "Epoch 3 Step 26000/287113 — Loss: 7.5228\n",
      "Epoch 3 Step 28000/287113 — Loss: 7.5188\n",
      "Epoch 3 Step 30000/287113 — Loss: 7.5149\n",
      "Epoch 3 Step 32000/287113 — Loss: 7.5109\n",
      "Epoch 3 Step 34000/287113 — Loss: 7.5070\n",
      "Epoch 3 Step 36000/287113 — Loss: 7.5030\n",
      "Epoch 3 Step 38000/287113 — Loss: 7.4991\n",
      "Epoch 3 Step 40000/287113 — Loss: 7.4951\n",
      "Epoch 3 Step 42000/287113 — Loss: 7.4912\n",
      "Epoch 3 Step 44000/287113 — Loss: 7.4872\n",
      "Epoch 3 Step 46000/287113 — Loss: 7.4832\n",
      "Epoch 3 Step 48000/287113 — Loss: 7.4793\n",
      "Epoch 3 Step 50000/287113 — Loss: 7.4753\n",
      "Epoch 3 Step 52000/287113 — Loss: 7.4714\n",
      "Epoch 3 Step 54000/287113 — Loss: 7.4674\n",
      "Epoch 3 Step 56000/287113 — Loss: 7.4635\n",
      "Epoch 3 Step 58000/287113 — Loss: 7.4595\n",
      "Epoch 3 Step 60000/287113 — Loss: 7.4556\n",
      "Epoch 3 Step 62000/287113 — Loss: 7.4516\n",
      "Epoch 3 Step 64000/287113 — Loss: 7.4476\n",
      "Epoch 3 Step 66000/287113 — Loss: 7.4437\n",
      "Epoch 3 Step 68000/287113 — Loss: 7.4397\n",
      "Epoch 3 Step 70000/287113 — Loss: 7.4358\n",
      "Epoch 3 Step 72000/287113 — Loss: 7.4318\n",
      "Epoch 3 Step 74000/287113 — Loss: 7.4279\n",
      "Epoch 3 Step 76000/287113 — Loss: 7.4239\n",
      "Epoch 3 Step 78000/287113 — Loss: 7.4200\n",
      "Epoch 3 Step 80000/287113 — Loss: 7.4160\n",
      "Epoch 3 Step 82000/287113 — Loss: 7.4120\n",
      "Epoch 3 Step 84000/287113 — Loss: 7.4081\n",
      "Epoch 3 Step 86000/287113 — Loss: 7.4041\n",
      "Epoch 3 Step 88000/287113 — Loss: 7.4002\n",
      "Epoch 3 Step 90000/287113 — Loss: 7.3962\n",
      "Epoch 3 Step 92000/287113 — Loss: 7.3923\n",
      "Epoch 3 Step 94000/287113 — Loss: 7.3883\n",
      "Epoch 3 Step 96000/287113 — Loss: 7.3844\n",
      "Epoch 3 Step 98000/287113 — Loss: 7.3804\n",
      "Epoch 3 Step 100000/287113 — Loss: 7.3764\n",
      "Epoch 3 Step 102000/287113 — Loss: 7.3725\n",
      "Epoch 3 Step 104000/287113 — Loss: 7.3685\n",
      "Epoch 3 Step 106000/287113 — Loss: 7.3646\n",
      "Epoch 3 Step 108000/287113 — Loss: 7.3606\n",
      "Epoch 3 Step 110000/287113 — Loss: 7.3567\n",
      "Epoch 3 Step 112000/287113 — Loss: 7.3527\n",
      "Epoch 3 Step 114000/287113 — Loss: 7.3487\n",
      "Epoch 3 Step 116000/287113 — Loss: 7.3448\n",
      "Epoch 3 Step 118000/287113 — Loss: 7.3408\n",
      "Epoch 3 Step 120000/287113 — Loss: 7.3369\n",
      "Epoch 3 Step 122000/287113 — Loss: 7.3329\n",
      "Epoch 3 Step 124000/287113 — Loss: 7.3290\n",
      "Epoch 3 Step 126000/287113 — Loss: 7.3250\n",
      "Epoch 3 Step 128000/287113 — Loss: 7.3211\n",
      "Epoch 3 Step 130000/287113 — Loss: 7.3171\n",
      "Epoch 3 Step 132000/287113 — Loss: 7.3132\n",
      "Epoch 3 Step 134000/287113 — Loss: 7.3092\n",
      "Epoch 3 Step 136000/287113 — Loss: 7.3052\n",
      "Epoch 3 Step 138000/287113 — Loss: 7.3013\n",
      "Epoch 3 Step 140000/287113 — Loss: 7.2973\n",
      "Epoch 3 Step 142000/287113 — Loss: 7.2934\n",
      "Epoch 3 Step 144000/287113 — Loss: 7.2894\n",
      "Epoch 3 Step 146000/287113 — Loss: 7.2855\n",
      "Epoch 3 Step 148000/287113 — Loss: 7.2815\n",
      "Epoch 3 Step 150000/287113 — Loss: 7.2775\n",
      "Epoch 3 Step 152000/287113 — Loss: 7.2736\n",
      "Epoch 3 Step 154000/287113 — Loss: 7.2696\n",
      "Epoch 3 Step 156000/287113 — Loss: 7.2657\n",
      "Epoch 3 Step 158000/287113 — Loss: 7.2617\n",
      "Epoch 3 Step 160000/287113 — Loss: 7.2578\n",
      "Epoch 3 Step 162000/287113 — Loss: 7.2538\n",
      "Epoch 3 Step 164000/287113 — Loss: 7.2499\n",
      "Epoch 3 Step 166000/287113 — Loss: 7.2459\n",
      "Epoch 3 Step 168000/287113 — Loss: 7.2419\n",
      "Epoch 3 Step 170000/287113 — Loss: 7.2380\n",
      "Epoch 3 Step 172000/287113 — Loss: 7.2340\n",
      "Epoch 3 Step 174000/287113 — Loss: 7.2301\n",
      "Epoch 3 Step 176000/287113 — Loss: 7.2261\n",
      "Epoch 3 Step 178000/287113 — Loss: 7.2222\n",
      "Epoch 3 Step 180000/287113 — Loss: 7.2182\n",
      "Epoch 3 Step 182000/287113 — Loss: 7.2143\n",
      "Epoch 3 Step 184000/287113 — Loss: 7.2103\n",
      "Epoch 3 Step 186000/287113 — Loss: 7.2063\n",
      "Epoch 3 Step 188000/287113 — Loss: 7.2024\n",
      "Epoch 3 Step 190000/287113 — Loss: 7.1984\n",
      "Epoch 3 Step 192000/287113 — Loss: 7.1945\n",
      "Epoch 3 Step 194000/287113 — Loss: 7.1905\n",
      "Epoch 3 Step 196000/287113 — Loss: 7.1866\n",
      "Epoch 3 Step 198000/287113 — Loss: 7.1826\n",
      "Epoch 3 Step 200000/287113 — Loss: 7.1787\n",
      "Epoch 3 Step 202000/287113 — Loss: 7.1747\n",
      "Epoch 3 Step 204000/287113 — Loss: 7.1707\n",
      "Epoch 3 Step 206000/287113 — Loss: 7.1668\n",
      "Epoch 3 Step 208000/287113 — Loss: 7.1628\n",
      "Epoch 3 Step 210000/287113 — Loss: 7.1589\n",
      "Epoch 3 Step 212000/287113 — Loss: 7.1549\n",
      "Epoch 3 Step 214000/287113 — Loss: 7.1510\n",
      "Epoch 3 Step 216000/287113 — Loss: 7.1470\n",
      "Epoch 3 Step 218000/287113 — Loss: 7.1430\n",
      "Epoch 3 Step 220000/287113 — Loss: 7.1391\n",
      "Epoch 3 Step 222000/287113 — Loss: 7.1351\n",
      "Epoch 3 Step 224000/287113 — Loss: 7.1312\n",
      "Epoch 3 Step 226000/287113 — Loss: 7.1272\n",
      "Epoch 3 Step 228000/287113 — Loss: 7.1233\n",
      "Epoch 3 Step 230000/287113 — Loss: 7.1193\n",
      "Epoch 3 Step 232000/287113 — Loss: 7.1154\n",
      "Epoch 3 Step 234000/287113 — Loss: 7.1114\n",
      "Epoch 3 Step 236000/287113 — Loss: 7.1074\n",
      "Epoch 3 Step 238000/287113 — Loss: 7.1035\n",
      "Epoch 3 Step 240000/287113 — Loss: 7.0995\n",
      "Epoch 3 Step 242000/287113 — Loss: 7.0956\n",
      "Epoch 3 Step 244000/287113 — Loss: 7.0916\n",
      "Epoch 3 Step 246000/287113 — Loss: 7.0877\n",
      "Epoch 3 Step 248000/287113 — Loss: 7.0837\n",
      "Epoch 3 Step 250000/287113 — Loss: 7.0798\n",
      "Epoch 3 Step 252000/287113 — Loss: 7.0758\n",
      "Epoch 3 Step 254000/287113 — Loss: 7.0718\n",
      "Epoch 3 Step 256000/287113 — Loss: 7.0679\n",
      "Epoch 3 Step 258000/287113 — Loss: 7.0639\n",
      "Epoch 3 Step 260000/287113 — Loss: 7.0600\n",
      "Epoch 3 Step 262000/287113 — Loss: 7.0560\n",
      "Epoch 3 Step 264000/287113 — Loss: 7.0521\n",
      "Epoch 3 Step 266000/287113 — Loss: 7.0481\n",
      "Epoch 3 Step 268000/287113 — Loss: 7.0442\n",
      "Epoch 3 Step 270000/287113 — Loss: 7.0402\n",
      "Epoch 3 Step 272000/287113 — Loss: 7.0362\n",
      "Epoch 3 Step 274000/287113 — Loss: 7.0323\n",
      "Epoch 3 Step 276000/287113 — Loss: 7.0283\n",
      "Epoch 3 Step 278000/287113 — Loss: 7.0244\n",
      "Epoch 3 Step 280000/287113 — Loss: 7.0204\n",
      "Epoch 3 Step 282000/287113 — Loss: 7.0165\n",
      "Epoch 3 Step 284000/287113 — Loss: 7.0125\n",
      "Epoch 3 Step 286000/287113 — Loss: 7.0086\n",
      "\n",
      "=== Epoch 3/3 ===\n",
      "Train Loss: 7.0064\n",
      " Val Loss:  6.8803\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROUGE-1:            0.5180\n",
      "ROUGE-2:            0.2908\n",
      "ROUGE-L:            0.4795\n",
      "BERTScore Precision:0.6877\n",
      "BERTScore Recall:   0.7358\n",
      "BERTScore F1:       0.7109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
    "    train_loss = train_epoch(train_loader, epoch)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_loss = eval_epoch(val_loader)\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    evaluate_model(val_loader, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e954e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.2538\n",
      "=== Test Evaluation ===\n",
      "ROUGE-1: 0.2743\n",
      "ROUGE-2: 0.2371\n",
      "ROUGE-L: 0.2552\n",
      "BERTScore F1: 0.5342\n",
      "BERTScore Precision: 0.5106\n",
      "BERTScore Recall: 0.5281\n"
     ]
    }
   ],
   "source": [
    "test_loss = eval_epoch(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "rouge_results, bert_score = evaluate_model(test_loader, \"Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
