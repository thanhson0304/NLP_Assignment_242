{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5743f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizerFast, \n",
    "    EncoderDecoderModel,\n",
    "    BertModel,\n",
    "    BertConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "import evaluate\n",
    "from bert_score import BERTScorer\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb74e411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2458d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\")\n",
    "train_data = dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "val_data   = dataset['validation'].shuffle(seed=42).select(range(200))\n",
    "test_data  = dataset['test'].shuffle(seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ddc5b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "special_tokens_dict = {}\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict['pad_token'] = '[PAD]'\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict['bos_token'] = '[CLS]'\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict['eos_token'] = '[SEP]'\n",
    "if special_tokens_dict:\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b40e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "config_encoder = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config_decoder = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.add_cross_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8fa9fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    'bert-base-uncased',\n",
    "    encoder_config=config_encoder,\n",
    "    decoder_config=config_decoder,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "642aeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id           = tokenizer.eos_token_id\n",
    "model.config.pad_token_id           = tokenizer.pad_token_id\n",
    "model.config.vocab_size             = len(tokenizer)\n",
    "\n",
    "model.base_model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.base_model.config.eos_token_id           = tokenizer.eos_token_id\n",
    "model.base_model.config.pad_token_id           = tokenizer.pad_token_id\n",
    "\n",
    "model.config.max_length          = 128\n",
    "model.config.min_length          = 10\n",
    "model.config.no_repeat_ngram_size= 3\n",
    "model.config.early_stopping      = True\n",
    "model.config.num_beams           = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e918981",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\", \"key\", \"dense\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96fd46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = str(self.data[idx]['article']).strip()\n",
    "        highlights = str(self.data[idx]['highlights']).strip()\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            article,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer(\n",
    "            highlights,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "278eef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(\n",
    "        attention_masks, batch_first=True, padding_value=0\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c06281d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataset = SummarizationDataset(train_data, tokenizer)\n",
    "val_dataset   = SummarizationDataset(val_data, tokenizer)\n",
    "test_dataset  = SummarizationDataset(test_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8333eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "epochs = 3\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=total_steps // 10, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4240af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(dataloader, num_samples=50):\n",
    "    model.eval()\n",
    "    generated_summaries = []\n",
    "    reference_summaries = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample_count = 0\n",
    "        for batch in dataloader:\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                min_length=10,\n",
    "                num_beams=2,  \n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False\n",
    "            )\n",
    "            \n",
    "            for i in range(len(generated_ids)):\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                generated_text = tokenizer.decode(\n",
    "                    generated_ids[i], skip_special_tokens=True\n",
    "                ).strip()\n",
    "                \n",
    "                reference_ids = labels[i][labels[i] != -100]\n",
    "                reference_text = tokenizer.decode(\n",
    "                    reference_ids, skip_special_tokens=True\n",
    "                ).strip()\n",
    "                \n",
    "                if generated_text and reference_text:\n",
    "                    generated_summaries.append(generated_text)\n",
    "                    reference_summaries.append(reference_text)\n",
    "                    sample_count += 1\n",
    "    \n",
    "    return generated_summaries, reference_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa78178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataloader, dataset_name=\"Validation\"):\n",
    "    print(f\"\\n=== {dataset_name} Evaluation ===\")\n",
    "    generated_summaries, reference_summaries = generate_summaries(dataloader, num_samples=50)\n",
    "    \n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=generated_summaries,\n",
    "        references=reference_summaries,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    P, R, F1 = bert_scorer.score(generated_summaries, reference_summaries)\n",
    "    bert_score = {\n",
    "        'precision': P.mean().item(),\n",
    "        'recall': R.mean().item(),\n",
    "        'f1': F1.mean().item()\n",
    "    }\n",
    "    \n",
    "    print(f\"ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
    "    print(f\"BERTScore F1: {bert_score['f1']:.4f}\")\n",
    "    print(f\"BERTScore Precision: {bert_score['precision']:.4f}\")\n",
    "    print(f\"BERTScore Recall: {bert_score['recall']:.4f}\")\n",
    "    \n",
    "    return rouge_results, bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df28c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    for step, batch in enumerate(dataloader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % 50 == 0 or step == num_batches:\n",
    "            avg_loss = total_loss / step\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"Epoch {epoch} Step {step}/{num_batches} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d077a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1463b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 2000/287113 — Loss: 10.4970\n",
      "Epoch 1 Step 4000/287113 — Loss: 10.4915\n",
      "Epoch 1 Step 6000/287113 — Loss: 10.4860\n",
      "Epoch 1 Step 8000/287113 — Loss: 10.4805\n",
      "Epoch 1 Step 10000/287113 — Loss: 10.4750\n",
      "Epoch 1 Step 12000/287113 — Loss: 10.4696\n",
      "Epoch 1 Step 14000/287113 — Loss: 10.4641\n",
      "Epoch 1 Step 16000/287113 — Loss: 10.4586\n",
      "Epoch 1 Step 18000/287113 — Loss: 10.4531\n",
      "Epoch 1 Step 20000/287113 — Loss: 10.4476\n",
      "Epoch 1 Step 22000/287113 — Loss: 10.4421\n",
      "Epoch 1 Step 24000/287113 — Loss: 10.4367\n",
      "Epoch 1 Step 26000/287113 — Loss: 10.4312\n",
      "Epoch 1 Step 28000/287113 — Loss: 10.4257\n",
      "Epoch 1 Step 30000/287113 — Loss: 10.4202\n",
      "Epoch 1 Step 32000/287113 — Loss: 10.4147\n",
      "Epoch 1 Step 34000/287113 — Loss: 10.4092\n",
      "Epoch 1 Step 36000/287113 — Loss: 10.4037\n",
      "Epoch 1 Step 38000/287113 — Loss: 10.3983\n",
      "Epoch 1 Step 40000/287113 — Loss: 10.3928\n",
      "Epoch 1 Step 42000/287113 — Loss: 10.3873\n",
      "Epoch 1 Step 44000/287113 — Loss: 10.3818\n",
      "Epoch 1 Step 46000/287113 — Loss: 10.3763\n",
      "Epoch 1 Step 48000/287113 — Loss: 10.3708\n",
      "Epoch 1 Step 50000/287113 — Loss: 10.3653\n",
      "Epoch 1 Step 52000/287113 — Loss: 10.3599\n",
      "Epoch 1 Step 54000/287113 — Loss: 10.3544\n",
      "Epoch 1 Step 56000/287113 — Loss: 10.3489\n",
      "Epoch 1 Step 58000/287113 — Loss: 10.3434\n",
      "Epoch 1 Step 60000/287113 — Loss: 10.3379\n",
      "Epoch 1 Step 62000/287113 — Loss: 10.3324\n",
      "Epoch 1 Step 64000/287113 — Loss: 10.3269\n",
      "Epoch 1 Step 66000/287113 — Loss: 10.3215\n",
      "Epoch 1 Step 68000/287113 — Loss: 10.3160\n",
      "Epoch 1 Step 70000/287113 — Loss: 10.3105\n",
      "Epoch 1 Step 72000/287113 — Loss: 10.3050\n",
      "Epoch 1 Step 74000/287113 — Loss: 10.2995\n",
      "Epoch 1 Step 76000/287113 — Loss: 10.2940\n",
      "Epoch 1 Step 78000/287113 — Loss: 10.2885\n",
      "Epoch 1 Step 80000/287113 — Loss: 10.2831\n",
      "Epoch 1 Step 82000/287113 — Loss: 10.2776\n",
      "Epoch 1 Step 84000/287113 — Loss: 10.2721\n",
      "Epoch 1 Step 86000/287113 — Loss: 10.2666\n",
      "Epoch 1 Step 88000/287113 — Loss: 10.2611\n",
      "Epoch 1 Step 90000/287113 — Loss: 10.2556\n",
      "Epoch 1 Step 92000/287113 — Loss: 10.2501\n",
      "Epoch 1 Step 94000/287113 — Loss: 10.2447\n",
      "Epoch 1 Step 96000/287113 — Loss: 10.2392\n",
      "Epoch 1 Step 98000/287113 — Loss: 10.2337\n",
      "Epoch 1 Step 100000/287113 — Loss: 10.2282\n",
      "Epoch 1 Step 102000/287113 — Loss: 10.2227\n",
      "Epoch 1 Step 104000/287113 — Loss: 10.2172\n",
      "Epoch 1 Step 106000/287113 — Loss: 10.2118\n",
      "Epoch 1 Step 108000/287113 — Loss: 10.2063\n",
      "Epoch 1 Step 110000/287113 — Loss: 10.2008\n",
      "Epoch 1 Step 112000/287113 — Loss: 10.1953\n",
      "Epoch 1 Step 114000/287113 — Loss: 10.1898\n",
      "Epoch 1 Step 116000/287113 — Loss: 10.1843\n",
      "Epoch 1 Step 118000/287113 — Loss: 10.1788\n",
      "Epoch 1 Step 120000/287113 — Loss: 10.1734\n",
      "Epoch 1 Step 122000/287113 — Loss: 10.1679\n",
      "Epoch 1 Step 124000/287113 — Loss: 10.1624\n",
      "Epoch 1 Step 126000/287113 — Loss: 10.1569\n",
      "Epoch 1 Step 128000/287113 — Loss: 10.1514\n",
      "Epoch 1 Step 130000/287113 — Loss: 10.1459\n",
      "Epoch 1 Step 132000/287113 — Loss: 10.1404\n",
      "Epoch 1 Step 134000/287113 — Loss: 10.1350\n",
      "Epoch 1 Step 136000/287113 — Loss: 10.1295\n",
      "Epoch 1 Step 138000/287113 — Loss: 10.1240\n",
      "Epoch 1 Step 140000/287113 — Loss: 10.1185\n",
      "Epoch 1 Step 142000/287113 — Loss: 10.1130\n",
      "Epoch 1 Step 144000/287113 — Loss: 10.1075\n",
      "Epoch 1 Step 146000/287113 — Loss: 10.1020\n",
      "Epoch 1 Step 148000/287113 — Loss: 10.0966\n",
      "Epoch 1 Step 150000/287113 — Loss: 10.0911\n",
      "Epoch 1 Step 152000/287113 — Loss: 10.0856\n",
      "Epoch 1 Step 154000/287113 — Loss: 10.0801\n",
      "Epoch 1 Step 156000/287113 — Loss: 10.0746\n",
      "Epoch 1 Step 158000/287113 — Loss: 10.0691\n",
      "Epoch 1 Step 160000/287113 — Loss: 10.0636\n",
      "Epoch 1 Step 162000/287113 — Loss: 10.0582\n",
      "Epoch 1 Step 164000/287113 — Loss: 10.0527\n",
      "Epoch 1 Step 166000/287113 — Loss: 10.0472\n",
      "Epoch 1 Step 168000/287113 — Loss: 10.0417\n",
      "Epoch 1 Step 170000/287113 — Loss: 10.0362\n",
      "Epoch 1 Step 172000/287113 — Loss: 10.0307\n",
      "Epoch 1 Step 174000/287113 — Loss: 10.0252\n",
      "Epoch 1 Step 176000/287113 — Loss: 10.0198\n",
      "Epoch 1 Step 178000/287113 — Loss: 10.0143\n",
      "Epoch 1 Step 180000/287113 — Loss: 10.0088\n",
      "Epoch 1 Step 182000/287113 — Loss: 10.0033\n",
      "Epoch 1 Step 184000/287113 — Loss: 9.9978\n",
      "Epoch 1 Step 186000/287113 — Loss: 9.9923\n",
      "Epoch 1 Step 188000/287113 — Loss: 9.9868\n",
      "Epoch 1 Step 190000/287113 — Loss: 9.9814\n",
      "Epoch 1 Step 192000/287113 — Loss: 9.9759\n",
      "Epoch 1 Step 194000/287113 — Loss: 9.9704\n",
      "Epoch 1 Step 196000/287113 — Loss: 9.9649\n",
      "Epoch 1 Step 198000/287113 — Loss: 9.9594\n",
      "Epoch 1 Step 200000/287113 — Loss: 9.9539\n",
      "Epoch 1 Step 202000/287113 — Loss: 9.9484\n",
      "Epoch 1 Step 204000/287113 — Loss: 9.9430\n",
      "Epoch 1 Step 206000/287113 — Loss: 9.9375\n",
      "Epoch 1 Step 208000/287113 — Loss: 9.9320\n",
      "Epoch 1 Step 210000/287113 — Loss: 9.9265\n",
      "Epoch 1 Step 212000/287113 — Loss: 9.9210\n",
      "Epoch 1 Step 214000/287113 — Loss: 9.9155\n",
      "Epoch 1 Step 216000/287113 — Loss: 9.9100\n",
      "Epoch 1 Step 218000/287113 — Loss: 9.9046\n",
      "Epoch 1 Step 220000/287113 — Loss: 9.8991\n",
      "Epoch 1 Step 222000/287113 — Loss: 9.8936\n",
      "Epoch 1 Step 224000/287113 — Loss: 9.8881\n",
      "Epoch 1 Step 226000/287113 — Loss: 9.8826\n",
      "Epoch 1 Step 228000/287113 — Loss: 9.8771\n",
      "Epoch 1 Step 230000/287113 — Loss: 9.8716\n",
      "Epoch 1 Step 232000/287113 — Loss: 9.8662\n",
      "Epoch 1 Step 234000/287113 — Loss: 9.8607\n",
      "Epoch 1 Step 236000/287113 — Loss: 9.8552\n",
      "Epoch 1 Step 238000/287113 — Loss: 9.8497\n",
      "Epoch 1 Step 240000/287113 — Loss: 9.8442\n",
      "Epoch 1 Step 242000/287113 — Loss: 9.8387\n",
      "Epoch 1 Step 244000/287113 — Loss: 9.8332\n",
      "Epoch 1 Step 246000/287113 — Loss: 9.8278\n",
      "Epoch 1 Step 248000/287113 — Loss: 9.8223\n",
      "Epoch 1 Step 250000/287113 — Loss: 9.8168\n",
      "Epoch 1 Step 252000/287113 — Loss: 9.8113\n",
      "Epoch 1 Step 254000/287113 — Loss: 9.8058\n",
      "Epoch 1 Step 256000/287113 — Loss: 9.8003\n",
      "Epoch 1 Step 258000/287113 — Loss: 9.7948\n",
      "Epoch 1 Step 260000/287113 — Loss: 9.7894\n",
      "Epoch 1 Step 262000/287113 — Loss: 9.7839\n",
      "Epoch 1 Step 264000/287113 — Loss: 9.7784\n",
      "Epoch 1 Step 266000/287113 — Loss: 9.7729\n",
      "Epoch 1 Step 268000/287113 — Loss: 9.7674\n",
      "Epoch 1 Step 270000/287113 — Loss: 9.7619\n",
      "Epoch 1 Step 272000/287113 — Loss: 9.7564\n",
      "Epoch 1 Step 274000/287113 — Loss: 9.7510\n",
      "Epoch 1 Step 276000/287113 — Loss: 9.7455\n",
      "Epoch 1 Step 278000/287113 — Loss: 9.7400\n",
      "Epoch 1 Step 280000/287113 — Loss: 9.7345\n",
      "Epoch 1 Step 282000/287113 — Loss: 9.7290\n",
      "Epoch 1 Step 284000/287113 — Loss: 9.7235\n",
      "Epoch 1 Step 286000/287113 — Loss: 9.7181\n",
      "\n",
      "=== Epoch 1/3 ===\n",
      "Train Loss: 9.7150\n",
      " Val Loss:  9.5855\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROUGE-1:            0.4038\n",
      "ROUGE-2:            0.2314\n",
      "ROUGE-L:            0.4228\n",
      "BERTScore Precision:0.6621\n",
      "BERTScore Recall:   0.7497\n",
      "BERTScore F1:       0.7032\n",
      "\n",
      "Epoch 2 Step 2000/287113 — Loss: 8.9158\n",
      "Epoch 2 Step 4000/287113 — Loss: 8.9111\n",
      "Epoch 2 Step 6000/287113 — Loss: 8.9065\n",
      "Epoch 2 Step 8000/287113 — Loss: 8.9018\n",
      "Epoch 2 Step 10000/287113 — Loss: 8.8972\n",
      "Epoch 2 Step 12000/287113 — Loss: 8.8925\n",
      "Epoch 2 Step 14000/287113 — Loss: 8.8879\n",
      "Epoch 2 Step 16000/287113 — Loss: 8.8832\n",
      "Epoch 2 Step 18000/287113 — Loss: 8.8785\n",
      "Epoch 2 Step 20000/287113 — Loss: 8.8739\n",
      "Epoch 2 Step 22000/287113 — Loss: 8.8692\n",
      "Epoch 2 Step 24000/287113 — Loss: 8.8646\n",
      "Epoch 2 Step 26000/287113 — Loss: 8.8599\n",
      "Epoch 2 Step 28000/287113 — Loss: 8.8552\n",
      "Epoch 2 Step 30000/287113 — Loss: 8.8506\n",
      "Epoch 2 Step 32000/287113 — Loss: 8.8459\n",
      "Epoch 2 Step 34000/287113 — Loss: 8.8413\n",
      "Epoch 2 Step 36000/287113 — Loss: 8.8366\n",
      "Epoch 2 Step 38000/287113 — Loss: 8.8320\n",
      "Epoch 2 Step 40000/287113 — Loss: 8.8273\n",
      "Epoch 2 Step 42000/287113 — Loss: 8.8226\n",
      "Epoch 2 Step 44000/287113 — Loss: 8.8180\n",
      "Epoch 2 Step 46000/287113 — Loss: 8.8133\n",
      "Epoch 2 Step 48000/287113 — Loss: 8.8087\n",
      "Epoch 2 Step 50000/287113 — Loss: 8.8040\n",
      "Epoch 2 Step 52000/287113 — Loss: 8.7993\n",
      "Epoch 2 Step 54000/287113 — Loss: 8.7947\n",
      "Epoch 2 Step 56000/287113 — Loss: 8.7900\n",
      "Epoch 2 Step 58000/287113 — Loss: 8.7854\n",
      "Epoch 2 Step 60000/287113 — Loss: 8.7807\n",
      "Epoch 2 Step 62000/287113 — Loss: 8.7760\n",
      "Epoch 2 Step 64000/287113 — Loss: 8.7714\n",
      "Epoch 2 Step 66000/287113 — Loss: 8.7667\n",
      "Epoch 2 Step 68000/287113 — Loss: 8.7621\n",
      "Epoch 2 Step 70000/287113 — Loss: 8.7574\n",
      "Epoch 2 Step 72000/287113 — Loss: 8.7527\n",
      "Epoch 2 Step 74000/287113 — Loss: 8.7481\n",
      "Epoch 2 Step 76000/287113 — Loss: 8.7434\n",
      "Epoch 2 Step 78000/287113 — Loss: 8.7388\n",
      "Epoch 2 Step 80000/287113 — Loss: 8.7341\n",
      "Epoch 2 Step 82000/287113 — Loss: 8.7295\n",
      "Epoch 2 Step 84000/287113 — Loss: 8.7248\n",
      "Epoch 2 Step 86000/287113 — Loss: 8.7201\n",
      "Epoch 2 Step 88000/287113 — Loss: 8.7155\n",
      "Epoch 2 Step 90000/287113 — Loss: 8.7108\n",
      "Epoch 2 Step 92000/287113 — Loss: 8.7062\n",
      "Epoch 2 Step 94000/287113 — Loss: 8.7015\n",
      "Epoch 2 Step 96000/287113 — Loss: 8.6968\n",
      "Epoch 2 Step 98000/287113 — Loss: 8.6922\n",
      "Epoch 2 Step 100000/287113 — Loss: 8.6875\n",
      "Epoch 2 Step 102000/287113 — Loss: 8.6829\n",
      "Epoch 2 Step 104000/287113 — Loss: 8.6782\n",
      "Epoch 2 Step 106000/287113 — Loss: 8.6735\n",
      "Epoch 2 Step 108000/287113 — Loss: 8.6689\n",
      "Epoch 2 Step 110000/287113 — Loss: 8.6642\n",
      "Epoch 2 Step 112000/287113 — Loss: 8.6596\n",
      "Epoch 2 Step 114000/287113 — Loss: 8.6549\n",
      "Epoch 2 Step 116000/287113 — Loss: 8.6502\n",
      "Epoch 2 Step 118000/287113 — Loss: 8.6456\n",
      "Epoch 2 Step 120000/287113 — Loss: 8.6409\n",
      "Epoch 2 Step 122000/287113 — Loss: 8.6363\n",
      "Epoch 2 Step 124000/287113 — Loss: 8.6316\n",
      "Epoch 2 Step 126000/287113 — Loss: 8.6269\n",
      "Epoch 2 Step 128000/287113 — Loss: 8.6223\n",
      "Epoch 2 Step 130000/287113 — Loss: 8.6176\n",
      "Epoch 2 Step 132000/287113 — Loss: 8.6130\n",
      "Epoch 2 Step 134000/287113 — Loss: 8.6083\n",
      "Epoch 2 Step 136000/287113 — Loss: 8.6037\n",
      "Epoch 2 Step 138000/287113 — Loss: 8.5990\n",
      "Epoch 2 Step 140000/287113 — Loss: 8.5943\n",
      "Epoch 2 Step 142000/287113 — Loss: 8.5897\n",
      "Epoch 2 Step 144000/287113 — Loss: 8.5850\n",
      "Epoch 2 Step 146000/287113 — Loss: 8.5804\n",
      "Epoch 2 Step 148000/287113 — Loss: 8.5757\n",
      "Epoch 2 Step 150000/287113 — Loss: 8.5710\n",
      "Epoch 2 Step 152000/287113 — Loss: 8.5664\n",
      "Epoch 2 Step 154000/287113 — Loss: 8.5617\n",
      "Epoch 2 Step 156000/287113 — Loss: 8.5571\n",
      "Epoch 2 Step 158000/287113 — Loss: 8.5524\n",
      "Epoch 2 Step 160000/287113 — Loss: 8.5477\n",
      "Epoch 2 Step 162000/287113 — Loss: 8.5431\n",
      "Epoch 2 Step 164000/287113 — Loss: 8.5384\n",
      "Epoch 2 Step 166000/287113 — Loss: 8.5338\n",
      "Epoch 2 Step 168000/287113 — Loss: 8.5291\n",
      "Epoch 2 Step 170000/287113 — Loss: 8.5244\n",
      "Epoch 2 Step 172000/287113 — Loss: 8.5198\n",
      "Epoch 2 Step 174000/287113 — Loss: 8.5151\n",
      "Epoch 2 Step 176000/287113 — Loss: 8.5105\n",
      "Epoch 2 Step 178000/287113 — Loss: 8.5058\n",
      "Epoch 2 Step 180000/287113 — Loss: 8.5011\n",
      "Epoch 2 Step 182000/287113 — Loss: 8.4965\n",
      "Epoch 2 Step 184000/287113 — Loss: 8.4918\n",
      "Epoch 2 Step 186000/287113 — Loss: 8.4872\n",
      "Epoch 2 Step 188000/287113 — Loss: 8.4825\n",
      "Epoch 2 Step 190000/287113 — Loss: 8.4779\n",
      "Epoch 2 Step 192000/287113 — Loss: 8.4732\n",
      "Epoch 2 Step 194000/287113 — Loss: 8.4685\n",
      "Epoch 2 Step 196000/287113 — Loss: 8.4639\n",
      "Epoch 2 Step 198000/287113 — Loss: 8.4592\n",
      "Epoch 2 Step 200000/287113 — Loss: 8.4546\n",
      "Epoch 2 Step 202000/287113 — Loss: 8.4499\n",
      "Epoch 2 Step 204000/287113 — Loss: 8.4452\n",
      "Epoch 2 Step 206000/287113 — Loss: 8.4406\n",
      "Epoch 2 Step 208000/287113 — Loss: 8.4359\n",
      "Epoch 2 Step 210000/287113 — Loss: 8.4313\n",
      "Epoch 2 Step 212000/287113 — Loss: 8.4266\n",
      "Epoch 2 Step 214000/287113 — Loss: 8.4219\n",
      "Epoch 2 Step 216000/287113 — Loss: 8.4173\n",
      "Epoch 2 Step 218000/287113 — Loss: 8.4126\n",
      "Epoch 2 Step 220000/287113 — Loss: 8.4080\n",
      "Epoch 2 Step 222000/287113 — Loss: 8.4033\n",
      "Epoch 2 Step 224000/287113 — Loss: 8.3986\n",
      "Epoch 2 Step 226000/287113 — Loss: 8.3940\n",
      "Epoch 2 Step 228000/287113 — Loss: 8.3893\n",
      "Epoch 2 Step 230000/287113 — Loss: 8.3847\n",
      "Epoch 2 Step 232000/287113 — Loss: 8.3800\n",
      "Epoch 2 Step 234000/287113 — Loss: 8.3754\n",
      "Epoch 2 Step 236000/287113 — Loss: 8.3707\n",
      "Epoch 2 Step 238000/287113 — Loss: 8.3660\n",
      "Epoch 2 Step 240000/287113 — Loss: 8.3614\n",
      "Epoch 2 Step 242000/287113 — Loss: 8.3567\n",
      "Epoch 2 Step 244000/287113 — Loss: 8.3521\n",
      "Epoch 2 Step 246000/287113 — Loss: 8.3474\n",
      "Epoch 2 Step 248000/287113 — Loss: 8.3427\n",
      "Epoch 2 Step 250000/287113 — Loss: 8.3381\n",
      "Epoch 2 Step 252000/287113 — Loss: 8.3334\n",
      "Epoch 2 Step 254000/287113 — Loss: 8.3288\n",
      "Epoch 2 Step 256000/287113 — Loss: 8.3241\n",
      "Epoch 2 Step 258000/287113 — Loss: 8.3194\n",
      "Epoch 2 Step 260000/287113 — Loss: 8.3148\n",
      "Epoch 2 Step 262000/287113 — Loss: 8.3101\n",
      "Epoch 2 Step 264000/287113 — Loss: 8.3055\n",
      "Epoch 2 Step 266000/287113 — Loss: 8.3008\n",
      "Epoch 2 Step 268000/287113 — Loss: 8.2961\n",
      "Epoch 2 Step 270000/287113 — Loss: 8.2915\n",
      "Epoch 2 Step 272000/287113 — Loss: 8.2868\n",
      "Epoch 2 Step 274000/287113 — Loss: 8.2822\n",
      "Epoch 2 Step 276000/287113 — Loss: 8.2775\n",
      "Epoch 2 Step 278000/287113 — Loss: 8.2728\n",
      "Epoch 2 Step 280000/287113 — Loss: 8.2682\n",
      "Epoch 2 Step 282000/287113 — Loss: 8.2635\n",
      "Epoch 2 Step 284000/287113 — Loss: 8.2589\n",
      "Epoch 2 Step 286000/287113 — Loss: 8.2542\n",
      "\n",
      "=== Epoch 2/3 ===\n",
      "Train Loss: 8.2516\n",
      " Val Loss:  8.1548\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROUGE-1:            0.4197\n",
      "ROUGE-2:            0.2605\n",
      "ROUGE-L:            0.3963\n",
      "BERTScore Precision:0.7098\n",
      "BERTScore Recall:   0.7718\n",
      "BERTScore F1:       0.7395\n",
      "\n",
      "Epoch 3 Step 2000/287113 — Loss: 7.5662\n",
      "Epoch 3 Step 4000/287113 — Loss: 7.5623\n",
      "Epoch 3 Step 6000/287113 — Loss: 7.5583\n",
      "Epoch 3 Step 8000/287113 — Loss: 7.5544\n",
      "Epoch 3 Step 10000/287113 — Loss: 7.5504\n",
      "Epoch 3 Step 12000/287113 — Loss: 7.5465\n",
      "Epoch 3 Step 14000/287113 — Loss: 7.5425\n",
      "Epoch 3 Step 16000/287113 — Loss: 7.5385\n",
      "Epoch 3 Step 18000/287113 — Loss: 7.5346\n",
      "Epoch 3 Step 20000/287113 — Loss: 7.5306\n",
      "Epoch 3 Step 22000/287113 — Loss: 7.5267\n",
      "Epoch 3 Step 24000/287113 — Loss: 7.5228\n",
      "Epoch 3 Step 26000/287113 — Loss: 7.5188\n",
      "Epoch 3 Step 28000/287113 — Loss: 7.5148\n",
      "Epoch 3 Step 30000/287113 — Loss: 7.5109\n",
      "Epoch 3 Step 32000/287113 — Loss: 7.5069\n",
      "Epoch 3 Step 34000/287113 — Loss: 7.5030\n",
      "Epoch 3 Step 36000/287113 — Loss: 7.4990\n",
      "Epoch 3 Step 38000/287113 — Loss: 7.4951\n",
      "Epoch 3 Step 40000/287113 — Loss: 7.4911\n",
      "Epoch 3 Step 42000/287113 — Loss: 7.4872\n",
      "Epoch 3 Step 44000/287113 — Loss: 7.4832\n",
      "Epoch 3 Step 46000/287113 — Loss: 7.4793\n",
      "Epoch 3 Step 48000/287113 — Loss: 7.4753\n",
      "Epoch 3 Step 50000/287113 — Loss: 7.4714\n",
      "Epoch 3 Step 52000/287113 — Loss: 7.4674\n",
      "Epoch 3 Step 54000/287113 — Loss: 7.4634\n",
      "Epoch 3 Step 56000/287113 — Loss: 7.4595\n",
      "Epoch 3 Step 58000/287113 — Loss: 7.4555\n",
      "Epoch 3 Step 60000/287113 — Loss: 7.4516\n",
      "Epoch 3 Step 62000/287113 — Loss: 7.4476\n",
      "Epoch 3 Step 64000/287113 — Loss: 7.4437\n",
      "Epoch 3 Step 66000/287113 — Loss: 7.4397\n",
      "Epoch 3 Step 68000/287113 — Loss: 7.4358\n",
      "Epoch 3 Step 70000/287113 — Loss: 7.4318\n",
      "Epoch 3 Step 72000/287113 — Loss: 7.4279\n",
      "Epoch 3 Step 74000/287113 — Loss: 7.4239\n",
      "Epoch 3 Step 76000/287113 — Loss: 7.4200\n",
      "Epoch 3 Step 78000/287113 — Loss: 7.4160\n",
      "Epoch 3 Step 80000/287113 — Loss: 7.4120\n",
      "Epoch 3 Step 82000/287113 — Loss: 7.4081\n",
      "Epoch 3 Step 84000/287113 — Loss: 7.4041\n",
      "Epoch 3 Step 86000/287113 — Loss: 7.4002\n",
      "Epoch 3 Step 88000/287113 — Loss: 7.3962\n",
      "Epoch 3 Step 90000/287113 — Loss: 7.3923\n",
      "Epoch 3 Step 92000/287113 — Loss: 7.3883\n",
      "Epoch 3 Step 94000/287113 — Loss: 7.3844\n",
      "Epoch 3 Step 96000/287113 — Loss: 7.3804\n",
      "Epoch 3 Step 98000/287113 — Loss: 7.3765\n",
      "Epoch 3 Step 100000/287113 — Loss: 7.3725\n",
      "Epoch 3 Step 102000/287113 — Loss: 7.3686\n",
      "Epoch 3 Step 104000/287113 — Loss: 7.3646\n",
      "Epoch 3 Step 106000/287113 — Loss: 7.3606\n",
      "Epoch 3 Step 108000/287113 — Loss: 7.3567\n",
      "Epoch 3 Step 110000/287113 — Loss: 7.3527\n",
      "Epoch 3 Step 112000/287113 — Loss: 7.3488\n",
      "Epoch 3 Step 114000/287113 — Loss: 7.3448\n",
      "Epoch 3 Step 116000/287113 — Loss: 7.3409\n",
      "Epoch 3 Step 118000/287113 — Loss: 7.3369\n",
      "Epoch 3 Step 120000/287113 — Loss: 7.3330\n",
      "Epoch 3 Step 122000/287113 — Loss: 7.3290\n",
      "Epoch 3 Step 124000/287113 — Loss: 7.3251\n",
      "Epoch 3 Step 126000/287113 — Loss: 7.3211\n",
      "Epoch 3 Step 128000/287113 — Loss: 7.3172\n",
      "Epoch 3 Step 130000/287113 — Loss: 7.3132\n",
      "Epoch 3 Step 132000/287113 — Loss: 7.3093\n",
      "Epoch 3 Step 134000/287113 — Loss: 7.3053\n",
      "Epoch 3 Step 136000/287113 — Loss: 7.3013\n",
      "Epoch 3 Step 138000/287113 — Loss: 7.2974\n",
      "Epoch 3 Step 140000/287113 — Loss: 7.2934\n",
      "Epoch 3 Step 142000/287113 — Loss: 7.2895\n",
      "Epoch 3 Step 144000/287113 — Loss: 7.2855\n",
      "Epoch 3 Step 146000/287113 — Loss: 7.2816\n",
      "Epoch 3 Step 148000/287113 — Loss: 7.2776\n",
      "Epoch 3 Step 150000/287113 — Loss: 7.2737\n",
      "Epoch 3 Step 152000/287113 — Loss: 7.2697\n",
      "Epoch 3 Step 154000/287113 — Loss: 7.2658\n",
      "Epoch 3 Step 156000/287113 — Loss: 7.2618\n",
      "Epoch 3 Step 158000/287113 — Loss: 7.2579\n",
      "Epoch 3 Step 160000/287113 — Loss: 7.2539\n",
      "Epoch 3 Step 162000/287113 — Loss: 7.2499\n",
      "Epoch 3 Step 164000/287113 — Loss: 7.2460\n",
      "Epoch 3 Step 166000/287113 — Loss: 7.2420\n",
      "Epoch 3 Step 168000/287113 — Loss: 7.2381\n",
      "Epoch 3 Step 170000/287113 — Loss: 7.2341\n",
      "Epoch 3 Step 172000/287113 — Loss: 7.2302\n",
      "Epoch 3 Step 174000/287113 — Loss: 7.2262\n",
      "Epoch 3 Step 176000/287113 — Loss: 7.2223\n",
      "Epoch 3 Step 178000/287113 — Loss: 7.2183\n",
      "Epoch 3 Step 180000/287113 — Loss: 7.2144\n",
      "Epoch 3 Step 182000/287113 — Loss: 7.2104\n",
      "Epoch 3 Step 184000/287113 — Loss: 7.2065\n",
      "Epoch 3 Step 186000/287113 — Loss: 7.2025\n",
      "Epoch 3 Step 188000/287113 — Loss: 7.1985\n",
      "Epoch 3 Step 190000/287113 — Loss: 7.1946\n",
      "Epoch 3 Step 192000/287113 — Loss: 7.1906\n",
      "Epoch 3 Step 194000/287113 — Loss: 7.1867\n",
      "Epoch 3 Step 196000/287113 — Loss: 7.1827\n",
      "Epoch 3 Step 198000/287113 — Loss: 7.1788\n",
      "Epoch 3 Step 200000/287113 — Loss: 7.1748\n",
      "Epoch 3 Step 202000/287113 — Loss: 7.1709\n",
      "Epoch 3 Step 204000/287113 — Loss: 7.1669\n",
      "Epoch 3 Step 206000/287113 — Loss: 7.1630\n",
      "Epoch 3 Step 208000/287113 — Loss: 7.1590\n",
      "Epoch 3 Step 210000/287113 — Loss: 7.1551\n",
      "Epoch 3 Step 212000/287113 — Loss: 7.1511\n",
      "Epoch 3 Step 214000/287113 — Loss: 7.1471\n",
      "Epoch 3 Step 216000/287113 — Loss: 7.1432\n",
      "Epoch 3 Step 218000/287113 — Loss: 7.1392\n",
      "Epoch 3 Step 220000/287113 — Loss: 7.1353\n",
      "Epoch 3 Step 222000/287113 — Loss: 7.1313\n",
      "Epoch 3 Step 224000/287113 — Loss: 7.1274\n",
      "Epoch 3 Step 226000/287113 — Loss: 7.1234\n",
      "Epoch 3 Step 228000/287113 — Loss: 7.1195\n",
      "Epoch 3 Step 230000/287113 — Loss: 7.1155\n",
      "Epoch 3 Step 232000/287113 — Loss: 7.1116\n",
      "Epoch 3 Step 234000/287113 — Loss: 7.1076\n",
      "Epoch 3 Step 236000/287113 — Loss: 7.1037\n",
      "Epoch 3 Step 238000/287113 — Loss: 7.0997\n",
      "Epoch 3 Step 240000/287113 — Loss: 7.0957\n",
      "Epoch 3 Step 242000/287113 — Loss: 7.0918\n",
      "Epoch 3 Step 244000/287113 — Loss: 7.0878\n",
      "Epoch 3 Step 246000/287113 — Loss: 7.0839\n",
      "Epoch 3 Step 248000/287113 — Loss: 7.0799\n",
      "Epoch 3 Step 250000/287113 — Loss: 7.0760\n",
      "Epoch 3 Step 252000/287113 — Loss: 7.0720\n",
      "Epoch 3 Step 254000/287113 — Loss: 7.0681\n",
      "Epoch 3 Step 256000/287113 — Loss: 7.0641\n",
      "Epoch 3 Step 258000/287113 — Loss: 7.0602\n",
      "Epoch 3 Step 260000/287113 — Loss: 7.0562\n",
      "Epoch 3 Step 262000/287113 — Loss: 7.0523\n",
      "Epoch 3 Step 264000/287113 — Loss: 7.0483\n",
      "Epoch 3 Step 266000/287113 — Loss: 7.0443\n",
      "Epoch 3 Step 268000/287113 — Loss: 7.0404\n",
      "Epoch 3 Step 270000/287113 — Loss: 7.0364\n",
      "Epoch 3 Step 272000/287113 — Loss: 7.0325\n",
      "Epoch 3 Step 274000/287113 — Loss: 7.0285\n",
      "Epoch 3 Step 276000/287113 — Loss: 7.0246\n",
      "Epoch 3 Step 278000/287113 — Loss: 7.0206\n",
      "Epoch 3 Step 280000/287113 — Loss: 7.0167\n",
      "Epoch 3 Step 282000/287113 — Loss: 7.0127\n",
      "Epoch 3 Step 284000/287113 — Loss: 7.0088\n",
      "Epoch 3 Step 286000/287113 — Loss: 7.0048\n",
      "\n",
      "=== Epoch 3/3 ===\n",
      "Train Loss: 7.0026\n",
      " Val Loss:  6.8715\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROUGE-1:            0.5195\n",
      "ROUGE-2:            0.3121\n",
      "ROUGE-L:            0.4441\n",
      "BERTScore Precision:0.7734\n",
      "BERTScore Recall:   0.7235\n",
      "BERTScore F1:       0.7476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
    "    train_loss = train_epoch(train_loader, epoch)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_loss = eval_epoch(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    evaluate_model(val_loader, f\"Validation (Epoch {epoch})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d76473e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.0231\n",
      "=== Test Evaluation ===\n",
      "ROUGE-1: 0.2549\n",
      "ROUGE-2: 0.1366\n",
      "ROUGE-L: 0.1971\n",
      "BERTScore F1: 0.4122\n",
      "BERTScore Precision: 0.3744\n",
      "BERTScore Recall: 0.4006\n"
     ]
    }
   ],
   "source": [
    "test_loss = eval_epoch(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "rouge_results, bert_score = evaluate_model(test_loader, \"Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
