{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70286db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from evaluate import load\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7ffd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('rajpurkar/squad')\n",
    "split = dataset['validation'].train_test_split(test_size=0.5, seed=42)\n",
    "raw_datasets = {\n",
    "    'train': dataset['train'],\n",
    "    'validation': split['train'],\n",
    "    'test': split['test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46af2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "max_length = 384\n",
    "doc_stride = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad38ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['question'], examples['context'],\n",
    "        truncation='only_second',\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    overflow_to_sample_mapping = tokenized.pop('overflow_to_sample_mapping')\n",
    "    offset_mapping = tokenized.pop('offset_mapping')\n",
    "\n",
    "    tokenized['start_positions'] = []\n",
    "    tokenized['end_positions'] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_idx = overflow_to_sample_mapping[i]\n",
    "        answers = examples['answers'][sample_idx]\n",
    "        cls_index = tokenized['input_ids'][i].index(tokenizer.cls_token_id)\n",
    "\n",
    "        if len(answers['answer_start']) == 0:\n",
    "            tokenized['start_positions'].append(cls_index)\n",
    "            tokenized['end_positions'].append(cls_index)\n",
    "        else:\n",
    "            start_char = answers['answer_start'][0]\n",
    "            end_char = start_char + len(answers['text'][0])\n",
    "            sequence_ids = tokenized.sequence_ids(i)\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(tokenized['input_ids'][i]) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if offsets[token_start_index][0] > end_char or offsets[token_end_index][1] < start_char:\n",
    "                tokenized['start_positions'].append(cls_index)\n",
    "                tokenized['end_positions'].append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized['start_positions'].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized['end_positions'].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a7a3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bd5b1b648e45df988714a18e106b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = {k: raw_datasets[k].map(\n",
    "    prepare_features,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[k].column_names\n",
    ") for k in ['train', 'validation', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6703eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(row['input_ids']),\n",
    "            'attention_mask': torch.tensor(row['attention_mask']),\n",
    "            'start_positions': torch.tensor(row['start_positions']),\n",
    "            'end_positions': torch.tensor(row['end_positions'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba5d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QADataset(tokenized_datasets['train'])\n",
    "val_dataset   = QADataset(tokenized_datasets['validation'])\n",
    "test_dataset  = QADataset(tokenized_datasets['test'])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d139c44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['query', 'value', 'dense']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef10aee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForQuestionAnswering(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BertForQuestionAnswering(\n",
       "      (bert): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (qa_outputs): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(base_model, lora_config)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora_' not in name:\n",
    "        param.requires_grad = False\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599ebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10bao\\AppData\\Local\\Temp\\ipykernel_18860\\3277783042.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metric = load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd2e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_pos = batch['start_positions'].to(device)\n",
    "        end_pos = batch['end_positions'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "            loss = (criterion(start_logits, start_pos) + criterion(end_logits, end_pos)) / 2\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} Step {step}/{len(train_loader)} - Loss: {running_loss/step:.4f}\")\n",
    "\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e52ae9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(data_loader, raw_data):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_pos = batch['start_positions'].to(device)\n",
    "        end_pos = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "        eval_loss += ((criterion(start_logits, start_pos) + criterion(end_logits, end_pos)) / 2).item()\n",
    "\n",
    "        for b in range(len(input_ids)):\n",
    "            start_idx = torch.argmax(start_logits[b]).item()\n",
    "            end_idx = torch.argmax(end_logits[b]).item()\n",
    "            if start_idx > end_idx:\n",
    "                answer = \"\"\n",
    "            else:\n",
    "                tokens = input_ids[b][start_idx:end_idx+1]\n",
    "                answer = tokenizer.decode(tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "            sample_index = i * data_loader.batch_size + b\n",
    "            if sample_index < len(raw_data):\n",
    "                predictions.append({\"id\": raw_data[sample_index][\"id\"], \"prediction_text\": answer})\n",
    "                references.append({\"id\": raw_data[sample_index][\"id\"], \"answers\": raw_data[sample_index][\"answers\"]})\n",
    "\n",
    "    metrics = metric.compute(predictions=predictions, references=references)\n",
    "    return eval_loss / len(data_loader), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bebaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/3 ===\n",
      "Epoch 1 Step 100/11066 - Loss: 5.6841\n",
      "Epoch 1 Step 200/11066 - Loss: 5.5798\n",
      "Epoch 1 Step 300/11066 - Loss: 5.5353\n",
      "Epoch 1 Step 400/11066 - Loss: 5.4284\n",
      "Epoch 1 Step 500/11066 - Loss: 5.3769\n",
      "Epoch 1 Step 600/11066 - Loss: 5.2902\n",
      "Epoch 1 Step 700/11066 - Loss: 5.1808\n",
      "Epoch 1 Step 800/11066 - Loss: 5.1156\n",
      "Epoch 1 Step 900/11066 - Loss: 5.0580\n",
      "Epoch 1 Step 1000/11066 - Loss: 4.9739\n",
      "Epoch 1 Step 1100/11066 - Loss: 4.9443\n",
      "Epoch 1 Step 1200/11066 - Loss: 4.9132\n",
      "Epoch 1 Step 1300/11066 - Loss: 4.8123\n",
      "Epoch 1 Step 1400/11066 - Loss: 4.7338\n",
      "Epoch 1 Step 1500/11066 - Loss: 4.6851\n",
      "Epoch 1 Step 1600/11066 - Loss: 4.6096\n",
      "Epoch 1 Step 1700/11066 - Loss: 4.5711\n",
      "Epoch 1 Step 1800/11066 - Loss: 4.5051\n",
      "Epoch 1 Step 1900/11066 - Loss: 4.4612\n",
      "Epoch 1 Step 2000/11066 - Loss: 4.4023\n",
      "Epoch 1 Step 2100/11066 - Loss: 4.3590\n",
      "Epoch 1 Step 2200/11066 - Loss: 4.2943\n",
      "Epoch 1 Step 2300/11066 - Loss: 4.2329\n",
      "Epoch 1 Step 2400/11066 - Loss: 4.1827\n",
      "Epoch 1 Step 2500/11066 - Loss: 4.1066\n",
      "Epoch 1 Step 2600/11066 - Loss: 4.0637\n",
      "Epoch 1 Step 2700/11066 - Loss: 4.0325\n",
      "Epoch 1 Step 2800/11066 - Loss: 3.9659\n",
      "Epoch 1 Step 2900/11066 - Loss: 3.9518\n",
      "Epoch 1 Step 3000/11066 - Loss: 3.9293\n",
      "Epoch 1 Step 3100/11066 - Loss: 3.8968\n",
      "Epoch 1 Step 3200/11066 - Loss: 3.8457\n",
      "Epoch 1 Step 3300/11066 - Loss: 3.8201\n",
      "Epoch 1 Step 3400/11066 - Loss: 3.7557\n",
      "Epoch 1 Step 3500/11066 - Loss: 3.7108\n",
      "Epoch 1 Step 3600/11066 - Loss: 3.6380\n",
      "Epoch 1 Step 3700/11066 - Loss: 3.5807\n",
      "Epoch 1 Step 3800/11066 - Loss: 3.5390\n",
      "Epoch 1 Step 3900/11066 - Loss: 3.4675\n",
      "Epoch 1 Step 4000/11066 - Loss: 3.4556\n",
      "Epoch 1 Step 4100/11066 - Loss: 3.4132\n",
      "Epoch 1 Step 4200/11066 - Loss: 3.3789\n",
      "Epoch 1 Step 4300/11066 - Loss: 3.3075\n",
      "Epoch 1 Step 4400/11066 - Loss: 3.2631\n",
      "Epoch 1 Step 4500/11066 - Loss: 3.2350\n",
      "Epoch 1 Step 4600/11066 - Loss: 3.2004\n",
      "Epoch 1 Step 4700/11066 - Loss: 3.1787\n",
      "Epoch 1 Step 4800/11066 - Loss: 3.1249\n",
      "Epoch 1 Step 4900/11066 - Loss: 3.0812\n",
      "Epoch 1 Step 5000/11066 - Loss: 3.0109\n",
      "Epoch 1 Step 5100/11066 - Loss: 2.9670\n",
      "Epoch 1 Step 5200/11066 - Loss: 2.9532\n",
      "Epoch 1 Step 5300/11066 - Loss: 2.9113\n",
      "Epoch 1 Step 5400/11066 - Loss: 2.8620\n",
      "Epoch 1 Step 5500/11066 - Loss: 2.8482\n",
      "Epoch 1 Step 5600/11066 - Loss: 2.8181\n",
      "Epoch 1 Step 5700/11066 - Loss: 2.8229\n",
      "Epoch 1 Step 5800/11066 - Loss: 2.7918\n",
      "Epoch 1 Step 5900/11066 - Loss: 2.7709\n",
      "Epoch 1 Step 6000/11066 - Loss: 2.7476\n",
      "Epoch 1 Step 6100/11066 - Loss: 2.7227\n",
      "Epoch 1 Step 6200/11066 - Loss: 2.7218\n",
      "Epoch 1 Step 6300/11066 - Loss: 2.6866\n",
      "Epoch 1 Step 6400/11066 - Loss: 2.6799\n",
      "Epoch 1 Step 6500/11066 - Loss: 2.6817\n",
      "Epoch 1 Step 6600/11066 - Loss: 2.6278\n",
      "Epoch 1 Step 6700/11066 - Loss: 2.6107\n",
      "Epoch 1 Step 6800/11066 - Loss: 2.5715\n",
      "Epoch 1 Step 6900/11066 - Loss: 2.5521\n",
      "Epoch 1 Step 7000/11066 - Loss: 2.4817\n",
      "Epoch 1 Step 7100/11066 - Loss: 2.4509\n",
      "Epoch 1 Step 7200/11066 - Loss: 2.4250\n",
      "Epoch 1 Step 7300/11066 - Loss: 2.4132\n",
      "Epoch 1 Step 7400/11066 - Loss: 2.3675\n",
      "Epoch 1 Step 7500/11066 - Loss: 2.3005\n",
      "Epoch 1 Step 7600/11066 - Loss: 2.2533\n",
      "Epoch 1 Step 7700/11066 - Loss: 2.1908\n",
      "Epoch 1 Step 7800/11066 - Loss: 2.1774\n",
      "Epoch 1 Step 7900/11066 - Loss: 2.1181\n",
      "Epoch 1 Step 8000/11066 - Loss: 2.1335\n",
      "Epoch 1 Step 8100/11066 - Loss: 2.0882\n",
      "Epoch 1 Step 8200/11066 - Loss: 2.0422\n",
      "Epoch 1 Step 8300/11066 - Loss: 2.0353\n",
      "Epoch 1 Step 8400/11066 - Loss: 2.0025\n",
      "Epoch 1 Step 8500/11066 - Loss: 1.9523\n",
      "Epoch 1 Step 8600/11066 - Loss: 1.9647\n",
      "Epoch 1 Step 8700/11066 - Loss: 1.9711\n",
      "Epoch 1 Step 8800/11066 - Loss: 1.9440\n",
      "Epoch 1 Step 8900/11066 - Loss: 1.9338\n",
      "Epoch 1 Step 9000/11066 - Loss: 1.9076\n",
      "Epoch 1 Step 9100/11066 - Loss: 1.8723\n",
      "Epoch 1 Step 9200/11066 - Loss: 1.8894\n",
      "Epoch 1 Step 9300/11066 - Loss: 1.8453\n",
      "Epoch 1 Step 9400/11066 - Loss: 1.8388\n",
      "Epoch 1 Step 9500/11066 - Loss: 1.8566\n",
      "Epoch 1 Step 9600/11066 - Loss: 1.8490\n",
      "Epoch 1 Step 9700/11066 - Loss: 1.8424\n",
      "Epoch 1 Step 9800/11066 - Loss: 1.8389\n",
      "Epoch 1 Step 9900/11066 - Loss: 1.8319\n",
      "Epoch 1 Step 10000/11066 - Loss: 1.8419\n",
      "Epoch 1 Step 10100/11066 - Loss: 1.8180\n",
      "Epoch 1 Step 10200/11066 - Loss: 1.8029\n",
      "Epoch 1 Step 10300/11066 - Loss: 1.7833\n",
      "Epoch 1 Step 10400/11066 - Loss: 1.7423\n",
      "Epoch 1 Step 10500/11066 - Loss: 1.7005\n",
      "Epoch 1 Step 10600/11066 - Loss: 1.6916\n",
      "Epoch 1 Step 10700/11066 - Loss: 1.6707\n",
      "Epoch 1 Step 10800/11066 - Loss: 1.6416\n",
      "Epoch 1 Step 10900/11066 - Loss: 1.5922\n",
      "Epoch 1 Step 11000/11066 - Loss: 1.5654\n",
      "\n",
      "Train Loss: 1.5654\n",
      "Val Loss: 1.2865 | EM: 56.40 | F1: 55.09\n",
      "\n",
      "\n",
      "=== Epoch 2/3 ===\n",
      "Epoch 2 Step 100/11066 - Loss: 1.5630\n",
      "Epoch 2 Step 200/11066 - Loss: 1.5358\n",
      "Epoch 2 Step 300/11066 - Loss: 1.5195\n",
      "Epoch 2 Step 400/11066 - Loss: 1.5445\n",
      "Epoch 2 Step 500/11066 - Loss: 1.5400\n",
      "Epoch 2 Step 600/11066 - Loss: 1.5004\n",
      "Epoch 2 Step 700/11066 - Loss: 1.5100\n",
      "Epoch 2 Step 800/11066 - Loss: 1.4768\n",
      "Epoch 2 Step 900/11066 - Loss: 1.4693\n",
      "Epoch 2 Step 1000/11066 - Loss: 1.4629\n",
      "Epoch 2 Step 1100/11066 - Loss: 1.4956\n",
      "Epoch 2 Step 1200/11066 - Loss: 1.4825\n",
      "Epoch 2 Step 1300/11066 - Loss: 1.4905\n",
      "Epoch 2 Step 1400/11066 - Loss: 1.4826\n",
      "Epoch 2 Step 1500/11066 - Loss: 1.4445\n",
      "Epoch 2 Step 1600/11066 - Loss: 1.4508\n",
      "Epoch 2 Step 1700/11066 - Loss: 1.4286\n",
      "Epoch 2 Step 1800/11066 - Loss: 1.3980\n",
      "Epoch 2 Step 1900/11066 - Loss: 1.3857\n",
      "Epoch 2 Step 2000/11066 - Loss: 1.3994\n",
      "Epoch 2 Step 2100/11066 - Loss: 1.3867\n",
      "Epoch 2 Step 2200/11066 - Loss: 1.3824\n",
      "Epoch 2 Step 2300/11066 - Loss: 1.3496\n",
      "Epoch 2 Step 2400/11066 - Loss: 1.3338\n",
      "Epoch 2 Step 2500/11066 - Loss: 1.3121\n",
      "Epoch 2 Step 2600/11066 - Loss: 1.2924\n",
      "Epoch 2 Step 2700/11066 - Loss: 1.2950\n",
      "Epoch 2 Step 2800/11066 - Loss: 1.2925\n",
      "Epoch 2 Step 2900/11066 - Loss: 1.2725\n",
      "Epoch 2 Step 3000/11066 - Loss: 1.2828\n",
      "Epoch 2 Step 3100/11066 - Loss: 1.3031\n",
      "Epoch 2 Step 3200/11066 - Loss: 1.2617\n",
      "Epoch 2 Step 3300/11066 - Loss: 1.2453\n",
      "Epoch 2 Step 3400/11066 - Loss: 1.2354\n",
      "Epoch 2 Step 3500/11066 - Loss: 1.2024\n",
      "Epoch 2 Step 3600/11066 - Loss: 1.1752\n",
      "Epoch 2 Step 3700/11066 - Loss: 1.1869\n",
      "Epoch 2 Step 3800/11066 - Loss: 1.2143\n",
      "Epoch 2 Step 3900/11066 - Loss: 1.2174\n",
      "Epoch 2 Step 4000/11066 - Loss: 1.1803\n",
      "Epoch 2 Step 4100/11066 - Loss: 1.1821\n",
      "Epoch 2 Step 4200/11066 - Loss: 1.1631\n",
      "Epoch 2 Step 4300/11066 - Loss: 1.1957\n",
      "Epoch 2 Step 4400/11066 - Loss: 1.1680\n",
      "Epoch 2 Step 4500/11066 - Loss: 1.1521\n",
      "Epoch 2 Step 4600/11066 - Loss: 1.1254\n",
      "Epoch 2 Step 4700/11066 - Loss: 1.1243\n",
      "Epoch 2 Step 4800/11066 - Loss: 1.0902\n",
      "Epoch 2 Step 4900/11066 - Loss: 1.0902\n",
      "Epoch 2 Step 5000/11066 - Loss: 1.1020\n",
      "Epoch 2 Step 5100/11066 - Loss: 1.0980\n",
      "Epoch 2 Step 5200/11066 - Loss: 1.1139\n",
      "Epoch 2 Step 5300/11066 - Loss: 1.0860\n",
      "Epoch 2 Step 5400/11066 - Loss: 1.1119\n",
      "Epoch 2 Step 5500/11066 - Loss: 1.0872\n",
      "Epoch 2 Step 5600/11066 - Loss: 1.0716\n",
      "Epoch 2 Step 5700/11066 - Loss: 1.0802\n",
      "Epoch 2 Step 5800/11066 - Loss: 1.0578\n",
      "Epoch 2 Step 5900/11066 - Loss: 1.0216\n",
      "Epoch 2 Step 6000/11066 - Loss: 1.0450\n",
      "Epoch 2 Step 6100/11066 - Loss: 1.0125\n",
      "Epoch 2 Step 6200/11066 - Loss: 1.0206\n",
      "Epoch 2 Step 6300/11066 - Loss: 1.0504\n",
      "Epoch 2 Step 6400/11066 - Loss: 1.0092\n",
      "Epoch 2 Step 6500/11066 - Loss: 0.9744\n",
      "Epoch 2 Step 6600/11066 - Loss: 1.0068\n",
      "Epoch 2 Step 6700/11066 - Loss: 1.0112\n",
      "Epoch 2 Step 6800/11066 - Loss: 1.0218\n",
      "Epoch 2 Step 6900/11066 - Loss: 1.0480\n",
      "Epoch 2 Step 7000/11066 - Loss: 1.0827\n",
      "Epoch 2 Step 7100/11066 - Loss: 1.0678\n",
      "Epoch 2 Step 7200/11066 - Loss: 1.0970\n",
      "Epoch 2 Step 7300/11066 - Loss: 1.0738\n",
      "Epoch 2 Step 7400/11066 - Loss: 1.0923\n",
      "Epoch 2 Step 7500/11066 - Loss: 1.0875\n",
      "Epoch 2 Step 7600/11066 - Loss: 1.0934\n",
      "Epoch 2 Step 7700/11066 - Loss: 1.0863\n",
      "Epoch 2 Step 7800/11066 - Loss: 1.0783\n",
      "Epoch 2 Step 7900/11066 - Loss: 1.0702\n",
      "Epoch 2 Step 8000/11066 - Loss: 1.1022\n",
      "Epoch 2 Step 8100/11066 - Loss: 1.1032\n",
      "Epoch 2 Step 8200/11066 - Loss: 1.0835\n",
      "Epoch 2 Step 8300/11066 - Loss: 1.1199\n",
      "Epoch 2 Step 8400/11066 - Loss: 1.1093\n",
      "Epoch 2 Step 8500/11066 - Loss: 1.0676\n",
      "Epoch 2 Step 8600/11066 - Loss: 1.0912\n",
      "Epoch 2 Step 8700/11066 - Loss: 1.0615\n",
      "Epoch 2 Step 8800/11066 - Loss: 1.0776\n",
      "Epoch 2 Step 8900/11066 - Loss: 1.0886\n",
      "Epoch 2 Step 9000/11066 - Loss: 1.0729\n",
      "Epoch 2 Step 9100/11066 - Loss: 1.0523\n",
      "Epoch 2 Step 9200/11066 - Loss: 1.0479\n",
      "Epoch 2 Step 9300/11066 - Loss: 1.0698\n",
      "Epoch 2 Step 9400/11066 - Loss: 1.0507\n",
      "Epoch 2 Step 9500/11066 - Loss: 1.0701\n",
      "Epoch 2 Step 9600/11066 - Loss: 1.0854\n",
      "Epoch 2 Step 9700/11066 - Loss: 1.0529\n",
      "Epoch 2 Step 9800/11066 - Loss: 1.0424\n",
      "Epoch 2 Step 9900/11066 - Loss: 1.0710\n",
      "Epoch 2 Step 10000/11066 - Loss: 1.0591\n",
      "Epoch 2 Step 10100/11066 - Loss: 1.0841\n",
      "Epoch 2 Step 10200/11066 - Loss: 1.0614\n",
      "Epoch 2 Step 10300/11066 - Loss: 1.0776\n",
      "Epoch 2 Step 10400/11066 - Loss: 1.0523\n",
      "Epoch 2 Step 10500/11066 - Loss: 1.0456\n",
      "Epoch 2 Step 10600/11066 - Loss: 1.0390\n",
      "Epoch 2 Step 10700/11066 - Loss: 1.0480\n",
      "Epoch 2 Step 10800/11066 - Loss: 1.0455\n",
      "Epoch 2 Step 10900/11066 - Loss: 1.0496\n",
      "Epoch 2 Step 11000/11066 - Loss: 1.0396\n",
      "\n",
      "Train Loss: 1.0396\n",
      "Val Loss: 0.7762 | EM: 56.65 | F1: 59.45\n",
      "\n",
      "\n",
      "=== Epoch 3/3 ===\n",
      "Epoch 3 Step 100/11066 - Loss: 1.0082\n",
      "Epoch 3 Step 200/11066 - Loss: 1.0054\n",
      "Epoch 3 Step 300/11066 - Loss: 0.9846\n",
      "Epoch 3 Step 400/11066 - Loss: 0.9778\n",
      "Epoch 3 Step 500/11066 - Loss: 0.9780\n",
      "Epoch 3 Step 600/11066 - Loss: 1.0018\n",
      "Epoch 3 Step 700/11066 - Loss: 0.9907\n",
      "Epoch 3 Step 800/11066 - Loss: 0.9620\n",
      "Epoch 3 Step 900/11066 - Loss: 0.9968\n",
      "Epoch 3 Step 1000/11066 - Loss: 0.9864\n",
      "Epoch 3 Step 1100/11066 - Loss: 0.9612\n",
      "Epoch 3 Step 1200/11066 - Loss: 1.0001\n",
      "Epoch 3 Step 1300/11066 - Loss: 1.0126\n",
      "Epoch 3 Step 1400/11066 - Loss: 1.0032\n",
      "Epoch 3 Step 1500/11066 - Loss: 1.0125\n",
      "Epoch 3 Step 1600/11066 - Loss: 0.9976\n",
      "Epoch 3 Step 1700/11066 - Loss: 0.9967\n",
      "Epoch 3 Step 1800/11066 - Loss: 1.0207\n",
      "Epoch 3 Step 1900/11066 - Loss: 0.9985\n",
      "Epoch 3 Step 2000/11066 - Loss: 1.0190\n",
      "Epoch 3 Step 2100/11066 - Loss: 1.0154\n",
      "Epoch 3 Step 2200/11066 - Loss: 0.9767\n",
      "Epoch 3 Step 2300/11066 - Loss: 0.9398\n",
      "Epoch 3 Step 2400/11066 - Loss: 0.9061\n",
      "Epoch 3 Step 2500/11066 - Loss: 0.9000\n",
      "Epoch 3 Step 2600/11066 - Loss: 0.8946\n",
      "Epoch 3 Step 2700/11066 - Loss: 0.9267\n",
      "Epoch 3 Step 2800/11066 - Loss: 0.9258\n",
      "Epoch 3 Step 2900/11066 - Loss: 0.8928\n",
      "Epoch 3 Step 3000/11066 - Loss: 0.9178\n",
      "Epoch 3 Step 3100/11066 - Loss: 0.9227\n",
      "Epoch 3 Step 3200/11066 - Loss: 0.9210\n",
      "Epoch 3 Step 3300/11066 - Loss: 0.8868\n",
      "Epoch 3 Step 3400/11066 - Loss: 0.8885\n",
      "Epoch 3 Step 3500/11066 - Loss: 0.8733\n",
      "Epoch 3 Step 3600/11066 - Loss: 0.8543\n",
      "Epoch 3 Step 3700/11066 - Loss: 0.8837\n",
      "Epoch 3 Step 3800/11066 - Loss: 0.9022\n",
      "Epoch 3 Step 3900/11066 - Loss: 0.8925\n",
      "Epoch 3 Step 4000/11066 - Loss: 0.8932\n",
      "Epoch 3 Step 4100/11066 - Loss: 0.8646\n",
      "Epoch 3 Step 4200/11066 - Loss: 0.9005\n",
      "Epoch 3 Step 4300/11066 - Loss: 0.9123\n",
      "Epoch 3 Step 4400/11066 - Loss: 0.9114\n",
      "Epoch 3 Step 4500/11066 - Loss: 0.9268\n",
      "Epoch 3 Step 4600/11066 - Loss: 0.9073\n",
      "Epoch 3 Step 4700/11066 - Loss: 0.8778\n",
      "Epoch 3 Step 4800/11066 - Loss: 0.8808\n",
      "Epoch 3 Step 4900/11066 - Loss: 0.8887\n",
      "Epoch 3 Step 5000/11066 - Loss: 0.9038\n",
      "Epoch 3 Step 5100/11066 - Loss: 0.9274\n",
      "Epoch 3 Step 5200/11066 - Loss: 0.9315\n",
      "Epoch 3 Step 5300/11066 - Loss: 0.9392\n",
      "Epoch 3 Step 5400/11066 - Loss: 0.9684\n",
      "Epoch 3 Step 5500/11066 - Loss: 1.0054\n",
      "Epoch 3 Step 5600/11066 - Loss: 1.0094\n",
      "Epoch 3 Step 5700/11066 - Loss: 1.0196\n",
      "Epoch 3 Step 5800/11066 - Loss: 0.9860\n",
      "Epoch 3 Step 5900/11066 - Loss: 0.9791\n",
      "Epoch 3 Step 6000/11066 - Loss: 0.9521\n",
      "Epoch 3 Step 6100/11066 - Loss: 0.9796\n",
      "Epoch 3 Step 6200/11066 - Loss: 0.9712\n",
      "Epoch 3 Step 6300/11066 - Loss: 0.9740\n",
      "Epoch 3 Step 6400/11066 - Loss: 0.9855\n",
      "Epoch 3 Step 6500/11066 - Loss: 0.9659\n",
      "Epoch 3 Step 6600/11066 - Loss: 0.9521\n",
      "Epoch 3 Step 6700/11066 - Loss: 0.9830\n",
      "Epoch 3 Step 6800/11066 - Loss: 0.9713\n",
      "Epoch 3 Step 6900/11066 - Loss: 0.9720\n",
      "Epoch 3 Step 7000/11066 - Loss: 0.9961\n",
      "Epoch 3 Step 7100/11066 - Loss: 0.9929\n",
      "Epoch 3 Step 7200/11066 - Loss: 0.9914\n",
      "Epoch 3 Step 7300/11066 - Loss: 0.9933\n",
      "Epoch 3 Step 7400/11066 - Loss: 0.9671\n",
      "Epoch 3 Step 7500/11066 - Loss: 0.9763\n",
      "Epoch 3 Step 7600/11066 - Loss: 1.0032\n",
      "Epoch 3 Step 7700/11066 - Loss: 1.0058\n",
      "Epoch 3 Step 7800/11066 - Loss: 1.0264\n",
      "Epoch 3 Step 7900/11066 - Loss: 0.9912\n",
      "Epoch 3 Step 8000/11066 - Loss: 0.9704\n",
      "Epoch 3 Step 8100/11066 - Loss: 0.9575\n",
      "Epoch 3 Step 8200/11066 - Loss: 0.9556\n",
      "Epoch 3 Step 8300/11066 - Loss: 0.9530\n",
      "Epoch 3 Step 8400/11066 - Loss: 0.9802\n",
      "Epoch 3 Step 8500/11066 - Loss: 1.0035\n",
      "Epoch 3 Step 8600/11066 - Loss: 1.0344\n",
      "Epoch 3 Step 8700/11066 - Loss: 1.0288\n",
      "Epoch 3 Step 8800/11066 - Loss: 1.0686\n",
      "Epoch 3 Step 8900/11066 - Loss: 1.0374\n",
      "Epoch 3 Step 9000/11066 - Loss: 1.0398\n",
      "Epoch 3 Step 9100/11066 - Loss: 1.0394\n",
      "Epoch 3 Step 9200/11066 - Loss: 1.0395\n",
      "Epoch 3 Step 9300/11066 - Loss: 1.0737\n",
      "Epoch 3 Step 9400/11066 - Loss: 1.0605\n",
      "Epoch 3 Step 9500/11066 - Loss: 1.0465\n",
      "Epoch 3 Step 9600/11066 - Loss: 1.0076\n",
      "Epoch 3 Step 9700/11066 - Loss: 0.9903\n",
      "Epoch 3 Step 9800/11066 - Loss: 1.0190\n",
      "Epoch 3 Step 9900/11066 - Loss: 1.0243\n",
      "Epoch 3 Step 10000/11066 - Loss: 0.9873\n",
      "Epoch 3 Step 10100/11066 - Loss: 0.9485\n",
      "Epoch 3 Step 10200/11066 - Loss: 0.9266\n",
      "Epoch 3 Step 10300/11066 - Loss: 0.9562\n",
      "Epoch 3 Step 10400/11066 - Loss: 0.9246\n",
      "Epoch 3 Step 10500/11066 - Loss: 0.8880\n",
      "Epoch 3 Step 10600/11066 - Loss: 0.9171\n",
      "Epoch 3 Step 10700/11066 - Loss: 0.9103\n",
      "Epoch 3 Step 10800/11066 - Loss: 0.9317\n",
      "Epoch 3 Step 10900/11066 - Loss: 0.9401\n",
      "Epoch 3 Step 11000/11066 - Loss: 0.9432\n",
      "\n",
      "Train Loss: 0.9432\n",
      "Val Loss: 0.7940 | EM: 62.22 | F1: 63.14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
    "    train_loss = train_epoch(epoch)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    val_loss, val_metrics = eval_epoch(val_loader, raw_datasets['validation'])\n",
    "    print(f\"Val Loss: {val_loss:.4f} | EM: {val_metrics['exact_match']:.2f} | F1: {val_metrics['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26aaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Performance ===\n",
      "Test Loss: 0.8653 | EM: 62.11 | F1: 62.58\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Test Performance ===\")\n",
    "test_loss, test_metrics = eval_epoch(test_loader, raw_datasets['test'])\n",
    "print(f\"Test Loss: {test_loss:.4f} | EM: {test_metrics['exact_match']:.2f} | F1: {test_metrics['f1']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
