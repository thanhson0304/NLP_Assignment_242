{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70286db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5433927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7ffd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('rajpurkar/squad')\n",
    "split = dataset['validation'].train_test_split(test_size=0.5, seed=42)\n",
    "raw_datasets = {\n",
    "    'train': dataset['train'],\n",
    "    'validation': split['train'],\n",
    "    'test': split['test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46af2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "max_length = 384\n",
    "doc_stride = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad38ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['question'], examples['context'],\n",
    "        truncation='only_second',\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    overflow_to_sample_mapping = tokenized.pop('overflow_to_sample_mapping')\n",
    "    offset_mapping = tokenized.pop('offset_mapping')\n",
    "\n",
    "    tokenized['start_positions'] = []\n",
    "    tokenized['end_positions'] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_idx = overflow_to_sample_mapping[i]\n",
    "        answers = examples['answers'][sample_idx]\n",
    "        cls_index = tokenized['input_ids'][i].index(tokenizer.cls_token_id)\n",
    "\n",
    "        if len(answers['answer_start']) == 0:\n",
    "            tokenized['start_positions'].append(cls_index)\n",
    "            tokenized['end_positions'].append(cls_index)\n",
    "        else:\n",
    "            start_char = answers['answer_start'][0]\n",
    "            end_char = start_char + len(answers['text'][0])\n",
    "            sequence_ids = tokenized.sequence_ids(i)\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(tokenized['input_ids'][i]) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if offsets[token_start_index][0] > end_char or offsets[token_end_index][1] < start_char:\n",
    "                tokenized['start_positions'].append(cls_index)\n",
    "                tokenized['end_positions'].append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized['start_positions'].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized['end_positions'].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ef7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = {k: raw_datasets[k].map(\n",
    "    prepare_features,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[k].column_names\n",
    ") for k in ['train', 'validation', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6703eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(row['input_ids']),\n",
    "            'attention_mask': torch.tensor(row['attention_mask']),\n",
    "            'start_positions': torch.tensor(row['start_positions']),\n",
    "            'end_positions': torch.tensor(row['end_positions'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba5d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QADataset(tokenized_datasets['train'])\n",
    "val_dataset   = QADataset(tokenized_datasets['validation'])\n",
    "test_dataset  = QADataset(tokenized_datasets['test'])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d139c44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaac0c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10bao\\AppData\\Local\\Temp\\ipykernel_19768\\4014476045.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metric = load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f64410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_pos = batch['start_positions'].to(device)\n",
    "        end_pos = batch['end_positions'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "            loss = (criterion(start_logits, start_pos) + criterion(end_logits, end_pos)) / 2\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} Step {step}/{len(train_loader)} - Loss: {running_loss/step:.4f}\")\n",
    "\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226c8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(data_loader, raw_data):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_pos = batch['start_positions'].to(device)\n",
    "        end_pos = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "        eval_loss += ((criterion(start_logits, start_pos) + criterion(end_logits, end_pos)) / 2).item()\n",
    "\n",
    "        for b in range(len(input_ids)):\n",
    "            start_idx = torch.argmax(start_logits[b]).item()\n",
    "            end_idx = torch.argmax(end_logits[b]).item()\n",
    "            if start_idx > end_idx:\n",
    "                answer = \"\"\n",
    "            else:\n",
    "                tokens = input_ids[b][start_idx:end_idx+1]\n",
    "                answer = tokenizer.decode(tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "            sample_index = i * data_loader.batch_size + b\n",
    "            if sample_index < len(raw_data):\n",
    "                predictions.append({\"id\": raw_data[sample_index][\"id\"], \"prediction_text\": answer})\n",
    "                references.append({\"id\": raw_data[sample_index][\"id\"], \"answers\": raw_data[sample_index][\"answers\"]})\n",
    "\n",
    "    metrics = metric.compute(predictions=predictions, references=references)\n",
    "    return eval_loss / len(data_loader), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20499b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/3 ===\n",
      "Epoch 1 Step 100/11066 - Loss: 5.3633\n",
      "Epoch 1 Step 200/11066 - Loss: 5.3016\n",
      "Epoch 1 Step 300/11066 - Loss: 5.1925\n",
      "Epoch 1 Step 400/11066 - Loss: 5.1055\n",
      "Epoch 1 Step 500/11066 - Loss: 4.9786\n",
      "Epoch 1 Step 600/11066 - Loss: 4.9305\n",
      "Epoch 1 Step 700/11066 - Loss: 4.8599\n",
      "Epoch 1 Step 800/11066 - Loss: 4.7666\n",
      "Epoch 1 Step 900/11066 - Loss: 4.6891\n",
      "Epoch 1 Step 1000/11066 - Loss: 4.6225\n",
      "Epoch 1 Step 1100/11066 - Loss: 4.5853\n",
      "Epoch 1 Step 1200/11066 - Loss: 4.4681\n",
      "Epoch 1 Step 1300/11066 - Loss: 4.3660\n",
      "Epoch 1 Step 1400/11066 - Loss: 4.3340\n",
      "Epoch 1 Step 1500/11066 - Loss: 4.2877\n",
      "Epoch 1 Step 1600/11066 - Loss: 4.2118\n",
      "Epoch 1 Step 1700/11066 - Loss: 4.1226\n",
      "Epoch 1 Step 1800/11066 - Loss: 4.0426\n",
      "Epoch 1 Step 1900/11066 - Loss: 3.9746\n",
      "Epoch 1 Step 2000/11066 - Loss: 3.8616\n",
      "Epoch 1 Step 2100/11066 - Loss: 3.8048\n",
      "Epoch 1 Step 2200/11066 - Loss: 3.6947\n",
      "Epoch 1 Step 2300/11066 - Loss: 3.6600\n",
      "Epoch 1 Step 2400/11066 - Loss: 3.5748\n",
      "Epoch 1 Step 2500/11066 - Loss: 3.5155\n",
      "Epoch 1 Step 2600/11066 - Loss: 3.4739\n",
      "Epoch 1 Step 2700/11066 - Loss: 3.4114\n",
      "Epoch 1 Step 2800/11066 - Loss: 3.4049\n",
      "Epoch 1 Step 2900/11066 - Loss: 3.3864\n",
      "Epoch 1 Step 3000/11066 - Loss: 3.3430\n",
      "Epoch 1 Step 3100/11066 - Loss: 3.3203\n",
      "Epoch 1 Step 3200/11066 - Loss: 3.2715\n",
      "Epoch 1 Step 3300/11066 - Loss: 3.2366\n",
      "Epoch 1 Step 3400/11066 - Loss: 3.2185\n",
      "Epoch 1 Step 3500/11066 - Loss: 3.1971\n",
      "Epoch 1 Step 3600/11066 - Loss: 3.1446\n",
      "Epoch 1 Step 3700/11066 - Loss: 3.0622\n",
      "Epoch 1 Step 3800/11066 - Loss: 3.0106\n",
      "Epoch 1 Step 3900/11066 - Loss: 3.0055\n",
      "Epoch 1 Step 4000/11066 - Loss: 2.9506\n",
      "Epoch 1 Step 4100/11066 - Loss: 2.8904\n",
      "Epoch 1 Step 4200/11066 - Loss: 2.8535\n",
      "Epoch 1 Step 4300/11066 - Loss: 2.8207\n",
      "Epoch 1 Step 4400/11066 - Loss: 2.8046\n",
      "Epoch 1 Step 4500/11066 - Loss: 2.8088\n",
      "Epoch 1 Step 4600/11066 - Loss: 2.7520\n",
      "Epoch 1 Step 4700/11066 - Loss: 2.6613\n",
      "Epoch 1 Step 4800/11066 - Loss: 2.6260\n",
      "Epoch 1 Step 4900/11066 - Loss: 2.6076\n",
      "Epoch 1 Step 5000/11066 - Loss: 2.5646\n",
      "Epoch 1 Step 5100/11066 - Loss: 2.5215\n",
      "Epoch 1 Step 5200/11066 - Loss: 2.5058\n",
      "Epoch 1 Step 5300/11066 - Loss: 2.4643\n",
      "Epoch 1 Step 5400/11066 - Loss: 2.3818\n",
      "Epoch 1 Step 5500/11066 - Loss: 2.3106\n",
      "Epoch 1 Step 5600/11066 - Loss: 2.3035\n",
      "Epoch 1 Step 5700/11066 - Loss: 2.2823\n",
      "Epoch 1 Step 5800/11066 - Loss: 2.2520\n",
      "Epoch 1 Step 5900/11066 - Loss: 2.2622\n",
      "Epoch 1 Step 6000/11066 - Loss: 2.2001\n",
      "Epoch 1 Step 6100/11066 - Loss: 2.1222\n",
      "Epoch 1 Step 6200/11066 - Loss: 2.1199\n",
      "Epoch 1 Step 6300/11066 - Loss: 2.0915\n",
      "Epoch 1 Step 6400/11066 - Loss: 2.0807\n",
      "Epoch 1 Step 6500/11066 - Loss: 2.0502\n",
      "Epoch 1 Step 6600/11066 - Loss: 1.9801\n",
      "Epoch 1 Step 6700/11066 - Loss: 1.9119\n",
      "Epoch 1 Step 6800/11066 - Loss: 1.9041\n",
      "Epoch 1 Step 6900/11066 - Loss: 1.8727\n",
      "Epoch 1 Step 7000/11066 - Loss: 1.8803\n",
      "Epoch 1 Step 7100/11066 - Loss: 1.9002\n",
      "Epoch 1 Step 7200/11066 - Loss: 1.9074\n",
      "Epoch 1 Step 7300/11066 - Loss: 1.9234\n",
      "Epoch 1 Step 7400/11066 - Loss: 1.8570\n",
      "Epoch 1 Step 7500/11066 - Loss: 1.8235\n",
      "Epoch 1 Step 7600/11066 - Loss: 1.7813\n",
      "Epoch 1 Step 7700/11066 - Loss: 1.7585\n",
      "Epoch 1 Step 7800/11066 - Loss: 1.7241\n",
      "Epoch 1 Step 7900/11066 - Loss: 1.6504\n",
      "Epoch 1 Step 8000/11066 - Loss: 1.6421\n",
      "Epoch 1 Step 8100/11066 - Loss: 1.6025\n",
      "Epoch 1 Step 8200/11066 - Loss: 1.5844\n",
      "Epoch 1 Step 8300/11066 - Loss: 1.5641\n",
      "Epoch 1 Step 8400/11066 - Loss: 1.5669\n",
      "Epoch 1 Step 8500/11066 - Loss: 1.5567\n",
      "Epoch 1 Step 8600/11066 - Loss: 1.4869\n",
      "Epoch 1 Step 8700/11066 - Loss: 1.5003\n",
      "Epoch 1 Step 8800/11066 - Loss: 1.4294\n",
      "Epoch 1 Step 8900/11066 - Loss: 1.4211\n",
      "Epoch 1 Step 9000/11066 - Loss: 1.3845\n",
      "Epoch 1 Step 9100/11066 - Loss: 1.3494\n",
      "Epoch 1 Step 9200/11066 - Loss: 1.3695\n",
      "Epoch 1 Step 9300/11066 - Loss: 1.3814\n",
      "Epoch 1 Step 9400/11066 - Loss: 1.3752\n",
      "Epoch 1 Step 9500/11066 - Loss: 1.3750\n",
      "Epoch 1 Step 9600/11066 - Loss: 1.3432\n",
      "Epoch 1 Step 9700/11066 - Loss: 1.3635\n",
      "Epoch 1 Step 9800/11066 - Loss: 1.3070\n",
      "Epoch 1 Step 9900/11066 - Loss: 1.2705\n",
      "Epoch 1 Step 10000/11066 - Loss: 1.2581\n",
      "Epoch 1 Step 10100/11066 - Loss: 1.1976\n",
      "Epoch 1 Step 10200/11066 - Loss: 1.1374\n",
      "Epoch 1 Step 10300/11066 - Loss: 1.0720\n",
      "Epoch 1 Step 10400/11066 - Loss: 1.0639\n",
      "Epoch 1 Step 10500/11066 - Loss: 0.9979\n",
      "Epoch 1 Step 10600/11066 - Loss: 0.9848\n",
      "Epoch 1 Step 10700/11066 - Loss: 0.9866\n",
      "Epoch 1 Step 10800/11066 - Loss: 0.9649\n",
      "Epoch 1 Step 10900/11066 - Loss: 0.9536\n",
      "Epoch 1 Step 11000/11066 - Loss: 0.9542\n",
      "\n",
      "Train Loss: 0.9542\n",
      "Val Loss: 0.7531 | EM: 53.99 | F1: 56.22\n",
      "\n",
      "\n",
      "=== Epoch 2/3 ===\n",
      "Epoch 2 Step 100/11066 - Loss: 0.9533\n",
      "Epoch 2 Step 200/11066 - Loss: 0.9752\n",
      "Epoch 2 Step 300/11066 - Loss: 0.9890\n",
      "Epoch 2 Step 400/11066 - Loss: 0.9351\n",
      "Epoch 2 Step 500/11066 - Loss: 0.8793\n",
      "Epoch 2 Step 600/11066 - Loss: 0.8204\n",
      "Epoch 2 Step 700/11066 - Loss: 0.8158\n",
      "Epoch 2 Step 800/11066 - Loss: 0.7742\n",
      "Epoch 2 Step 900/11066 - Loss: 0.7756\n",
      "Epoch 2 Step 1000/11066 - Loss: 0.7675\n",
      "Epoch 2 Step 1100/11066 - Loss: 0.7495\n",
      "Epoch 2 Step 1200/11066 - Loss: 0.7655\n",
      "Epoch 2 Step 1300/11066 - Loss: 0.7686\n",
      "Epoch 2 Step 1400/11066 - Loss: 0.7961\n",
      "Epoch 2 Step 1500/11066 - Loss: 0.7764\n",
      "Epoch 2 Step 1600/11066 - Loss: 0.7254\n",
      "Epoch 2 Step 1700/11066 - Loss: 0.7075\n",
      "Epoch 2 Step 1800/11066 - Loss: 0.6775\n",
      "Epoch 2 Step 1900/11066 - Loss: 0.6919\n",
      "Epoch 2 Step 2000/11066 - Loss: 0.6540\n",
      "Epoch 2 Step 2100/11066 - Loss: 0.6610\n",
      "Epoch 2 Step 2200/11066 - Loss: 0.6475\n",
      "Epoch 2 Step 2300/11066 - Loss: 0.6637\n",
      "Epoch 2 Step 2400/11066 - Loss: 0.6532\n",
      "Epoch 2 Step 2500/11066 - Loss: 0.6599\n",
      "Epoch 2 Step 2600/11066 - Loss: 0.6099\n",
      "Epoch 2 Step 2700/11066 - Loss: 0.6364\n",
      "Epoch 2 Step 2800/11066 - Loss: 0.6263\n",
      "Epoch 2 Step 2900/11066 - Loss: 0.6508\n",
      "Epoch 2 Step 3000/11066 - Loss: 0.6204\n",
      "Epoch 2 Step 3100/11066 - Loss: 0.6608\n",
      "Epoch 2 Step 3200/11066 - Loss: 0.6046\n",
      "Epoch 2 Step 3300/11066 - Loss: 0.5530\n",
      "Epoch 2 Step 3400/11066 - Loss: 0.5277\n",
      "Epoch 2 Step 3500/11066 - Loss: 0.5241\n",
      "Epoch 2 Step 3600/11066 - Loss: 0.5213\n",
      "Epoch 2 Step 3700/11066 - Loss: 0.5075\n",
      "Epoch 2 Step 3800/11066 - Loss: 0.5435\n",
      "Epoch 2 Step 3900/11066 - Loss: 0.5812\n",
      "Epoch 2 Step 4000/11066 - Loss: 0.5338\n",
      "Epoch 2 Step 4100/11066 - Loss: 0.5393\n",
      "Epoch 2 Step 4200/11066 - Loss: 0.4960\n",
      "Epoch 2 Step 4300/11066 - Loss: 0.5240\n",
      "Epoch 2 Step 4400/11066 - Loss: 0.5010\n",
      "Epoch 2 Step 4500/11066 - Loss: 0.4625\n",
      "Epoch 2 Step 4600/11066 - Loss: 0.4487\n",
      "Epoch 2 Step 4700/11066 - Loss: 0.4409\n",
      "Epoch 2 Step 4800/11066 - Loss: 0.4011\n",
      "Epoch 2 Step 4900/11066 - Loss: 0.4091\n",
      "Epoch 2 Step 5000/11066 - Loss: 0.3939\n",
      "Epoch 2 Step 5100/11066 - Loss: 0.4132\n",
      "Epoch 2 Step 5200/11066 - Loss: 0.4475\n",
      "Epoch 2 Step 5300/11066 - Loss: 0.4194\n",
      "Epoch 2 Step 5400/11066 - Loss: 0.3692\n",
      "Epoch 2 Step 5500/11066 - Loss: 0.3457\n",
      "Epoch 2 Step 5600/11066 - Loss: 0.3184\n",
      "Epoch 2 Step 5700/11066 - Loss: 0.3119\n",
      "Epoch 2 Step 5800/11066 - Loss: 0.3446\n",
      "Epoch 2 Step 5900/11066 - Loss: 0.3109\n",
      "Epoch 2 Step 6000/11066 - Loss: 0.3166\n",
      "Epoch 2 Step 6100/11066 - Loss: 0.3574\n",
      "Epoch 2 Step 6200/11066 - Loss: 0.3665\n",
      "Epoch 2 Step 6300/11066 - Loss: 0.3618\n",
      "Epoch 2 Step 6400/11066 - Loss: 0.3406\n",
      "Epoch 2 Step 6500/11066 - Loss: 0.3622\n",
      "Epoch 2 Step 6600/11066 - Loss: 0.3680\n",
      "Epoch 2 Step 6700/11066 - Loss: 0.4071\n",
      "Epoch 2 Step 6800/11066 - Loss: 0.4513\n",
      "Epoch 2 Step 6900/11066 - Loss: 0.4349\n",
      "Epoch 2 Step 7000/11066 - Loss: 0.4775\n",
      "Epoch 2 Step 7100/11066 - Loss: 0.4305\n",
      "Epoch 2 Step 7200/11066 - Loss: 0.4274\n",
      "Epoch 2 Step 7300/11066 - Loss: 0.4070\n",
      "Epoch 2 Step 7400/11066 - Loss: 0.3730\n",
      "Epoch 2 Step 7500/11066 - Loss: 0.3221\n",
      "Epoch 2 Step 7600/11066 - Loss: 0.3614\n",
      "Epoch 2 Step 7700/11066 - Loss: 0.3783\n",
      "Epoch 2 Step 7800/11066 - Loss: 0.3684\n",
      "Epoch 2 Step 7900/11066 - Loss: 0.3260\n",
      "Epoch 2 Step 8000/11066 - Loss: 0.3466\n",
      "Epoch 2 Step 8100/11066 - Loss: 0.3263\n",
      "Epoch 2 Step 8200/11066 - Loss: 0.3022\n",
      "Epoch 2 Step 8300/11066 - Loss: 0.2978\n",
      "Epoch 2 Step 8400/11066 - Loss: 0.2714\n",
      "Epoch 2 Step 8500/11066 - Loss: 0.2339\n",
      "Epoch 2 Step 8600/11066 - Loss: 0.2038\n",
      "Epoch 2 Step 8700/11066 - Loss: 0.2078\n",
      "Epoch 2 Step 8800/11066 - Loss: 0.2340\n",
      "Epoch 2 Step 8900/11066 - Loss: 0.1894\n",
      "Epoch 2 Step 9000/11066 - Loss: 0.1648\n",
      "Epoch 2 Step 9100/11066 - Loss: 0.1680\n",
      "Epoch 2 Step 9200/11066 - Loss: 0.2017\n",
      "Epoch 2 Step 9300/11066 - Loss: 0.1884\n",
      "Epoch 2 Step 9400/11066 - Loss: 0.1960\n",
      "Epoch 2 Step 9500/11066 - Loss: 0.1924\n",
      "Epoch 2 Step 9600/11066 - Loss: 0.2130\n",
      "Epoch 2 Step 9700/11066 - Loss: 0.2527\n",
      "Epoch 2 Step 9800/11066 - Loss: 0.2839\n",
      "Epoch 2 Step 9900/11066 - Loss: 0.3248\n",
      "Epoch 2 Step 10000/11066 - Loss: 0.3487\n",
      "Epoch 2 Step 10100/11066 - Loss: 0.3487\n",
      "Epoch 2 Step 10200/11066 - Loss: 0.3231\n",
      "Epoch 2 Step 10300/11066 - Loss: 0.3160\n",
      "Epoch 2 Step 10400/11066 - Loss: 0.3204\n",
      "Epoch 2 Step 10500/11066 - Loss: 0.3346\n",
      "Epoch 2 Step 10600/11066 - Loss: 0.2862\n",
      "Epoch 2 Step 10700/11066 - Loss: 0.2552\n",
      "Epoch 2 Step 10800/11066 - Loss: 0.2546\n",
      "Epoch 2 Step 10900/11066 - Loss: 0.2696\n",
      "Epoch 2 Step 11000/11066 - Loss: 0.2574\n",
      "\n",
      "Train Loss: 0.2574\n",
      "Val Loss: 0.2069 | EM: 57.27 | F1: 59.03\n",
      "\n",
      "\n",
      "=== Epoch 3/3 ===\n",
      "Epoch 3 Step 100/11066 - Loss: 0.3012\n",
      "Epoch 3 Step 200/11066 - Loss: 0.2788\n",
      "Epoch 3 Step 300/11066 - Loss: 0.2309\n",
      "Epoch 3 Step 400/11066 - Loss: 0.2138\n",
      "Epoch 3 Step 500/11066 - Loss: 0.1916\n",
      "Epoch 3 Step 600/11066 - Loss: 0.1631\n",
      "Epoch 3 Step 700/11066 - Loss: 0.1545\n",
      "Epoch 3 Step 800/11066 - Loss: 0.1743\n",
      "Epoch 3 Step 900/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 1000/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 1100/11066 - Loss: 0.1792\n",
      "Epoch 3 Step 1200/11066 - Loss: 0.1939\n",
      "Epoch 3 Step 1300/11066 - Loss: 0.1646\n",
      "Epoch 3 Step 1400/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 1500/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 1600/11066 - Loss: 0.1809\n",
      "Epoch 3 Step 1700/11066 - Loss: 0.1877\n",
      "Epoch 3 Step 1800/11066 - Loss: 0.1535\n",
      "Epoch 3 Step 1900/11066 - Loss: 0.2000\n",
      "Epoch 3 Step 2000/11066 - Loss: 0.2315\n",
      "Epoch 3 Step 2100/11066 - Loss: 0.1914\n",
      "Epoch 3 Step 2200/11066 - Loss: 0.2260\n",
      "Epoch 3 Step 2300/11066 - Loss: 0.1885\n",
      "Epoch 3 Step 2400/11066 - Loss: 0.2317\n",
      "Epoch 3 Step 2500/11066 - Loss: 0.2421\n",
      "Epoch 3 Step 2600/11066 - Loss: 0.2398\n",
      "Epoch 3 Step 2700/11066 - Loss: 0.2824\n",
      "Epoch 3 Step 2800/11066 - Loss: 0.2455\n",
      "Epoch 3 Step 2900/11066 - Loss: 0.2892\n",
      "Epoch 3 Step 3000/11066 - Loss: 0.2614\n",
      "Epoch 3 Step 3100/11066 - Loss: 0.2586\n",
      "Epoch 3 Step 3200/11066 - Loss: 0.2297\n",
      "Epoch 3 Step 3300/11066 - Loss: 0.2114\n",
      "Epoch 3 Step 3400/11066 - Loss: 0.2112\n",
      "Epoch 3 Step 3500/11066 - Loss: 0.1745\n",
      "Epoch 3 Step 3600/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 3700/11066 - Loss: 0.1899\n",
      "Epoch 3 Step 3800/11066 - Loss: 0.1656\n",
      "Epoch 3 Step 3900/11066 - Loss: 0.1956\n",
      "Epoch 3 Step 4000/11066 - Loss: 0.2229\n",
      "Epoch 3 Step 4100/11066 - Loss: 0.2007\n",
      "Epoch 3 Step 4200/11066 - Loss: 0.1752\n",
      "Epoch 3 Step 4300/11066 - Loss: 0.1595\n",
      "Epoch 3 Step 4400/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 4500/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 4600/11066 - Loss: 0.1573\n",
      "Epoch 3 Step 4700/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 4800/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 4900/11066 - Loss: 0.1911\n",
      "Epoch 3 Step 5000/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 5100/11066 - Loss: 0.1611\n",
      "Epoch 3 Step 5200/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 5300/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 5400/11066 - Loss: 0.1605\n",
      "Epoch 3 Step 5500/11066 - Loss: 0.2061\n",
      "Epoch 3 Step 5600/11066 - Loss: 0.2001\n",
      "Epoch 3 Step 5700/11066 - Loss: 0.1524\n",
      "Epoch 3 Step 5800/11066 - Loss: 0.1561\n",
      "Epoch 3 Step 5900/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 6000/11066 - Loss: 0.1864\n",
      "Epoch 3 Step 6100/11066 - Loss: 0.1659\n",
      "Epoch 3 Step 6200/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 6300/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 6400/11066 - Loss: 0.1974\n",
      "Epoch 3 Step 6500/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 6600/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 6700/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 6800/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 6900/11066 - Loss: 0.1783\n",
      "Epoch 3 Step 7000/11066 - Loss: 0.1830\n",
      "Epoch 3 Step 7100/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 7200/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 7300/11066 - Loss: 0.1824\n",
      "Epoch 3 Step 7400/11066 - Loss: 0.1796\n",
      "Epoch 3 Step 7500/11066 - Loss: 0.2036\n",
      "Epoch 3 Step 7600/11066 - Loss: 0.1730\n",
      "Epoch 3 Step 7700/11066 - Loss: 0.2118\n",
      "Epoch 3 Step 7800/11066 - Loss: 0.2160\n",
      "Epoch 3 Step 7900/11066 - Loss: 0.1729\n",
      "Epoch 3 Step 8000/11066 - Loss: 0.1539\n",
      "Epoch 3 Step 8100/11066 - Loss: 0.1523\n",
      "Epoch 3 Step 8200/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 8300/11066 - Loss: 0.1500\n",
      "Epoch 3 Step 8400/11066 - Loss: 0.1903\n",
      "Epoch 3 Step 8500/11066 - Loss: 0.1949\n",
      "Epoch 3 Step 8600/11066 - Loss: 0.1940\n",
      "Epoch 3 Step 8700/11066 - Loss: 0.1983\n",
      "Epoch 3 Step 8800/11066 - Loss: 0.2222\n",
      "Epoch 3 Step 8900/11066 - Loss: 0.2593\n",
      "Epoch 3 Step 9000/11066 - Loss: 0.2583\n",
      "Epoch 3 Step 9100/11066 - Loss: 0.2863\n",
      "Epoch 3 Step 9200/11066 - Loss: 0.2960\n",
      "Epoch 3 Step 9300/11066 - Loss: 0.2804\n",
      "Epoch 3 Step 9400/11066 - Loss: 0.2699\n",
      "Epoch 3 Step 9500/11066 - Loss: 0.2630\n",
      "Epoch 3 Step 9600/11066 - Loss: 0.2626\n",
      "Epoch 3 Step 9700/11066 - Loss: 0.2488\n",
      "Epoch 3 Step 9800/11066 - Loss: 0.1976\n",
      "Epoch 3 Step 9900/11066 - Loss: 0.2159\n",
      "Epoch 3 Step 10000/11066 - Loss: 0.1698\n",
      "Epoch 3 Step 10100/11066 - Loss: 0.1508\n",
      "Epoch 3 Step 10200/11066 - Loss: 0.1940\n",
      "Epoch 3 Step 10300/11066 - Loss: 0.2018\n",
      "Epoch 3 Step 10400/11066 - Loss: 0.1902\n",
      "Epoch 3 Step 10500/11066 - Loss: 0.1708\n",
      "Epoch 3 Step 10600/11066 - Loss: 0.1756\n",
      "Epoch 3 Step 10700/11066 - Loss: 0.1671\n",
      "Epoch 3 Step 10800/11066 - Loss: 0.1866\n",
      "Epoch 3 Step 10900/11066 - Loss: 0.2194\n",
      "Epoch 3 Step 11000/11066 - Loss: 0.2478\n",
      "\n",
      "Train Loss: 0.2478\n",
      "Val Loss: 0.1832 | EM: 60.70 | F1: 62.13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
    "    train_loss = train_epoch(epoch)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    val_loss, val_metrics = eval_epoch(val_loader, raw_datasets['validation'])\n",
    "    print(f\"Val Loss: {val_loss:.4f} | EM: {val_metrics['exact_match']:.2f} | F1: {val_metrics['f1']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a62f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Performance ===\n",
      "Test Loss: 0.8653 | EM: 62.11 | F1: 62.58\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Test Performance ===\")\n",
    "test_loss, test_metrics = eval_epoch(test_loader, raw_datasets['test'])\n",
    "print(f\"Test Loss: {test_loss:.4f} | EM: {test_metrics['exact_match']:.2f} | F1: {test_metrics['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7fc6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bdf8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
