{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685339ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q datasets peft accelerate evaluate scikit-learn bitsandbytes rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdc336",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7367e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, random, numpy as np, evaluate\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import tqdm\n",
    "from transformers import GPT2Model\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast, GPT2Model, GPT2LMHeadModel,\n",
    "    TrainingArguments, Trainer,BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "\n",
    "tok = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qlora_model(base_name, head_cls, lora_r=8, task_type=\"SEQ_CLS\"):\n",
    "    qconf = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    if head_cls is GPT2LMHeadModel:\n",
    "        base = GPT2LMHeadModel.from_pretrained(\n",
    "            base_name, quantization_config=qconf, device_map=\"auto\"\n",
    "        )\n",
    "    else:\n",
    "        base = GPT2Model.from_pretrained(\n",
    "            base_name, quantization_config=qconf, device_map=\"auto\"\n",
    "        )\n",
    "    model = head_cls(base) if head_cls is not GPT2LMHeadModel else base\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        task_type=getattr(TaskType, task_type),\n",
    "        target_modules=[\"c_attn\"],\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg).to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19df4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SpanQA(nn.Module):\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.config = base.config\n",
    "        H = base.config.hidden_size\n",
    "        self.s = nn.Linear(H, 1)\n",
    "        self.e = nn.Linear(H, 1)\n",
    "    def forward(self, input_ids=None, attention_mask=None,\n",
    "                start_positions=None, end_positions=None, labels=None,**kwargs):\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "        hs = self.base(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        ).last_hidden_state\n",
    "        start = self.s(hs).squeeze(-1)\n",
    "        end   = self.e(hs).squeeze(-1)\n",
    "        loss  = None\n",
    "        if start_positions is not None:\n",
    "            ce  = nn.CrossEntropyLoss()\n",
    "            loss = (ce(start,start_positions)+ce(end,end_positions))/2\n",
    "        return {\"loss\": loss, \"start_logits\": start, \"end_logits\": end}\n",
    "\n",
    "qa_model = make_qlora_model(\"gpt2\", GPT2SpanQA, lora_r=8, task_type=\"TOKEN_CLS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d930a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr(t): print(\"\\n\" + \"═\"*15 + \" \" + t + \" \" + \"═\"*15)\n",
    "hr(\"Training QA\")\n",
    "raw_qa = load_dataset(\"squad\", split=\"train[:80000]\")\n",
    "\n",
    "def tok_qa(ex):\n",
    "    q, c = ex[\"question\"], ex[\"context\"]\n",
    "    a = ex[\"answers\"][\"text\"][0]\n",
    "\n",
    "    enc = tok(\n",
    "        q + tok.eos_token + c,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    offsets = enc.pop(\"offset_mapping\")\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "    idx = c.find(a)\n",
    "    if idx == -1:\n",
    "        return {\"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"start_positions\": None,\n",
    "                \"end_positions\": None}\n",
    "\n",
    "    qlen = len(q) + len(tok.eos_token)\n",
    "    start_char = qlen + idx\n",
    "    end_char = start_char + len(a)\n",
    "\n",
    "    start_pos = next(\n",
    "        (i for i, (s, e) in enumerate(offsets) if s <= start_char < e),\n",
    "        None\n",
    "    )\n",
    "    end_pos = next(\n",
    "        (i for i, (s, e) in enumerate(offsets) if s < end_char <= e),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    if start_pos is None or end_pos is None:\n",
    "        return {\"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"start_positions\": None,\n",
    "                \"end_positions\": None}\n",
    "\n",
    "    return {\"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"start_positions\": start_pos,\n",
    "            \"end_positions\": end_pos}\n",
    "\n",
    "# Filter out examples without valid start positions\n",
    "keep = [\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"]\n",
    "train_qa = raw_qa.map(\n",
    "    tok_qa,\n",
    "    batched=False,\n",
    "    remove_columns=[c for c in raw_qa.column_names if c not in keep]\n",
    ")\n",
    "\n",
    "train_qa = train_qa.filter(lambda ex: ex[\"start_positions\"] is not None)\n",
    "\n",
    "# Validation set preparation\n",
    "val_raw = load_dataset(\"squad\", split=\"validation[:5000]\")\n",
    "val_qa = val_raw.map(\n",
    "    tok_qa,\n",
    "    batched=False,\n",
    "    remove_columns=val_raw.column_names\n",
    ").filter(lambda ex: ex[\"start_positions\"] is not None)\n",
    "\n",
    "# Collate function to handle padding\n",
    "def coll_qa(batch):\n",
    "    input_ids = [torch.tensor(ex[\"input_ids\"]) for ex in batch]\n",
    "    masks = [torch.tensor(ex[\"attention_mask\"]) for ex in batch]\n",
    "    pad_id = tok.pad_token_id\n",
    "\n",
    "    input_ids_padded = pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=pad_id\n",
    "    )\n",
    "    masks_padded = pad_sequence(\n",
    "        masks, batch_first=True, padding_value=0\n",
    "    )\n",
    "    starts = torch.tensor([ex[\"start_positions\"] for ex in batch])\n",
    "    ends = torch.tensor([ex[\"end_positions\"] for ex in batch])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": masks_padded,\n",
    "        \"start_positions\": starts,\n",
    "        \"end_positions\": ends,\n",
    "    }\n",
    "\n",
    "# Compute the metrics (EM and F1)\n",
    "metric_squad = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics_qa(p):\n",
    "    # Get predictions and labels\n",
    "    preds = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    start_pred = preds[0]\n",
    "    end_pred = preds[1]\n",
    "\n",
    "    # Convert the start and end predictions to text\n",
    "    decoded_preds = tok.decode(start_pred, skip_special_tokens=True)\n",
    "\n",
    "    # Calculate EM and F1\n",
    "    em = metric_squad.compute(predictions=decoded_preds, references=labels)[\"exact_match\"]\n",
    "    f1 = metric_squad.compute(predictions=decoded_preds, references=labels)[\"f1\"]\n",
    "\n",
    "    return {\"eval_exact_match\": em, \"eval_f1\": f1}\n",
    "\n",
    "# TrainingArguments with matching eval & save strategy\n",
    "args_qa = TrainingArguments(\n",
    "    output_dir=\"qa-lora\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=250,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Trainer setup with eval and metric computation\n",
    "trainer_qa = Trainer(\n",
    "    model=qa_model,\n",
    "    args=args_qa,\n",
    "    train_dataset=train_qa,\n",
    "    eval_dataset=val_qa,          # Evaluation dataset\n",
    "    data_collator=coll_qa,\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=compute_metrics_qa  \n",
    ")\n",
    "\n",
    "trainer_qa.train()\n",
    "qa_model.save_pretrained(\"qa-qlora\")\n",
    "PeftModel.from_pretrained(GPT2Model.from_pretrained(\"gpt2\"),\n",
    "                         \"qa-qlora\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e87a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr(\"Evaluation\")\n",
    "metric_squad = evaluate.load(\"squad\")\n",
    "val_qa = load_dataset(\"squad\", split=\"validation[:5000]\")\n",
    "\n",
    "pred_list, ref_list = [], []\n",
    "for i, ex in enumerate(tqdm.tqdm(val_qa, desc=\"QA\")):\n",
    "    enc = tok(ex[\"question\"] + tok.eos_token + ex[\"context\"],\n",
    "              return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    outs = qa_model(**enc)\n",
    "    s = int(torch.argmax(outs[\"start_logits\"]))\n",
    "    e = int(torch.argmax(outs[\"end_logits\"]))\n",
    "    ans = tok.decode(enc[\"input_ids\"][0][s:e+1]).strip()\n",
    "\n",
    "    pred_list.append({\"id\": str(i), \"prediction_text\": ans})\n",
    "    ref_list.append({\"id\": str(i),\n",
    "                     \"answers\": {\"text\": ex[\"answers\"][\"text\"],\n",
    "                                 \"answer_start\": []}})\n",
    "\n",
    "print(\"QA EM / F1:\",\n",
    "      metric_squad.compute(predictions=pred_list, references=ref_list))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
