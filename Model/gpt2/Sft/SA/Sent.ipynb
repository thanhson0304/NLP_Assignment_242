{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers datasets torch accelerate trl rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Config, GPT2Tokenizer\n",
    "from transformers import GPT2PreTrainedModel, GPT2Model\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "# ─── 1) GPT-2 + Value & Sentiment Head ────────────────────────────────────────\n",
    "class GPT2ForSentiment(PPOTrainer.model_class):  # placeholder, we'll build manually\n",
    "    def __init__(self, config: GPT2Config, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.value_head = nn.Linear(config.hidden_size, 1, bias=False)\n",
    "        self.dropout    = nn.Dropout(config.summary_first_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        hidden_states = self.transformer(input_ids, attention_mask=attention_mask)[0]\n",
    "        last_hidden   = hidden_states[:, -1, :]           # (B, H)\n",
    "\n",
    "        # value (critic)\n",
    "        values = self.value_head(last_hidden).squeeze(-1) # (B,)\n",
    "\n",
    "        # classification logits\n",
    "        pooled = self.dropout(last_hidden)\n",
    "        logits = self.classifier(pooled)                 # (B, 2)\n",
    "\n",
    "        # classification loss if labels given\n",
    "        loss_cls = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss_cls = loss_fct(logits, labels)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"loss\": loss_cls,\n",
    "            \"values\": values\n",
    "        }\n",
    "\n",
    "# ─── 2) Data, Tokenizer, Model ────────────────────────────────────────────────\n",
    "dataset  = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
    "val_ds   = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "\n",
    "config    = GPT2Config.from_pretrained(\"gpt2\")\n",
    "model     = GPT2ForSentiment(config)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ─── 3) PPO Setup ─────────────────────────────────────────────────────────────\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=\"gpt2\",\n",
    "    learning_rate=5e-6,\n",
    "    batch_size=8,\n",
    "    ppo_epochs=3,\n",
    "    kl_ctl={\"method\":\"adaptive\", \"target\":0.05},\n",
    "    log_with=\"tensorboard\",\n",
    ")\n",
    "ppo_trainer = PPOTrainer(model=model, tokenizer=tokenizer, config=ppo_config)\n",
    "\n",
    "# ─── 4) Reward fn (soft classification prob) ─────────────────────────────────\n",
    "def sentiment_reward(prompts, responses, labels):\n",
    "    # we ignore 'responses' text; use model outputs instead\n",
    "    batch = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    out   = model(batch.input_ids.to(model.device), attention_mask=batch.attention_mask.to(model.device))\n",
    "    probs = torch.softmax(out[\"logits\"], dim=-1)\n",
    "    r     = probs[torch.arange(len(labels)), torch.tensor(labels)]\n",
    "    return (r - r.mean()) / (r.std() + 1e-8)\n",
    "\n",
    "# ─── 5) Training Loop ─────────────────────────────────────────────────────────\n",
    "for epoch in range(3):\n",
    "    for i in range(0, len(dataset), ppo_config.batch_size):\n",
    "        batch  = dataset[i : i + ppo_config.batch_size]\n",
    "        texts  = batch[\"sentence\"]\n",
    "        labs   = batch[\"label\"]\n",
    "        prompts = [f\"Review: {t}\\nSentiment:\" for t in texts]\n",
    "\n",
    "        # a) generate (but we won’t use text)\n",
    "        response_toks = ppo_trainer.generate(prompts, max_length=4)\n",
    "\n",
    "        # b) reward via classification head\n",
    "        rewards = sentiment_reward(prompts, response_toks, labs)\n",
    "\n",
    "        # c) PPO step\n",
    "        stats = ppo_trainer.step(prompts, response_toks, rewards)\n",
    "        print(f\"[Epoch {epoch}] Sentiment reward: {stats['objective/mean_reward']:.4f}\")\n",
    "\n",
    "# ─── 6) Save ────────────────────────────────────────────────────────────────────\n",
    "ppo_trainer.save_pretrained(\"gpt2-ppo-sentiment\")\n",
    "tokenizer.save_pretrained(\"gpt2-ppo-sentiment\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
